{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример подготовки датасета формата Method2Test для input модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем написанный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset_m2t import Code2TestPrepareToInput, input_json_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызываем инциализацию датасета"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время инициализации датасета: 9.785 секунды\n"
     ]
    }
   ],
   "source": [
    "dataset_m2t_input = Code2TestPrepareToInput(input_json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:06<00:00, 43042.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время проверки формата запросов: 6.594 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:05<00:00, 52987.69it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 4066967.10it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 3169269.75it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 3027819.08it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 3908165.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время извлечения глобальных признаков из текста кода: 5.648 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:01<00:00, 202978.72it/s]\n",
      "100%|██████████| 281060/281060 [00:01<00:00, 199157.33it/s]\n",
      "100%|██████████| 281060/281060 [00:02<00:00, 105874.60it/s]\n",
      "100%|██████████| 281060/281060 [00:02<00:00, 108611.69it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 357458.54it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 343598.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время извлечения комментариев и описаний из текста кода: 9.667 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:03<00:00, 82498.38it/s]\n",
      "100%|██████████| 281060/281060 [00:06<00:00, 40261.82it/s]\n",
      "100%|██████████| 281060/281060 [00:01<00:00, 149921.12it/s]\n",
      "100%|██████████| 281060/281060 [00:05<00:00, 47593.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время очистки текста кода: 18.224 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:35<00:00, 7957.15it/s] \n",
      "100%|██████████| 281060/281060 [01:14<00:00, 3774.37it/s]\n",
      "100%|██████████| 281060/281060 [00:02<00:00, 113290.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время получения ast-деревьев на основе текста кода: 112.859 секунды\n",
      "Время подготовки датасета: 152.993 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 281060/281060 [00:01<00:00, 242589.30it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 385425.06it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 421370.99it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 347020.26it/s]\n",
      "100%|██████████| 281060/281060 [00:03<00:00, 91182.64it/s] \n",
      "100%|██████████| 281060/281060 [00:00<00:00, 332190.50it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 465367.43it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 376185.17it/s]\n",
      "100%|██████████| 281060/281060 [00:01<00:00, 258068.64it/s]\n",
      "100%|██████████| 281060/281060 [00:01<00:00, 260424.79it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 409331.39it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 288841.70it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 426618.99it/s]\n",
      "100%|██████████| 281060/281060 [00:00<00:00, 355836.60it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 387552.89it/s]\n",
      "100%|██████████| 280458/280458 [00:02<00:00, 108664.64it/s]\n",
      "100%|██████████| 280458/280458 [00:04<00:00, 62810.73it/s] \n",
      "100%|██████████| 280458/280458 [00:15<00:00, 18590.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время подготовки focal_method: 38.664 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280458/280458 [00:01<00:00, 148550.56it/s]\n",
      "100%|██████████| 280458/280458 [00:01<00:00, 275918.05it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 367280.08it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 444334.94it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 284898.12it/s]\n",
      "100%|██████████| 280458/280458 [00:05<00:00, 54791.23it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 309600.48it/s]\n",
      "100%|██████████| 280458/280458 [00:01<00:00, 228650.01it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 282198.57it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 330346.28it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 432062.06it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 350408.67it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 362420.10it/s]\n",
      "100%|██████████| 280458/280458 [00:03<00:00, 84761.17it/s] \n",
      "100%|██████████| 280458/280458 [00:06<00:00, 41095.11it/s]\n",
      "100%|██████████| 280458/280458 [00:23<00:00, 11942.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Время подготовки focal_cls: 51.376 секунды\n",
      "Время подготовки датасета: 91.144 секунды\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_m2t_input.prepare_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим на датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = dataset_m2t_input.get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query</th>\n",
       "      <th>response</th>\n",
       "      <th>code_features</th>\n",
       "      <th>LANG_TOKEN</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>You are a professional python software enginee...</td>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>(python, ```python\\ndef get(self, key, default...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get(self, key, default=None): kl = key.low...</td>\n",
       "      <td>class NoCaseDict(dict): FUNC_TOKEN</td>\n",
       "      <td>Module( body=[ FunctionDef( name='get', args=a...</td>\n",
       "      <td>Module( body=[ ClassDef( name='NoCaseDict', ba...</td>\n",
       "      <td>INFO_TOKEN</td>\n",
       "      <td>INFO_TOKEN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>You are a professional python software enginee...</td>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>(python, ```python\\ndef get(self, url_pattern)...</td>\n",
       "      <td>python</td>\n",
       "      <td>def get(self, url_pattern): return self.route(...</td>\n",
       "      <td>class Microdot: def route(self, url_pattern, m...</td>\n",
       "      <td>Module( body=[ FunctionDef( name='get', args=a...</td>\n",
       "      <td>Module( body=[ ClassDef( name='Microdot', base...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Decorator that is used to r...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Decorator that is used to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>You are a professional python software enginee...</td>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>(python, ```python\\ndef post(self, url_pattern...</td>\n",
       "      <td>python</td>\n",
       "      <td>def post(self, url_pattern): return self.route...</td>\n",
       "      <td>class Microdot: def route(self, url_pattern, m...</td>\n",
       "      <td>Module( body=[ FunctionDef( name='post', args=...</td>\n",
       "      <td>Module( body=[ ClassDef( name='Microdot', base...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Decorator that is used to r...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Decorator that is used to r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>You are a professional python software enginee...</td>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>(python, ```python\\ndef mount(self, subapp, ur...</td>\n",
       "      <td>python</td>\n",
       "      <td>def mount(self, subapp, url_prefix=''): for me...</td>\n",
       "      <td>class Microdot: FUNC_TOKEN</td>\n",
       "      <td>Module( body=[ FunctionDef( name='mount', args...</td>\n",
       "      <td>Module( body=[ ClassDef( name='Microdot', base...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Mount a sub-application, op...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Mount a sub-application, op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>You are a professional python software enginee...</td>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>(python, ```python\\ndef iob2bio(tags):\\n    pr...</td>\n",
       "      <td>python</td>\n",
       "      <td>def iob2bio(tags): processed_tags = [] # shoul...</td>\n",
       "      <td>def split_tag(tag: str): if tag in [\"O\", \"-X-\"...</td>\n",
       "      <td>Module( body=[ FunctionDef( name='iob2bio', ar...</td>\n",
       "      <td>Module( body=[ FunctionDef( name='split_tag', ...</td>\n",
       "      <td>DESCRIPTION_TOKEN: should be bio format case1 ...</td>\n",
       "      <td>DESCRIPTION_TOKEN: Split tag into state and na...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              query  \\\n",
       "0   1  You are a professional python software enginee...   \n",
       "1   2  You are a professional python software enginee...   \n",
       "2   3  You are a professional python software enginee...   \n",
       "3   4  You are a professional python software enginee...   \n",
       "4   5  You are a professional python software enginee...   \n",
       "\n",
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                       code_features LANG_TOKEN  \\\n",
       "0  (python, ```python\\ndef get(self, key, default...     python   \n",
       "1  (python, ```python\\ndef get(self, url_pattern)...     python   \n",
       "2  (python, ```python\\ndef post(self, url_pattern...     python   \n",
       "3  (python, ```python\\ndef mount(self, subapp, ur...     python   \n",
       "4  (python, ```python\\ndef iob2bio(tags):\\n    pr...     python   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  def get(self, key, default=None): kl = key.low...   \n",
       "1  def get(self, url_pattern): return self.route(...   \n",
       "2  def post(self, url_pattern): return self.route...   \n",
       "3  def mount(self, subapp, url_prefix=''): for me...   \n",
       "4  def iob2bio(tags): processed_tags = [] # shoul...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                 class NoCaseDict(dict): FUNC_TOKEN   \n",
       "1  class Microdot: def route(self, url_pattern, m...   \n",
       "2  class Microdot: def route(self, url_pattern, m...   \n",
       "3                         class Microdot: FUNC_TOKEN   \n",
       "4  def split_tag(tag: str): if tag in [\"O\", \"-X-\"...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  Module( body=[ FunctionDef( name='get', args=a...   \n",
       "1  Module( body=[ FunctionDef( name='get', args=a...   \n",
       "2  Module( body=[ FunctionDef( name='post', args=...   \n",
       "3  Module( body=[ FunctionDef( name='mount', args...   \n",
       "4  Module( body=[ FunctionDef( name='iob2bio', ar...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0  Module( body=[ ClassDef( name='NoCaseDict', ba...   \n",
       "1  Module( body=[ ClassDef( name='Microdot', base...   \n",
       "2  Module( body=[ ClassDef( name='Microdot', base...   \n",
       "3  Module( body=[ ClassDef( name='Microdot', base...   \n",
       "4  Module( body=[ FunctionDef( name='split_tag', ...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                         INFO_TOKEN   \n",
       "1  DESCRIPTION_TOKEN: Decorator that is used to r...   \n",
       "2  DESCRIPTION_TOKEN: Decorator that is used to r...   \n",
       "3  DESCRIPTION_TOKEN: Mount a sub-application, op...   \n",
       "4  DESCRIPTION_TOKEN: should be bio format case1 ...   \n",
       "\n",
       "                                      focal_cls_info  \n",
       "0                                         INFO_TOKEN  \n",
       "1  DESCRIPTION_TOKEN: Decorator that is used to r...  \n",
       "2  DESCRIPTION_TOKEN: Decorator that is used to r...  \n",
       "3  DESCRIPTION_TOKEN: Mount a sub-application, op...  \n",
       "4  DESCRIPTION_TOKEN: Split tag into state and na...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Информация о датасете:\n",
    "\n",
    "* id: идентификатор\n",
    "\n",
    "* query: promt-текст для языковой модели\n",
    "\n",
    "* code_features: кортеж признаков, извлечённых из текста кода\n",
    "\n",
    "* LANG_TOKEN: токен языка (в данном датасете - 'python')\n",
    "\n",
    "* focal_method: код метода-функции (очищенный от комментариев и описаний, переносов строк, табуляций и приведённый в формат строки)\n",
    "\n",
    "* focal_cls: код класса или окружения, внутри которого метод-функция (очищенный от комментариев и описаний, переносов строк, табуляций и приведённый в формат строки, искомая функция заменяется на <FUNC_TOKEN>)\n",
    "\n",
    "* focal_method_ast, focal_cls_ast - ast представления кода для *focal_method* и *focal_class* (Приведённая к формату строки без с удалёнными переносами строк и табуляциями)\n",
    "\n",
    "* focal_method_info, focal_cls_info - информация о коде, представленная на естественном языке (формат <DESCRIPTION_TOKEN>: {описание} <COMMENT_TOKEN>: {комментарии}, в случае отсутствия описания и комментариев формат <INFO_TOKEN>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 280458 entries, 0 to 281059\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count   Dtype \n",
      "---  ------             --------------   ----- \n",
      " 0   id                 280458 non-null  int64 \n",
      " 1   query              280458 non-null  object\n",
      " 2   response           280458 non-null  object\n",
      " 3   code_features      280458 non-null  object\n",
      " 4   LANG_TOKEN         280458 non-null  object\n",
      " 5   focal_method       280458 non-null  object\n",
      " 6   focal_cls          280458 non-null  object\n",
      " 7   focal_method_ast   280458 non-null  object\n",
      " 8   focal_cls_ast      280458 non-null  object\n",
      " 9   focal_method_info  280458 non-null  object\n",
      " 10  focal_cls_info     280458 non-null  object\n",
      "dtypes: int64(1), object(10)\n",
      "memory usage: 25.7+ MB\n"
     ]
    }
   ],
   "source": [
    "code_dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вид колонок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                                 491\n",
       "query                You are a professional python software enginee...\n",
       "response             from tests.test_utils import assert_pytree_all...\n",
       "code_features        (python, ```python\\ndef array_to_slog(x: Array...\n",
       "LANG_TOKEN                                                      python\n",
       "focal_method         def array_to_slog(x: Array) -> SLArray: return...\n",
       "focal_cls            from .typing import Array, SLArray, ArrayList,...\n",
       "focal_method_ast     Module( body=[ FunctionDef( name='array_to_slo...\n",
       "focal_cls_ast        Module( body=[ ImportFrom( module='typing', na...\n",
       "focal_method_info    DESCRIPTION_TOKEN: Converts a regular array in...\n",
       "focal_cls_info       DESCRIPTION_TOKEN: Converts a regular array in...\n",
       "Name: 490, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.iloc[490, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a professional python software engineer. You are asked to generate a complete test class for a focal method in a focal class.\n",
      "You will be given the following information of the focal method:\n",
      "1. Source code of the focal method.\n",
      "2. Source code of the focal class(Code that is not relevant to focal method's execution is filtered).\n",
      "3. Source code of callee examples of the focal method.\n",
      "You will ONLY return unit test code for the focal method including necessary imports and dependencies, make sure it compile without errors, and use reflection to invoke private methods. \n",
      "Note that no additional explanations required.\n",
      "\n",
      "Here are the information of the focal method:\n",
      "1. Source code of the focal method.\n",
      "```python\n",
      "def array_to_slog(x: Array) -> SLArray:\n",
      "    \"\"\"Converts a regular array into (sign, logabs) form.\n",
      "\n",
      "    Args:\n",
      "        x (Array): input data.\n",
      "\n",
      "    Returns:\n",
      "        (SLArray): data in form (sign(x), log(abs(x)))\n",
      "    \"\"\"\n",
      "    return (jnp.sign(x), jnp.log(jnp.abs(x)))\n",
      "```\n",
      "\n",
      "2. Source code of the focal class(Codes that are may not related to focal method are filtered).\n",
      "```python\n",
      "from .typing import Array, SLArray, ArrayList, SLArrayList\n",
      "import jax.numpy as jnp\n",
      "\n",
      "def array_to_slog(x: Array) -> SLArray:\n",
      "    \"\"\"Converts a regular array into (sign, logabs) form.\n",
      "    Args:\n",
      "        x (Array): input data.\n",
      "    Returns:\n",
      "        (SLArray): data in form (sign(x), log(abs(x)))\n",
      "    \"\"\"\n",
      "    return (jnp.sign(x), jnp.log(jnp.abs(x)))\n",
      "```\n",
      "\n",
      "3. Source code of callee examples of the focal method.\n",
      ">>>Callee example 1\n",
      "```python\n",
      "from .core import (\n",
      "    Module,\n",
      "    get_alternating_signs,\n",
      "    get_nelec_per_split,\n",
      "    is_tuple_of_arrays,\n",
      "    split,\n",
      ")\n",
      "from typing import Optional, Tuple\n",
      "import jax.numpy as jnp\n",
      "from vmcnet.utils.slog_helpers import array_list_to_slog, array_to_slog, slog_multiply\n",
      "from vmcnet.utils.typing import Array, ArrayList, SLArray, SLArrayList, ParticleSplit\n",
      "\n",
      "def get_submatrices_along_first_col(x: Array) -> Tuple[int, Array]:\n",
      "    \"\"\"Get the submatrices of x by deleting row i and col 0, for all rows of x.\n",
      "    Args:\n",
      "        x (Array): a tensor of orbital matrices which is square in the last two\n",
      "            dimensions, thus of shape (..., n, n). The second last dimension is the\n",
      "            particle dimension, and the last is the orbital dimension.\n",
      "    Returns:\n",
      "        (int, Array): n, submatrices of shape (..., n, n-1, n-1), obtained by\n",
      "        deleting row (..., i, :) and deleted column is (..., :, 0), for 0 <= i <= n - 1.\n",
      "    \"\"\"\n",
      "    if len(x.shape) < 2 or x.shape[-1] != x.shape[-2]:\n",
      "        msg = \"Calculating cofactors requires shape (..., n, n), got {}\"\n",
      "        raise ValueError(msg.format(x.shape))\n",
      "    n = x.shape[-1]\n",
      "    # Calculate minor_(0,i) by deleting the first orbital and ith particle indices\n",
      "    submats = [jnp.delete(jnp.delete(x, i, axis=-2), 0, axis=-1) for i in range(n)]\n",
      "    # Stack on axis -3 to ensure shape (..., n) once det removes the last two axes\n",
      "    stacked_submats = jnp.stack(submats, axis=-3)\n",
      "    return n, stacked_submats\n",
      "def slog_cofactor_antieq(x: Array) -> SLArray:\n",
      "    \"\"\"Compute a cofactor-based antiequivariance, returning results in slogabs form.\n",
      "    See :func:`~vmcnet.models.antiequivariance.cofactor_antieq`. This function performs\n",
      "    the same operations, but gives a result in the (sign, log) domain, going through\n",
      "    a jnp.linalg.slogdet call instead of a jnp.linalg.det call.\n",
      "    Args:\n",
      "        x (Array): a tensor of orbital matrices which is square in the last two\n",
      "            dimensions, thus of shape (..., n, n). The second last dimension is the\n",
      "            particle dimension, and the last is the orbital dimension.\n",
      "    Returns:\n",
      "        (Array, Array): tuple of arrays, each of shape (..., n). The first\n",
      "        is sign(result), and the second is log(abs(result)).\n",
      "    \"\"\"\n",
      "    # Calculate x_(i, 0) by selecting orbital index 0\n",
      "    first_orbital_vals = x[..., 0]\n",
      "    orbital_signs, orbital_logs = array_to_slog(first_orbital_vals)\n",
      "    n, stacked_submatrices = get_submatrices_along_first_col(x)\n",
      "    # TODO(ggoldsh): find a faster way to calculate these overlapping determinants.\n",
      "    (cofactor_signs, cofactor_logs) = jnp.linalg.slogdet(stacked_submatrices)\n",
      "    signs_and_logs = (\n",
      "        orbital_signs * cofactor_signs * get_alternating_signs(n),\n",
      "        orbital_logs + cofactor_logs,\n",
      "    )\n",
      "    return signs_and_logs\n",
      "```\n",
      "\n",
      "\n",
      "Please note that the test class you return should include multiple test cases covering different functionalities. There is no upper limit on the number of test cases, but you need to ensure that the test cases provide high test coverage and test extreme and special cases of the code as much as possible.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[490, :]['query'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from pyner.named_entity.corpus import bio2bioes\n",
      "from pyner.named_entity.corpus import iob2bio\n",
      "\n",
      "TEST_IOB1 = [\"I-PER\", \"I-ORG\", \"I-ORG\", \"B-ORG\", \"O\"]\n",
      "TEST_BIO1 = [\"B-PER\", \"B-ORG\", \"I-ORG\", \"B-ORG\", \"O\"]\n",
      "TEST_BIOES1 = [\"S-PER\", \"B-ORG\", \"E-ORG\", \"S-ORG\", \"O\"]\n",
      "def test_iob2bio1():\n",
      "    assert iob2bio(TEST_IOB1) == TEST_BIO1\n",
      "def test_iob2bioes1():\n",
      "    assert bio2bioes(iob2bio(TEST_IOB1)) == TEST_BIOES1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def iob2bio(tags): processed_tags = [] # should be bio format prev_state = None prev_label = None for t, tag in enumerate(tags): state, label = split_tag(tag) # case1. I-ORG I-ORG # ^^^^^ if t == 0 and state == \"I\": new_state = \"B\" # case2. I-ORG I-PERSON # ^^^^^^^^ elif state == \"I\" and prev_label != label: new_state = \"B\" # case3. O I-ORG # ^^^^^ elif state == \"I\" and prev_state == \"O\": new_state = \"B\" # case4. I-ORG I-ORG # ^^^^^ elif state == \"I\" and prev_label == label: new_state = \"I\" else: new_state = state if label is None: new_tag = \"O\" else: new_tag = f\"{new_state}-{label}\" processed_tags.append(new_tag) prev_state = state prev_label = label return processed_tags\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_method'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_cls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def split_tag(tag: str): if tag in [\"O\", \"-X-\"]: state, label = \"O\", None else: state, label = tag.split(\"-\") return state, label FUNC_TOKEN\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_cls'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_method_ast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module( body=[ FunctionDef( name='iob2bio', args=arguments( posonlyargs=[], args=[ arg(arg='tags')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[ Assign( targets=[ Name(id='processed_tags', ctx=Store())], value=List(elts=[], ctx=Load())), Assign( targets=[ Name(id='prev_state', ctx=Store())], value=Constant(value=None)), Assign( targets=[ Name(id='prev_label', ctx=Store())], value=Constant(value=None)), For( target=Tuple( elts=[ Name(id='t', ctx=Store()), Name(id='tag', ctx=Store())], ctx=Store()), iter=Call( func=Name(id='enumerate', ctx=Load()), args=[ Name(id='tags', ctx=Load())], keywords=[]), body=[ Assign( targets=[ Tuple( elts=[ Name(id='state', ctx=Store()), Name(id='label', ctx=Store())], ctx=Store())], value=Call( func=Name(id='split_tag', ctx=Load()), args=[ Name(id='tag', ctx=Load())], keywords=[])), If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='t', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value=0)]), Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_label', ctx=Load()), ops=[ NotEq()], comparators=[ Name(id='label', ctx=Load())])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='O')])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_label', ctx=Load()), ops=[ Eq()], comparators=[ Name(id='label', ctx=Load())])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='I'))], orelse=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Name(id='state', ctx=Load()))])])])]), If( test=Compare( left=Name(id='label', ctx=Load()), ops=[ Is()], comparators=[ Constant(value=None)]), body=[ Assign( targets=[ Name(id='new_tag', ctx=Store())], value=Constant(value='O'))], orelse=[ Assign( targets=[ Name(id='new_tag', ctx=Store())], value=JoinedStr( values=[ FormattedValue( value=Name(id='new_state', ctx=Load()), conversion=-1), Constant(value='-'), FormattedValue( value=Name(id='label', ctx=Load()), conversion=-1)]))]), Expr( value=Call( func=Attribute( value=Name(id='processed_tags', ctx=Load()), attr='append', ctx=Load()), args=[ Name(id='new_tag', ctx=Load())], keywords=[])), Assign( targets=[ Name(id='prev_state', ctx=Store())], value=Name(id='state', ctx=Load())), Assign( targets=[ Name(id='prev_label', ctx=Store())], value=Name(id='label', ctx=Load()))], orelse=[]), Return( value=Name(id='processed_tags', ctx=Load()))], decorator_list=[], type_params=[])], type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_method_ast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_cls_ast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module( body=[ FunctionDef( name='split_tag', args=arguments( posonlyargs=[], args=[ arg( arg='tag', annotation=Name(id='str', ctx=Load()))], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[ Expr( value=Constant(value='\\n Split tag into state and named entity category.\\n Parameters\\n ---\\n tag (str)\\n NE tag (e.g. B-PER)\\n ')), If( test=Compare( left=Name(id='tag', ctx=Load()), ops=[ In()], comparators=[ List( elts=[ Constant(value='O'), Constant(value='-X-')], ctx=Load())]), body=[ Assign( targets=[ Tuple( elts=[ Name(id='state', ctx=Store()), Name(id='label', ctx=Store())], ctx=Store())], value=Tuple( elts=[ Constant(value='O'), Constant(value=None)], ctx=Load()))], orelse=[ Assign( targets=[ Tuple( elts=[ Name(id='state', ctx=Store()), Name(id='label', ctx=Store())], ctx=Store())], value=Call( func=Attribute( value=Name(id='tag', ctx=Load()), attr='split', ctx=Load()), args=[ Constant(value='-')], keywords=[]))]), Return( value=Tuple( elts=[ Name(id='state', ctx=Load()), Name(id='label', ctx=Load())], ctx=Load()))], decorator_list=[], type_params=[]), FunctionDef( name='iob2bio', args=arguments( posonlyargs=[], args=[ arg(arg='tags')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[ Assign( targets=[ Name(id='processed_tags', ctx=Store())], value=List(elts=[], ctx=Load())), Assign( targets=[ Name(id='prev_state', ctx=Store())], value=Constant(value=None)), Assign( targets=[ Name(id='prev_label', ctx=Store())], value=Constant(value=None)), For( target=Tuple( elts=[ Name(id='t', ctx=Store()), Name(id='tag', ctx=Store())], ctx=Store()), iter=Call( func=Name(id='enumerate', ctx=Load()), args=[ Name(id='tags', ctx=Load())], keywords=[]), body=[ Assign( targets=[ Tuple( elts=[ Name(id='state', ctx=Store()), Name(id='label', ctx=Store())], ctx=Store())], value=Call( func=Name(id='split_tag', ctx=Load()), args=[ Name(id='tag', ctx=Load())], keywords=[])), If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='t', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value=0)]), Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_label', ctx=Load()), ops=[ NotEq()], comparators=[ Name(id='label', ctx=Load())])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='O')])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='B'))], orelse=[ If( test=BoolOp( op=And(), values=[ Compare( left=Name(id='state', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value='I')]), Compare( left=Name(id='prev_label', ctx=Load()), ops=[ Eq()], comparators=[ Name(id='label', ctx=Load())])]), body=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Constant(value='I'))], orelse=[ Assign( targets=[ Name(id='new_state', ctx=Store())], value=Name(id='state', ctx=Load()))])])])]), If( test=Compare( left=Name(id='label', ctx=Load()), ops=[ Is()], comparators=[ Constant(value=None)]), body=[ Assign( targets=[ Name(id='new_tag', ctx=Store())], value=Constant(value='O'))], orelse=[ Assign( targets=[ Name(id='new_tag', ctx=Store())], value=JoinedStr( values=[ FormattedValue( value=Name(id='new_state', ctx=Load()), conversion=-1), Constant(value='-'), FormattedValue( value=Name(id='label', ctx=Load()), conversion=-1)]))]), Expr( value=Call( func=Attribute( value=Name(id='processed_tags', ctx=Load()), attr='append', ctx=Load()), args=[ Name(id='new_tag', ctx=Load())], keywords=[])), Assign( targets=[ Name(id='prev_state', ctx=Store())], value=Name(id='state', ctx=Load())), Assign( targets=[ Name(id='prev_label', ctx=Store())], value=Name(id='label', ctx=Load()))], orelse=[]), Return( value=Name(id='processed_tags', ctx=Load()))], decorator_list=[], type_params=[])], type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_cls_ast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_method_info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_TOKEN: should be bio format case1 IORG IORG case2 IORG IPERSON case3 O IORG case4 IORG IORG\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_method_info'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "focal_cls_info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DESCRIPTION_TOKEN: Split tag into state and named entity category. Parameters --- tag (str) NE tag (e.g. B-PER) COMMENTS_TOKEN: # should be bio format# case1. I-ORG I-ORG# ^^^^^# case2. I-ORG I-PERSON# ^^^^^^^^# case3. O I-ORG# ^^^^^# case4. I-ORG I-ORG# ^^^^^\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset.iloc[4, :]['focal_cls_info'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(280458, 11)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
