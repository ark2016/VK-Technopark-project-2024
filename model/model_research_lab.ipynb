{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура модели анализа кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном файле проводится анализ архитектуры модели, токенизатора и подготовка к обучению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from torch.utils.tensorboard import summary, writer, SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее считываем исходный датасет и немного дорабатываем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_dataset = pd.read_parquet('code_dataset.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем дополнительеные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from create_dataset_m2t_tokens import final_preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 280458/280458 [00:00<00:00, 391088.22it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 308981.95it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 318787.39it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 359814.29it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 317530.69it/s]\n",
      "100%|██████████| 280458/280458 [00:00<00:00, 294418.12it/s]\n",
      "100%|██████████| 280458/280458 [00:01<00:00, 154419.45it/s]\n",
      "100%|██████████| 280458/280458 [00:02<00:00, 108791.82it/s]\n"
     ]
    }
   ],
   "source": [
    "code_dataset = final_preparations(code_dataset_copy=code_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "      <th>input_string_focal_method</th>\n",
       "      <th>input_string_focal_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0                                        <AST_TOKEN>   \n",
       "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "3                                        <AST_TOKEN>   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
       "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
       "\n",
       "                                      focal_cls_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "3                                       <INFO_TOKEN>   \n",
       "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
       "\n",
       "                           input_string_focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                              input_string_focal_cls  \n",
       "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<FUNC_TOKEN> def get(self, key, default=None): kl = key.lower() return super().get(self.keymap.get(kl, kl), default) <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='get', args=arguments( posonlyargs=[], args=[ arg(arg='self'), arg(arg='key'), arg(arg='default')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=None)]), body=[ Assign( targets=[ Name(id='kl', ctx=Store())], value=Call( func=Attribute( value=Name(id='key', ctx=Load()), attr='lower', ctx=Load()), args=[], keywords=[])), Return( value=Call( func=Attribute( value=Call( func=Name(id='super', ctx=Load()), args=[], keywords=[]), attr='get', ctx=Load()), args=[ Call( func=Attribute( value=Attribute( value=Name(id='self', ctx=Load()), attr='keymap', ctx=Load()), attr='get', ctx=Load()), args=[ Name(id='kl', ctx=Load()), Name(id='kl', ctx=Load())], keywords=[]), Name(id='default', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "print(code_dataset['input_string_focal_method'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, переходим к анализу архитектур нейросетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решено использовать подход, основанный на обучении (fine-tuning) нейросети CodeBERT, в основе которой лежит модель RoBERTa. Далее будем использовать метамодель в виде декодера (CodeGen или GPTBigCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизаторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_code_bert = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizerGPT = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работает базовый токенизатор для CodeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед этим добавим новые служебные токены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_special_tokens = ['<FUNC_TOKEN>',\n",
    "            '<INFO_TOKEN>',\n",
    "            '<CLS_TOKEN>', \n",
    "            '<AST_TOKEN>', \n",
    "            '<DESCRIPTION_TOKEN>',\n",
    "            '<COMMENTS_TOKEN>']\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': new_special_tokens\n",
    "}\n",
    "\n",
    "tokenizer_code_bert.add_special_tokens(special_tokens_dict)\n",
    "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина закодированной последовательности: 325\n",
      "Как выглядят токены исходной фразы: ['<FUNC_TOKEN>', 'Ġdef', 'Ġget', '(', 'self', ',', 'Ġkey', ',', 'Ġdefault', '=', 'None', '):', 'Ġk', 'l', 'Ġ=', 'Ġkey', '.', 'lower', '()', 'Ġreturn', 'Ġsuper', '().', 'get', '(', 'self', '.', 'key', 'map', '.', 'get', '(', 'kl', ',', 'Ġk', 'l', '),', 'Ġdefault', ')', 'Ġ', '<INFO_TOKEN>', 'Ġ', '<AST_TOKEN>', 'ĠModule', '(', 'Ġbody', '=[', 'ĠFunction', 'Def', '(', 'Ġname', \"='\", 'get', \"',\", 'Ġargs', '=', 'arg', 'uments', '(', 'Ġpos', 'only', 'args', '=[', '],', 'Ġargs', '=[', 'Ġarg', '(', 'arg', \"='\", 'self', \"'),\", 'Ġarg', '(', 'arg', \"='\", 'key', \"'),\", 'Ġarg', '(', 'arg', \"='\", 'default', \"')\", '],', 'Ġk', 'w', 'only', 'args', '=[', '],', 'Ġk', 'w', '_', 'default', 's', '=[', '],', 'Ġdefaults', '=[', 'ĠConstant', '(', 'value', '=', 'None', ')]', '),', 'Ġbody', '=[', 'ĠAss', 'ign', '(', 'Ġtargets', '=[', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Store', '())', '],', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'key', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'lower', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', '],', 'Ġkeywords', '=[', '])', '),', 'ĠReturn', '(', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Name', '(', 'id', \"='\", 'super', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', '],', 'Ġkeywords', '=', '[]', '),', 'Ġatt', 'r', \"='\", 'get', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'self', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'key', 'map', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'get', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '),', 'ĠName', '(', 'id', \"='\", 'default', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '))', '],', 'Ġdecor', 'ator', '_', 'list', '=[', '],', 'Ġtype', '_', 'params', '=[', '])', '],', 'Ġtype', '_', 'ign', 'ores', '=[', '])']\n",
      "Индексы токенов: [50265, 3816, 120, 1640, 13367, 6, 762, 6, 6814, 5214, 29802, 3256, 449, 462, 5457, 762, 4, 29668, 43048, 671, 2422, 49123, 6460, 1640, 13367, 4, 5282, 32557, 4, 6460, 1640, 16291, 6, 449, 462, 238, 6814, 43, 1437, 50266, 1437, 50268, 42122, 1640, 809, 49310, 42419, 17425, 1640, 766, 47579, 6460, 3934, 49503, 5214, 5384, 30179, 1640, 8593, 8338, 48204, 49310, 7479, 49503, 49310, 29480, 1640, 5384, 47579, 13367, 41734, 29480, 1640, 5384, 47579, 5282, 41734, 29480, 1640, 5384, 47579, 43234, 27645, 7479, 449, 605, 8338, 48204, 49310, 7479, 449, 605, 1215, 43234, 29, 49310, 7479, 31896, 49310, 33685, 1640, 19434, 5214, 29802, 46077, 238, 809, 49310, 6331, 4932, 1640, 3247, 49310, 10704, 1640, 808, 47579, 16291, 3934, 740, 43820, 5214, 40266, 49338, 7479, 923, 5214, 20653, 1640, 26437, 5214, 49385, 1640, 923, 5214, 31723, 1640, 808, 47579, 5282, 3934, 740, 43820, 5214, 47167, 43048, 238, 15095, 338, 47579, 29668, 3934, 740, 43820, 5214, 47167, 43048, 238, 49503, 49310, 7479, 32712, 49310, 45587, 238, 11968, 1640, 923, 5214, 20653, 1640, 26437, 5214, 49385, 1640, 923, 5214, 20653, 1640, 26437, 5214, 31723, 1640, 808, 47579, 16101, 3934, 740, 43820, 5214, 47167, 43048, 238, 49503, 49310, 7479, 32712, 5214, 48992, 238, 15095, 338, 47579, 6460, 3934, 740, 43820, 5214, 47167, 43048, 238, 49503, 49310, 3310, 1640, 26437, 5214, 49385, 1640, 923, 5214, 49385, 1640, 923, 5214, 31723, 1640, 808, 47579, 13367, 3934, 740, 43820, 5214, 47167, 43048, 238, 15095, 338, 47579, 5282, 32557, 3934, 740, 43820, 5214, 47167, 43048, 238, 15095, 338, 47579, 6460, 3934, 740, 43820, 5214, 47167, 43048, 238, 49503, 49310, 10704, 1640, 808, 47579, 16291, 3934, 740, 43820, 5214, 47167, 43048, 238, 10704, 1640, 808, 47579, 16291, 3934, 740, 43820, 5214, 47167, 49338, 7479, 32712, 5214, 48992, 238, 10704, 1640, 808, 47579, 43234, 3934, 740, 43820, 5214, 47167, 49338, 7479, 32712, 5214, 48992, 35122, 7479, 12489, 2630, 1215, 8458, 49310, 7479, 1907, 1215, 49237, 49310, 45587, 7479, 1907, 1215, 4932, 4765, 49310, 45587]\n",
      "Декодированная строка: <FUNC_TOKEN> def get(self, key, default=None): kl = key.lower() return super().get(self.keymap.get(kl, kl), default) <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='get', args=arguments( posonlyargs=[], args=[ arg(arg='self'), arg(arg='key'), arg(arg='default')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=None)]), body=[ Assign( targets=[ Name(id='kl', ctx=Store())], value=Call( func=Attribute( value=Name(id='key', ctx=Load()), attr='lower', ctx=Load()), args=[], keywords=[])), Return( value=Call( func=Attribute( value=Call( func=Name(id='super', ctx=Load()), args=[], keywords=[]), attr='get', ctx=Load()), args=[ Call( func=Attribute( value=Attribute( value=Name(id='self', ctx=Load()), attr='keymap', ctx=Load()), attr='get', ctx=Load()), args=[ Name(id='kl', ctx=Load()), Name(id='kl', ctx=Load())], keywords=[]), Name(id='default', ctx=Load())], keywords=[]))], decorator_list=[], type_params=[])], type_ignores=[])\n"
     ]
    }
   ],
   "source": [
    "def tokenization_example(input_str: str):\n",
    "\t'''Функция отображения результатов токенизации'''\n",
    "\tcode_bert_tokens_example = tokenizer_code_bert.tokenize(input_str)\n",
    "\tcode_bert_tokens_ids = tokenizer_code_bert.convert_tokens_to_ids(code_bert_tokens_example)\n",
    "\tcode_bert_decoded = tokenizer_code_bert.decode(code_bert_tokens_ids)\n",
    "\tprint(f\"Длина закодированной последовательности: {len(code_bert_tokens_example)}\")\n",
    "\tprint(f\"Как выглядят токены исходной фразы: {code_bert_tokens_example}\")\n",
    "\tprint(f\"Индексы токенов: {code_bert_tokens_ids}\")\n",
    "\tprint(f\"Декодированная строка: {code_bert_decoded}\")\n",
    "\n",
    "tokenization_example(code_dataset['input_string_focal_method'].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необхоимо описать класс Dataset для нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Code2TestDataset(Dataset):\n",
    "\t'''Класс датасет для задачи генерации тестов'''\n",
    "\n",
    "\tdef __init__(self, code_dataset, tokenizer_code_bert, tokenizer_gpt, max_length=512):\n",
    "\t\t'''\n",
    "\t\tКонструктор датасета\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- code_dataset: датасет pd.DataFrame\n",
    "\t\t- tokenizer_code_bert: токенизатор code_bert\n",
    "\t\t- tokenizer_gpt: токенизатор gpt\n",
    "\t\t- max_length: максимальная длина последовательности (default: 512)\n",
    "\t\t'''\n",
    "\t\tself.code_dataset = code_dataset\n",
    "\t\tself.tokenizer_code_bert = tokenizer_code_bert\n",
    "\t\tself.tokenizer_gpt = tokenizer_gpt\n",
    "\t\tself.max_length = max_length\n",
    "\n",
    "\tdef __getitem__(self, idx, idx_to_token=False):\n",
    "\t\t'''\n",
    "\t\tGet-метод - возвращает сэмпл по индексу\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- idx: индекс\n",
    "\t\t- idx_to_token: флаг для отображения токенов из индексов (default: False)\n",
    "\t\t'''\n",
    "\t\tfocal_method_input = self.code_dataset.at[idx, 'input_string_focal_method']\n",
    "\t\tfocal_cls_input = self.code_dataset.at[idx, 'input_string_focal_cls']\n",
    "\t\tresponse = self.code_dataset.at[idx, 'response']\n",
    "\n",
    "\t\tdef encode_text(text, tokenizer):\n",
    "\t\t\tencoding = tokenizer.encode_plus(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tadd_special_tokens=True,\n",
    "\t\t\t\tmax_length=self.max_length if tokenizer == self.tokenizer_code_bert else self.max_length * 2,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\treturn_attention_mask=True,\n",
    "\t\t\t\treturn_tensors='pt',\n",
    "\t\t\t)\n",
    "\t\t\tinput_ids = encoding['input_ids'].flatten()\n",
    "\t\t\tattention_mask = encoding['attention_mask'].flatten()\n",
    "\t\t\treturn input_ids, attention_mask\n",
    "\n",
    "\t\tinput_ids_focal_method, attention_mask_focal_method = encode_text(focal_method_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_focal_cls, attention_mask_focal_cls = encode_text(focal_cls_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_response, attention_mask_response = encode_text(response, self.tokenizer_gpt)\n",
    "\n",
    "\t\tif idx_to_token:\n",
    "\t\t\treturn {\n",
    "\t\t\t\t'input_ids_focal_method': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_method),\n",
    "\t\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t\t'input_ids_focal_cls': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_cls),\n",
    "\t\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t\t'ids_response': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_response),\n",
    "\t\t\t\t'attention_mask_response': attention_mask_response\n",
    "\t\t\t}\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids_focal_method': input_ids_focal_method,\n",
    "\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t'input_ids_focal_cls': input_ids_focal_cls,\n",
    "\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t'ids_response': input_ids_response,\n",
    "\t\t\t'attention_mask_response': attention_mask_response\n",
    "\t\t}\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t'''Функция возвращает длину датасета. В качестве длины берется размер датасета по axis = 0'''\n",
    "\t\treturn self.code_dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем написанный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "code2test_dataset = Code2TestDataset(code_dataset=code_dataset,\n",
    "                                     tokenizer_code_bert=tokenizer_code_bert,\n",
    "                                     tokenizer_gpt=tokenizerGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<FUNC_TOKEN>', 'Ġdef', 'Ġarray', '_', 'to', '_', 's', 'log', '(', 'x', ':', 'ĠArray', ')', 'Ġ->', 'ĠSL', 'Array', ':', 'Ġreturn', 'Ġ(', 'j', 'np', '.', 'sign', '(', 'x', '),', 'Ġj', 'np', '.', 'log', '(', 'j', 'np', '.', 'abs', '(', 'x', ')))', 'Ġ', '<INFO_TOKEN>', 'Ġ', '<DESCRIPTION_TOKEN>', 'ĠCon', 'verts', 'Ġa', 'Ġregular', 'Ġarray', 'Ġinto', 'Ġ(', 'sign', ',', 'Ġlog', 'abs', ')', 'Ġform', '.', 'ĠAr', 'gs', ':', 'Ġx', 'Ġ(', 'Array', '):', 'Ġinput', 'Ġdata', '.', 'ĠReturns', ':', 'Ġ(', 'SL', 'Array', '):', 'Ġdata', 'Ġin', 'Ġform', 'Ġ(', 'sign', '(', 'x', '),', 'Ġlog', '(', 'abs', '(', 'x', ')))', 'Ġ', '<COMMENTS_TOKEN>', 'Ġ', '<AST_TOKEN>', 'ĠModule', '(', 'Ġbody', '=[', 'ĠFunction', 'Def', '(', 'Ġname', \"='\", 'array', '_', 'to', '_', 's', 'log', \"',\", 'Ġargs', '=', 'arg', 'uments', '(', 'Ġpos', 'only', 'args', '=[', '],', 'Ġargs', '=[', 'Ġarg', '(', 'Ġarg', \"='\", 'x', \"',\", 'Ġannotation', '=', 'Name', '(', 'id', \"='\", 'Array', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '))', '],', 'Ġk', 'w', 'only', 'args', '=[', '],', 'Ġk', 'w', '_', 'default', 's', '=[', '],', 'Ġdefaults', '=', '[]', '),', 'Ġbody', '=[', 'ĠEx', 'pr', '(', 'Ġvalue', '=', 'Con', 'stant', '(', 'value', \"='\", 'Con', 'verts', 'Ġa', 'Ġregular', 'Ġarray', 'Ġinto', 'Ġ(', 'sign', ',', 'Ġlog', 'abs', ')', 'Ġform', '.', '\\\\', 'n', '\\\\', 'n', 'ĠAr', 'gs', ':\\\\', 'n', 'Ġx', 'Ġ(', 'Array', '):', 'Ġinput', 'Ġdata', '.', '\\\\', 'n', '\\\\', 'n', 'ĠReturns', ':\\\\', 'n', 'Ġ(', 'SL', 'Array', '):', 'Ġdata', 'Ġin', 'Ġform', 'Ġ(', 'sign', '(', 'x', '),', 'Ġlog', '(', 'abs', '(', 'x', '))', ')\\\\', 'n', \"Ġ'\", ')),', 'ĠReturn', '(', 'Ġvalue', '=', 'T', 'uple', '(', 'Ġel', 'ts', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'sign', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'x', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '),', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'log', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'abs', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'x', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=[', '])', '],', 'Ġkeywords', '=[', '])', '],', 'Ġc', 'tx', '=', 'Load', '()', '))', '],', 'Ġdecor', 'ator', '_', 'list', '=[', '],', 'Ġreturns', '=', 'Name', '(', 'id', \"='\", 'SL', 'Array', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġtype', '_', 'params', '=[', '])', '],', 'Ġtype', '_', 'ign', 'ores', '=[', '])', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(490, idx_to_token=True)['input_ids_focal_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', '<CLS_TOKEN>', 'Ġfrom', 'Ġ.', 'ty', 'ping', 'Ġimport', 'ĠArray', ',', 'ĠSL', 'Array', ',', 'ĠArray', 'List', ',', 'ĠSL', 'Array', 'List', 'import', 'Ġj', 'ax', '.', 'n', 'umpy', 'Ġas', 'Ġj', 'np', 'Ġ', '<FUNC_TOKEN>', 'Ġ', '<INFO_TOKEN>', 'ĠModule', '(', 'Ġbody', '=[', 'ĠImport', 'From', '(', 'Ġmodule', \"='\", 'ty', 'ping', \"',\", 'Ġnames', '=[', 'Ġalias', '(', 'name', \"='\", 'Array', \"'),\", 'Ġalias', '(', 'name', \"='\", 'SL', 'Array', \"'),\", 'Ġalias', '(', 'name', \"='\", 'Array', 'List', \"'),\", 'Ġalias', '(', 'name', \"='\", 'SL', 'Array', 'List', \"')\", '],', 'Ġlevel', '=', '1', '),', 'ĠImport', '(', 'Ġnames', '=[', 'Ġalias', '(', 'name', \"='\", 'j', 'ax', '.', 'n', 'umpy', \"',\", 'Ġas', 'name', \"='\", 'j', 'np', \"')\", ']),', 'ĠFunction', 'Def', '(', 'Ġname', \"='\", 'array', '_', 'to', '_', 's', 'log', \"',\", 'Ġargs', '=', 'arg', 'uments', '(', 'Ġpos', 'only', 'args', '=[', '],', 'Ġargs', '=[', 'Ġarg', '(', 'Ġarg', \"='\", 'x', \"',\", 'Ġannotation', '=', 'Name', '(', 'id', \"='\", 'Array', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '))', '],', 'Ġk', 'w', 'only', 'args', '=[', '],', 'Ġk', 'w', '_', 'default', 's', '=[', '],', 'Ġdefaults', '=', '[]', '),', 'Ġbody', '=[', 'ĠEx', 'pr', '(', 'Ġvalue', '=', 'Con', 'stant', '(', 'value', \"='\", 'Con', 'verts', 'Ġa', 'Ġregular', 'Ġarray', 'Ġinto', 'Ġ(', 'sign', ',', 'Ġlog', 'abs', ')', 'Ġform', '.', '\\\\', 'n', 'ĠAr', 'gs', ':\\\\', 'n', 'Ġx', 'Ġ(', 'Array', '):', 'Ġinput', 'Ġdata', '.', '\\\\', 'n', 'ĠReturns', ':\\\\', 'n', 'Ġ(', 'SL', 'Array', '):', 'Ġdata', 'Ġin', 'Ġform', 'Ġ(', 'sign', '(', 'x', '),', 'Ġlog', '(', 'abs', '(', 'x', '))', ')\\\\', 'n', \"Ġ'\", ')),', 'ĠReturn', '(', 'Ġvalue', '=', 'T', 'uple', '(', 'Ġel', 'ts', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'sign', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'x', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '),', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'log', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'j', 'np', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'abs', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'x', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=[', '])', '],', 'Ġkeywords', '=[', '])', '],', 'Ġc', 'tx', '=', 'Load', '()', '))', '],', 'Ġdecor', 'ator', '_', 'list', '=[', '],', 'Ġreturns', '=', 'Name', '(', 'id', \"='\", 'SL', 'Array', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġtype', '_', 'params', '=[', '])', '],', 'Ġtype', '_', 'ign', 'ores', '=[', '])', 'Ġ', '<AST_TOKEN>', 'ĠModule', '(', 'Ġbody', '=[', 'ĠImport', 'From', '(', 'Ġmodule', \"='\", 'ty', 'ping', \"',\", 'Ġnames', '=[', 'Ġalias', '(', 'name', \"='\", 'Array', \"'),\", 'Ġalias', '(', 'name', \"='\", 'SL', 'Array', \"'),\", 'Ġalias', '(', 'name', \"='\", 'Array', 'List', \"'),\", 'Ġalias', '(', 'name', \"='\", 'SL', 'Array', 'List', \"')\", '],', 'Ġlevel', '=', '1', '),', 'ĠImport', '(', 'Ġnames', '=[', 'Ġalias', '(', 'name', \"='\", 'j', 'ax', '.', 'n', 'umpy', \"',\", 'Ġas', 'name', \"='\", 'j', 'np', \"')\", ']),', 'ĠFunction', 'Def', '(', 'Ġname', \"='\", 'array', '_', 'to', '_', 's', 'log', \"',\", 'Ġargs', '=', 'arg', 'uments', '(', 'Ġpos', 'only', 'args', '=[', '],', 'Ġargs', '=[', 'Ġarg', '(', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(490, idx_to_token=True)['input_ids_focal_cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['from', 'Ġtests', '.', 'test', '_', 'utils', 'Ġimport', 'Ġassert', '_', 'py', 'tree', '_', 'all', 'close', 'Ċ', 'import', 'Ġv', 'mc', 'net', '.', 'utils', '.', 's', 'log', '_', 'help', 'ers', 'Ġas', 'Ġhelpers', 'Ċ', 'from', 'Ġtyping', 'Ġimport', 'ĠT', 'uple', 'Ċ', 'from', 'Ġv', 'mc', 'net', '.', 'utils', '.', 'ty', 'ping', 'Ġimport', 'ĠArray', ',', 'ĠSL', 'Array', 'Ċ', 'import', 'Ġj', 'ax', '.', 'n', 'umpy', 'Ġas', 'Ġj', 'np', 'Ċ', 'Ċ', 'def', 'Ġ_', 'get', '_', 'array', '_', 'and', '_', 's', 'log', '_', 'vals', '()', 'Ġ->', 'ĠT', 'uple', '[', 'Array', ',', 'ĠSL', 'Array', ']:', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġv', 'als', 'Ġ=', 'Ġj', 'np', '.', 'array', '([', 'j', 'np', '.', 'e', ',', 'Ġ-', 'j', 'np', '.', 'e', '**', '0', '.', '5', ',', 'Ġ0', ',', 'Ġ1', '])', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsigns', 'Ġ=', 'Ġj', 'np', '.', 'array', '([', '1', ',', 'Ġ-', '1', ',', 'Ġ0', ',', 'Ġ1', '])', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġlogs', 'Ġ=', 'Ġj', 'np', '.', 'array', '([', '1', ',', 'Ġ0', '.', '5', ',', 'Ġ-', 'j', 'np', '.', 'inf', ',', 'Ġ0', '])', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġreturn', 'Ġ(', 'vals', ',', 'Ġ(', 'sign', 's', ',', 'Ġlogs', '))', 'Ċ', 'def', 'Ġtest', '_', 'array', '_', 'to', '_', 's', 'log', '():', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ\"\"\"', 'Test', 'Ġconversion', 'Ġfrom', 'Ġarray', 'Ġto', 'Ġslog', 'Ġtuple', '.\"', '\"\"', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġ(', 'vals', ',', 'Ġexpected', '_', 's', 'log', 's', ')', 'Ġ=', 'Ġ_', 'get', '_', 'array', '_', 'and', '_', 's', 'log', '_', 'vals', '()', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġsl', 'ogs', 'Ġ=', 'Ġhelpers', '.', 'array', '_', 'to', '_', 's', 'log', '(', 'vals', ')', 'Ċ', 'Ġ', 'Ġ', 'Ġ', 'Ġassert', '_', 'py', 'tree', '_', 'all', 'close', '(', 's', 'log', 's', ',', 'Ġexpected', '_', 's', 'log', 's', ')', 'Ċ', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(490, idx_to_token=True)['ids_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1,  ..., 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset[490]['attention_mask_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(490, idx_to_token=True)['attention_mask_focal_method'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(490, idx_to_token=True)['attention_mask_focal_cls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина датасета составляет: 280458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Длина датасета составляет: {len(code2test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё работает корректно! Следующим шагом необходимо разбить датасет на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_datasets(dataset_cls = Code2TestDataset,\n",
    "\t\t\t\tmax_length = 512,\n",
    "\t\t\t\tdata = code_dataset,\n",
    "\t\t\t\ttokenizer_code_bert = tokenizer_code_bert,\n",
    "\t\t\t\ttokenizer_gpt = tokenizerGPT,\n",
    "\t\t\t\ttrain_size = 0.7):\n",
    "\t'''\n",
    "\tФункция get_datasets() возвращает train и val датасеты на основе конструктора AccentDataset, делая train_val_spilt\n",
    "\t\n",
    "\tПараметры:\n",
    "\t-dataset_cls: класс датасета, конструктор которого будет вызываться (default: Code2TestDataset)\n",
    "\t-max_length: максимальная статья последовательности токенов\n",
    "\t-data: датасает pd.DataFrame (default: code_dataset)\n",
    "\t-tokenizer: токенизатор (default: tokenizer_code_bert)\n",
    "\t-train_size: размер тренировочной выборки (default: 0.7)\n",
    "\t\n",
    "\t'''\n",
    "\t\n",
    "\tdataset = dataset_cls(code_dataset = data,\n",
    "\t\t\t\t\t   \ttokenizer_code_bert = tokenizer_code_bert,\n",
    "\t\t\t\t\t\ttokenizer_gpt=tokenizer_gpt,\n",
    "\t\t\t\t\t\tmax_length=max_length)\n",
    "\t\n",
    "\ttrain_size = int(train_size * len(dataset))\n",
    "\tval_size = len(dataset) - train_size\n",
    "\ttrain_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\treturn train_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset = get_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем полученные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество данных в train и val выборках соответственно: (196320, 84138)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество данных в train и val выборках соответственно: {len(train_dataset), len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(tokens_ids, tokenizer):\n",
    "\t'''Декодирование последовательности токенов'''\n",
    "\tcode_bert_decoded = tokenizer.decode(tokens_ids)\n",
    "\tprint(f\"Декодированная строка: {code_bert_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тренировочный сэмпл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><FUNC_TOKEN> def score_to_label(pred_scores, outliers_fraction=0.1): pred_scores = column_or_1d(pred_scores) check_parameter(outliers_fraction, 0, 1) threshold = percentile(pred_scores, 100 * (1 - outliers_fraction)) pred_labels = (pred_scores > threshold).astype('int') return pred_labels <INFO_TOKEN> <DESCRIPTION_TOKEN> Turn raw outlier outlier scores to binary labels (0 or 1). Parameters ---------- pred_scores : list or numpy array of shape (n_samples,) Raw outlier scores. Outliers are assumed have larger values. outliers_fraction : float in (0,1) Percentage of outliers. Returns ------- outlier_labels : numpy array of shape (n_samples,) For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. <COMMENTS_TOKEN> check input values <AST_TOKEN> Module( body=[ FunctionDef( name='score_to_label', args=arguments( posonlyargs=[], args=[ arg(arg='pred_scores'), arg(arg='outliers_fraction')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=0.1)]), body=[ Expr( value=Constant(value='Turn raw outlier outlier scores to binary labels (0 or 1).\\n\\n Parameters\\n ----------\\n pred_scores : list or numpy array of shape (n_samples,)\\n Raw outlier scores. Outliers are assumed have larger values.\\n\\n outliers_fraction : float in (0,1)\\n Percentage of outliers.\\n\\n Returns\\n -------\\n outlier_labels : numpy array of shape (n_samples,)\\n For each observation, tells whether or not\\n it should be considered as an outlier according to the\\n fitted model. Return the outlier probability, ranging\\n in [0,1].\\n ')), Assign( targets=[ Name(id='pred_scores', ctx=Store())], value=Call( func=Name(id='column_or_1d',</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataset[0]['input_ids_focal_method'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><CLS_TOKEN> from sklearn.utils import column_or_1dfrom pyod.utils.utility import check_parameterfrom numpy import percentile <FUNC_TOKEN> <INFO_TOKEN> Module( body=[ ImportFrom( module='sklearn.utils', names=[ alias(name='column_or_1d')], level=0), ImportFrom( module='pyod.utils.utility', names=[ alias(name='check_parameter')], level=0), ImportFrom( module='numpy', names=[ alias(name='percentile')], level=0), FunctionDef( name='score_to_label', args=arguments( posonlyargs=[], args=[ arg(arg='pred_scores'), arg(arg='outliers_fraction')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=0.1)]), body=[ Expr( value=Constant(value='Turn raw outlier outlier scores to binary labels (0 or 1).\\n Parameters\\n ----------\\n pred_scores : list or numpy array of shape (n_samples,)\\n Raw outlier scores. Outliers are assumed have larger values.\\n outliers_fraction : float in (0,1)\\n Percentage of outliers.\\n Returns\\n -------\\n outlier_labels : numpy array of shape (n_samples,)\\n For each observation, tells whether or not\\n it should be considered as an outlier according to the\\n fitted model. Return the outlier probability, ranging\\n in [0,1].\\n ')), Assign( targets=[ Name(id='pred_scores', ctx=Store())], value=Call( func=Name(id='column_or_1d', ctx=Load()), args=[ Name(id='pred_scores', ctx=Load())], keywords=[])), Expr( value=Call( func=Name(id='check_parameter', ctx=Load()), args=[ Name(id='outliers_fraction', ctx=Load()), Constant(value=0), Constant(value=1)], keywords=[])), Assign( targets=[ Name(id='threshold', ctx=Store())], value=Call( func=Name(id='percentile', ctx=Load()),</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataset[0]['input_ids_focal_cls'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: import unittest\n",
      "from numpy.testing import assert_allclose\n",
      "from utils.utility import score_to_label\n",
      "\n",
      "class TestMetrics(unittest.TestCase):\n",
      "    def test_score_to_label(self):\n",
      "        manual_scores = [0.1, 0.4, 0.2, 0.3, 0.5, 0.9, 0.7, 1, 0.8, 0.6]\n",
      "        labels = score_to_label(manual_scores, outliers_fraction=0.1)\n",
      "        assert_allclose(labels, [0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "        labels = score_to_label(manual_scores, outliers_fraction=0.3)\n",
      "        assert_allclose(labels, [0, 0, 0, 0, 0, 1, 0, 1, 1, 0])\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataset[0]['ids_response'], tokenizerGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Валидационный сэмпл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><FUNC_TOKEN> def valid_actions(self): num_raises_so_far = sum([p.raised for p in self.players]) if num_raises_so_far == self.num_players: return ['F', 'C'] else: if self.round == 0: return ['F', 'C', '2R'] else: return ['F', 'C', '4R'] <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='valid_actions', args=arguments( posonlyargs=[], args=[ arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[ Assign( targets=[ Name(id='num_raises_so_far', ctx=Store())], value=Call( func=Name(id='sum', ctx=Load()), args=[ ListComp( elt=Attribute( value=Name(id='p', ctx=Load()), attr='raised', ctx=Load()), generators=[ comprehension( target=Name(id='p', ctx=Store()), iter=Attribute( value=Name(id='self', ctx=Load()), attr='players', ctx=Load()), ifs=[], is_async=0)])], keywords=[])), If( test=Compare( left=Name(id='num_raises_so_far', ctx=Load()), ops=[ Eq()], comparators=[ Attribute( value=Name(id='self', ctx=Load()), attr='num_players', ctx=Load())]), body=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C')], ctx=Load()))], orelse=[ If( test=Compare( left=Attribute( value=Name(id='self', ctx=Load()), attr='round', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value=0)]), body=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C'), Constant(value='2R')], ctx=Load()))], orelse=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C'), Constant(value='4R')], ctx=Load()</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(val_dataset[0]['input_ids_focal_method'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(val_dataset[0]['input_ids_focal_cls'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: from leduc.state import Leduc\n",
      "from leduc.state import State\n",
      "\n",
      "def test_valid_actions():\n",
      "    state = State([1, 2, 3], 2, None)\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C', '1R'], actions\n",
      "    state.take('C')\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C', '1R'], actions\n",
      "    state = State([1, 2, 3], 2, None)\n",
      "    state.take('1R')\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C'], actions\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(val_dataset[0]['ids_response'], tokenizerGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректно работает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее получим DataLoader, по которому будем итерироваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(train_dataset = train_dataset,\n",
    "\t\t\tval_dataset = val_dataset,\n",
    "\t\t\tshuffle_train = True,\n",
    "\t\t\tshuffle_val = False,\n",
    "\t\t\tbatch_size = 32):\n",
    "\t\n",
    "\t'''\n",
    "\tФункция get_loaders() для получения train, val даталоадеров\n",
    "\n",
    "\tПараметры:\n",
    "\t-train_dataset: тренировочный датасет (default: train_dataset)\n",
    "\t-val_dataset: валидационный датасет (default: val_dataset)\n",
    "\t-shuffle_train: флаг перемешивания для train (default: True)\n",
    "\t-shuffle_val: флаг перемешивания для val (default: False)\n",
    "\t-batch_size: размер батча данных (default: 32)\n",
    "\t'''\n",
    "\t\n",
    "\t# train_dataloader\n",
    "\ttrain_dataloader = DataLoader(\n",
    "\t\t\ttrain_dataset,   \n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_train,\n",
    "\t\t)\n",
    "\n",
    "\t# validation_dataloader\n",
    "\tvalidation_dataloader = DataLoader(\n",
    "\t\t\tval_dataset, \n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_val,\n",
    "\t\t)\n",
    "\t\n",
    "\t# Возвращаем даталоадеры\n",
    "\treturn train_dataloader, validation_dataloader\n",
    "\n",
    "train_dataloader, validation_dataloader = get_loaders(batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><FUNC_TOKEN> def score_to_label(pred_scores, outliers_fraction=0.1): pred_scores = column_or_1d(pred_scores) check_parameter(outliers_fraction, 0, 1) threshold = percentile(pred_scores, 100 * (1 - outliers_fraction)) pred_labels = (pred_scores > threshold).astype('int') return pred_labels <INFO_TOKEN> <DESCRIPTION_TOKEN> Turn raw outlier outlier scores to binary labels (0 or 1). Parameters ---------- pred_scores : list or numpy array of shape (n_samples,) Raw outlier scores. Outliers are assumed have larger values. outliers_fraction : float in (0,1) Percentage of outliers. Returns ------- outlier_labels : numpy array of shape (n_samples,) For each observation, tells whether or not it should be considered as an outlier according to the fitted model. Return the outlier probability, ranging in [0,1]. <COMMENTS_TOKEN> check input values <AST_TOKEN> Module( body=[ FunctionDef( name='score_to_label', args=arguments( posonlyargs=[], args=[ arg(arg='pred_scores'), arg(arg='outliers_fraction')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=0.1)]), body=[ Expr( value=Constant(value='Turn raw outlier outlier scores to binary labels (0 or 1).\\n\\n Parameters\\n ----------\\n pred_scores : list or numpy array of shape (n_samples,)\\n Raw outlier scores. Outliers are assumed have larger values.\\n\\n outliers_fraction : float in (0,1)\\n Percentage of outliers.\\n\\n Returns\\n -------\\n outlier_labels : numpy array of shape (n_samples,)\\n For each observation, tells whether or not\\n it should be considered as an outlier according to the\\n fitted model. Return the outlier probability, ranging\\n in [0,1].\\n ')), Assign( targets=[ Name(id='pred_scores', ctx=Store())], value=Call( func=Name(id='column_or_1d',</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataloader.dataset[0]['input_ids_focal_method'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><CLS_TOKEN> from sklearn.utils import column_or_1dfrom pyod.utils.utility import check_parameterfrom numpy import percentile <FUNC_TOKEN> <INFO_TOKEN> Module( body=[ ImportFrom( module='sklearn.utils', names=[ alias(name='column_or_1d')], level=0), ImportFrom( module='pyod.utils.utility', names=[ alias(name='check_parameter')], level=0), ImportFrom( module='numpy', names=[ alias(name='percentile')], level=0), FunctionDef( name='score_to_label', args=arguments( posonlyargs=[], args=[ arg(arg='pred_scores'), arg(arg='outliers_fraction')], kwonlyargs=[], kw_defaults=[], defaults=[ Constant(value=0.1)]), body=[ Expr( value=Constant(value='Turn raw outlier outlier scores to binary labels (0 or 1).\\n Parameters\\n ----------\\n pred_scores : list or numpy array of shape (n_samples,)\\n Raw outlier scores. Outliers are assumed have larger values.\\n outliers_fraction : float in (0,1)\\n Percentage of outliers.\\n Returns\\n -------\\n outlier_labels : numpy array of shape (n_samples,)\\n For each observation, tells whether or not\\n it should be considered as an outlier according to the\\n fitted model. Return the outlier probability, ranging\\n in [0,1].\\n ')), Assign( targets=[ Name(id='pred_scores', ctx=Store())], value=Call( func=Name(id='column_or_1d', ctx=Load()), args=[ Name(id='pred_scores', ctx=Load())], keywords=[])), Expr( value=Call( func=Name(id='check_parameter', ctx=Load()), args=[ Name(id='outliers_fraction', ctx=Load()), Constant(value=0), Constant(value=1)], keywords=[])), Assign( targets=[ Name(id='threshold', ctx=Store())], value=Call( func=Name(id='percentile', ctx=Load()),</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataloader.dataset[0]['input_ids_focal_cls'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: import unittest\n",
      "from numpy.testing import assert_allclose\n",
      "from utils.utility import score_to_label\n",
      "\n",
      "class TestMetrics(unittest.TestCase):\n",
      "    def test_score_to_label(self):\n",
      "        manual_scores = [0.1, 0.4, 0.2, 0.3, 0.5, 0.9, 0.7, 1, 0.8, 0.6]\n",
      "        labels = score_to_label(manual_scores, outliers_fraction=0.1)\n",
      "        assert_allclose(labels, [0, 0, 0, 0, 0, 0, 0, 1, 0, 0])\n",
      "        labels = score_to_label(manual_scores, outliers_fraction=0.3)\n",
      "        assert_allclose(labels, [0, 0, 0, 0, 0, 1, 0, 1, 1, 0])\n",
      "if __name__ == '__main__':\n",
      "    unittest.main()\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(train_dataloader.dataset[0]['ids_response'], tokenizerGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><FUNC_TOKEN> def valid_actions(self): num_raises_so_far = sum([p.raised for p in self.players]) if num_raises_so_far == self.num_players: return ['F', 'C'] else: if self.round == 0: return ['F', 'C', '2R'] else: return ['F', 'C', '4R'] <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='valid_actions', args=arguments( posonlyargs=[], args=[ arg(arg='self')], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[ Assign( targets=[ Name(id='num_raises_so_far', ctx=Store())], value=Call( func=Name(id='sum', ctx=Load()), args=[ ListComp( elt=Attribute( value=Name(id='p', ctx=Load()), attr='raised', ctx=Load()), generators=[ comprehension( target=Name(id='p', ctx=Store()), iter=Attribute( value=Name(id='self', ctx=Load()), attr='players', ctx=Load()), ifs=[], is_async=0)])], keywords=[])), If( test=Compare( left=Name(id='num_raises_so_far', ctx=Load()), ops=[ Eq()], comparators=[ Attribute( value=Name(id='self', ctx=Load()), attr='num_players', ctx=Load())]), body=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C')], ctx=Load()))], orelse=[ If( test=Compare( left=Attribute( value=Name(id='self', ctx=Load()), attr='round', ctx=Load()), ops=[ Eq()], comparators=[ Constant(value=0)]), body=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C'), Constant(value='2R')], ctx=Load()))], orelse=[ Return( value=List( elts=[ Constant(value='F'), Constant(value='C'), Constant(value='4R')], ctx=Load()</s>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(validation_dataloader.dataset[0]['input_ids_focal_method'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: <s><CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(validation_dataloader.dataset[0]['input_ids_focal_cls'], tokenizer_code_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Декодированная строка: from leduc.state import Leduc\n",
      "from leduc.state import State\n",
      "\n",
      "def test_valid_actions():\n",
      "    state = State([1, 2, 3], 2, None)\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C', '1R'], actions\n",
      "    state.take('C')\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C', '1R'], actions\n",
      "    state = State([1, 2, 3], 2, None)\n",
      "    state.take('1R')\n",
      "    actions = state.valid_actions()\n",
      "    assert actions == ['F', 'C'], actions\n",
      "<PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    }
   ],
   "source": [
    "decode_sequence(validation_dataloader.dataset[0]['ids_response'], tokenizerGPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка итерирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/98160 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректно отрабатывает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, собираем архитектуру и готовимся обучать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50271, 768, padding_idx=1)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_code_bert = AutoModel.from_pretrained(\"microsoft/codebert-base\", output_hidden_states= True)\n",
    "model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает модель codeBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "\t\n",
    "\t# Проверка корректности работы\n",
    "\tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\t\n",
    "\toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "\tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "\tprint(last_hidden_state_code_bert.size())\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, для каждого токена мы получим свое закодированное значение размерности 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50258, 768)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "modelGPT2Path = \"gpt2\"\n",
    "config = AutoConfig.from_pretrained(modelGPT2Path, is_decoder=True, add_cross_attention= True)\n",
    "config.add_cross_attention = True  # Включение cross-attention\n",
    "\n",
    "modelGPT2 = AutoModel.from_pretrained(modelGPT2Path, config=config)\n",
    "modelGPT2.resize_token_embeddings(len(tokenizerGPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает модель GPTBigCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 768])\n",
      "torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "\tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\t\n",
    "\toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "\tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "\n",
    "\tprint(last_hidden_state_code_bert.size())\n",
    "\t\n",
    "\t# Проверка корректности работы\n",
    "\tresponse_input_ids = batch['ids_response'].to(device)\n",
    "\tresponse_input_mask = batch['attention_mask_response'].to(device)\n",
    "\tgpt_output = modelGPT2(input_ids=response_input_ids, \n",
    "\t\t\t\t\t\t\t  attention_mask=response_input_mask, \n",
    "\t\t\t\t\t\t\t  encoder_hidden_states = last_hidden_state_code_bert)\n",
    "\tprint(gpt_output['last_hidden_state'].size())\n",
    "\t\n",
    "\t\n",
    "\t# outputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "\t# last_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "\t# print(last_hidden_state_code_bert.size())\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, как-то худо-бедно всё это дело запускается. Пробуем строить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class LargeCodeModel(nn.Module):\n",
    "\t'''Класс для сложной языковой модели, которая обрабатывает входной код'''\n",
    "\tdef __init__(self, bert_model_name, gpt2_name):\n",
    "\t\tsuper(LargeCodeModel, self).__init__()\n",
    "\t\t\n",
    "\t\tself.bert1 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.bert2 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.tokenizer_code_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "\t\tself.new_special_tokens = ['<FUNC_TOKEN>',\n",
    "            '<INFO_TOKEN>',\n",
    "            '<CLS_TOKEN>', \n",
    "            '<AST_TOKEN>', \n",
    "            '<DESCRIPTION_TOKEN>',\n",
    "            '<COMMENTS_TOKEN>']\n",
    "\n",
    "\t\tself.special_tokens_dict = {\n",
    "\t\t\t'additional_special_tokens': new_special_tokens\n",
    "\t\t}\n",
    "\n",
    "\t\tself.tokenizer_code_bert.add_special_tokens(self.special_tokens_dict)\n",
    "\t\tself.bert1.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\t\tself.bert2.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\n",
    "\t\tself.gpt2_config = AutoConfig.from_pretrained(gpt2_name, is_decoder=True, add_cross_attention= True)\n",
    "\t\tself.gpt2_config.add_cross_attention = True  # Включение cross-attention\n",
    "\t\tself.tokenizerGPT = AutoTokenizer.from_pretrained(gpt2_name)\n",
    "\t\tself.tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\t\tself.gpt2 = GPT2LMHeadModel.from_pretrained(modelGPT2Path, config=config)\n",
    "\t\tself.gpt2.resize_token_embeddings(len(self.tokenizerGPT))\n",
    "\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.bert1.config.hidden_size)\n",
    "\n",
    "\t\tself.projection = nn.Linear(\n",
    "            self.bert1.config.hidden_size + self.bert2.config.hidden_size,\n",
    "            self.gpt2.config.hidden_size\n",
    "        )\n",
    "\n",
    "\t# forward call\n",
    "\tdef forward(self, focal_method_input_ids, \n",
    "\t\t\t \t\t\tfocal_method_attention_masks, \n",
    "\t\t\t\t\t\tfocal_cls_input_ids,\n",
    "\t\t\t\t\t\tfocal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks):\n",
    "\t\t\n",
    "\t\tprint(focal_method_input_ids.size())\n",
    "\t\tprint(focal_method_attention_masks.size())\n",
    "\t\t\n",
    "\t\tbert1_outputs = self.bert1(focal_method_input_ids, focal_method_attention_masks)\n",
    "\t\tlast_hidden_state_bert1 = bert1_outputs['last_hidden_state']\n",
    "\n",
    "\t\tbert2_outputs = self.bert2(focal_cls_input_ids, focal_cls_attention_masks)\n",
    "\t\tlast_hidden_state_bert2 = bert2_outputs['last_hidden_state']\n",
    "\n",
    "\t\t# print(last_hidden_state_bert1.size())\n",
    "\t\t# print(last_hidden_state_bert2.size())\n",
    "\n",
    "\t\tconcat_hidden_states = torch.cat([last_hidden_state_bert1, last_hidden_state_bert2], dim=1)\n",
    "\n",
    "\t\t# print(concat_hidden_states.size())\n",
    "\n",
    "\t\t# LayerNormalization\n",
    "\t\tnormalized_hidden_states = self.layer_norm(concat_hidden_states)\n",
    "\n",
    "\t\t# Для BatchNorm\n",
    "\t\t# batch_norm_input = concat_hidden_states.view(-1, 768)\n",
    "\t\t# normalized_hidden_states = self.batch_norm(batch_norm_input)\n",
    "\t\t# normalized_hidden_states = normalized_hidden_states.view(2, 1024, 768)\n",
    "\t\t# print(normalized_hidden_states.size())\n",
    "\t\t# print(torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1).size())\n",
    "\t\t# print(response_ids.size())\n",
    "\t\t# print(response_input_mask.size())\n",
    "\n",
    "\t\t# print(response_attention_masks.size())\n",
    "\t\t\n",
    "\t\tgpt2_outputs = self.gpt2(\n",
    "            input_ids=response_ids,\n",
    "            attention_mask=response_attention_masks,\n",
    "            encoder_hidden_states=normalized_hidden_states,\n",
    "            encoder_attention_mask=torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1),\n",
    "\t\t\tlabels=response_ids\n",
    "        )\n",
    "\n",
    "\t\treturn gpt2_outputs\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлаживаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CodeModel = LargeCodeModel(bert_model_name=\"microsoft/codebert-base\",\n",
    "                           gpt2_name=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512])\n",
      "torch.Size([2, 512])\n",
      "torch.Size([2, 1024, 768])\n",
      "torch.Size([2, 1024, 768])\n",
      "torch.Size([2, 1024])\n",
      "torch.Size([2, 1024, 50258])\n",
      "tensor(15.0449, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_dataloader):\n",
    "    \n",
    "\tfocal_method_input_ids = batch['input_ids_focal_method']\n",
    "\tfocal_method_attention_masks = batch['attention_mask_focal_method']\n",
    "\n",
    "\tfocal_cls_input_ids = batch['input_ids_focal_cls']\n",
    "\tfocal_cls_attention_masks = batch['attention_mask_focal_cls']\n",
    "\n",
    "\tresponse_ids = batch['ids_response']\n",
    "\tresponse_attention_masks = batch['attention_mask_response']\n",
    "\n",
    "\toutput = CodeModel(focal_method_input_ids, focal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks)\n",
    "\t\n",
    "\tprint(output['logits'].size())\n",
    "\tprint(output['loss'])\n",
    "\tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необходимо объявить функцию train-val loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
