{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Архитектура модели анализа кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном файле проводится анализ архитектуры модели, токенизатора и подготовка к обучению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:13.382026Z",
     "iopub.status.busy": "2024-12-12T06:15:13.381216Z",
     "iopub.status.idle": "2024-12-12T06:15:30.190140Z",
     "shell.execute_reply": "2024-12-12T06:15:30.189378Z",
     "shell.execute_reply.started": "2024-12-12T06:15:13.381983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from torch.utils.tensorboard import summary, writer, SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устанавливаем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:30.193189Z",
     "iopub.status.busy": "2024-12-12T06:15:30.192033Z",
     "iopub.status.idle": "2024-12-12T06:15:30.201396Z",
     "shell.execute_reply": "2024-12-12T06:15:30.200706Z",
     "shell.execute_reply.started": "2024-12-12T06:15:30.193156Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее считываем исходный датасет и немного дорабатываем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:30.202945Z",
     "iopub.status.busy": "2024-12-12T06:15:30.202362Z",
     "iopub.status.idle": "2024-12-12T06:15:52.551962Z",
     "shell.execute_reply": "2024-12-12T06:15:52.551214Z",
     "shell.execute_reply.started": "2024-12-12T06:15:30.202918Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code_dataset = pd.read_parquet('/kaggle/input/upd-parquet-dataset/upd_code_dataset.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.553687Z",
     "iopub.status.busy": "2024-12-12T06:15:52.553301Z",
     "iopub.status.idle": "2024-12-12T06:15:52.573492Z",
     "shell.execute_reply": "2024-12-12T06:15:52.572692Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.553649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "      <th>input_string_focal_method</th>\n",
       "      <th>input_string_focal_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0                                        <AST_TOKEN>   \n",
       "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "3                                        <AST_TOKEN>   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
       "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
       "\n",
       "                                      focal_cls_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "3                                       <INFO_TOKEN>   \n",
       "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
       "\n",
       "                           input_string_focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                              input_string_focal_cls  \n",
       "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.575470Z",
     "iopub.status.busy": "2024-12-12T06:15:52.575217Z",
     "iopub.status.idle": "2024-12-12T06:15:52.642582Z",
     "shell.execute_reply": "2024-12-12T06:15:52.641652Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.575446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code_dataset = code_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, переходим к анализу архитектур нейросетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решено использовать подход, основанный на обучении (fine-tuning) нейросети CodeBERT, в основе которой лежит модель RoBERTa. Далее будем использовать метамодель в виде декодера (CodeGen или GPTBigCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.649211Z",
     "iopub.status.busy": "2024-12-12T06:15:52.648956Z",
     "iopub.status.idle": "2024-12-12T06:15:54.090651Z",
     "shell.execute_reply": "2024-12-12T06:15:54.089570Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.649177Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:54.092773Z",
     "iopub.status.busy": "2024-12-12T06:15:54.092032Z",
     "iopub.status.idle": "2024-12-12T06:15:54.263313Z",
     "shell.execute_reply": "2024-12-12T06:15:54.262376Z",
     "shell.execute_reply.started": "2024-12-12T06:15:54.092724Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизаторы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:54.264744Z",
     "iopub.status.busy": "2024-12-12T06:15:54.264409Z",
     "iopub.status.idle": "2024-12-12T06:15:58.345038Z",
     "shell.execute_reply": "2024-12-12T06:15:58.344170Z",
     "shell.execute_reply.started": "2024-12-12T06:15:54.264710Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7488fdae613d4515aba8c54bb72963bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a801330eb277404498e292761a4a612d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1d1f6ef20e34f28a572605fdc06825a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "166aa3e758bf40d88241c8c0d95493be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d401a214433443d39bafa92c2ed54041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af314443f6104c9699392c320d9b8d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7657689df5746de994979033d33e71b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7160e5c61954835bb896e6a59926f1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a87bc7843644d4d96e89eea38c82903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c6299d9803544108830b4dd9e19b5d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_code_bert = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
    "tokenizerGPT = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим как работает базовый токенизатор для CodeBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед этим добавим новые служебные токены:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.346632Z",
     "iopub.status.busy": "2024-12-12T06:15:58.346241Z",
     "iopub.status.idle": "2024-12-12T06:15:58.353377Z",
     "shell.execute_reply": "2024-12-12T06:15:58.352545Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.346593Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_special_tokens = ['<FUNC_TOKEN>',\n",
    "            '<INFO_TOKEN>',\n",
    "            '<CLS_TOKEN>', \n",
    "            '<AST_TOKEN>', \n",
    "            '<DESCRIPTION_TOKEN>',\n",
    "            '<COMMENTS_TOKEN>']\n",
    "\n",
    "special_tokens_dict = {\n",
    "    'additional_special_tokens': new_special_tokens\n",
    "}\n",
    "\n",
    "tokenizer_code_bert.add_special_tokens(special_tokens_dict)\n",
    "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.354761Z",
     "iopub.status.busy": "2024-12-12T06:15:58.354472Z",
     "iopub.status.idle": "2024-12-12T06:15:58.369240Z",
     "shell.execute_reply": "2024-12-12T06:15:58.368554Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.354735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def tokenization_example(input_str: str):\n",
    "\t'''Функция отображения результатов токенизации'''\n",
    "\tcode_bert_tokens_example = tokenizer_code_bert.tokenize(input_str)\n",
    "\tcode_bert_tokens_ids = tokenizer_code_bert.convert_tokens_to_ids(code_bert_tokens_example)\n",
    "\tcode_bert_decoded = tokenizer_code_bert.decode(code_bert_tokens_ids)\n",
    "\tprint(f\"Длина закодированной последовательности: {len(code_bert_tokens_example)}\")\n",
    "\tprint(f\"Как выглядят токены исходной фразы: {code_bert_tokens_example}\")\n",
    "\tprint(f\"Индексы токенов: {code_bert_tokens_ids}\")\n",
    "\tprint(f\"Декодированная строка: {code_bert_decoded}\")\n",
    "\n",
    "# tokenization_example(code_dataset['input_string_focal_method'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.370424Z",
     "iopub.status.busy": "2024-12-12T06:15:58.370199Z",
     "iopub.status.idle": "2024-12-12T06:15:58.389555Z",
     "shell.execute_reply": "2024-12-12T06:15:58.388650Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.370401Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "      <th>input_string_focal_method</th>\n",
       "      <th>input_string_focal_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0                                        <AST_TOKEN>   \n",
       "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "3                                        <AST_TOKEN>   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
       "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
       "\n",
       "                                      focal_cls_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "3                                       <INFO_TOKEN>   \n",
       "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
       "\n",
       "                           input_string_focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                              input_string_focal_cls  \n",
       "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необхоимо описать класс Dataset для нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.391373Z",
     "iopub.status.busy": "2024-12-12T06:15:58.390891Z",
     "iopub.status.idle": "2024-12-12T06:15:58.400975Z",
     "shell.execute_reply": "2024-12-12T06:15:58.400275Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.391333Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Code2TestDataset(Dataset):\n",
    "\t'''Класс датасет для задачи генерации тестов'''\n",
    "\n",
    "\tdef __init__(self, code_dataset, tokenizer_code_bert, tokenizer_gpt, max_length=512):\n",
    "\t\t'''\n",
    "\t\tКонструктор датасета\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- code_dataset: датасет pd.DataFrame\n",
    "\t\t- tokenizer_code_bert: токенизатор code_bert\n",
    "\t\t- tokenizer_gpt: токенизатор gpt\n",
    "\t\t- max_length: максимальная длина последовательности (default: 512)\n",
    "\t\t'''\n",
    "\t\tself.code_dataset = code_dataset\n",
    "\t\tself.tokenizer_code_bert = tokenizer_code_bert\n",
    "\t\tself.tokenizer_gpt = tokenizer_gpt\n",
    "\t\tself.max_length = max_length\n",
    "\n",
    "\tdef __getitem__(self, idx, idx_to_token=False):\n",
    "\t\t'''\n",
    "\t\tGet-метод - возвращает сэмпл по индексу\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- idx: индекс\n",
    "\t\t- idx_to_token: флаг для отображения токенов из индексов (default: False)\n",
    "\t\t'''\n",
    "\t\tfocal_method_input = self.code_dataset.at[idx, 'input_string_focal_method']\n",
    "\t\tfocal_cls_input = self.code_dataset.at[idx, 'input_string_focal_cls']\n",
    "\t\tresponse = self.code_dataset.at[idx, 'response']\n",
    "\n",
    "\t\tdef encode_text(text, tokenizer):\n",
    "\t\t\tencoding = tokenizer.encode_plus(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tadd_special_tokens=True,\n",
    "\t\t\t\tmax_length=self.max_length if tokenizer == self.tokenizer_code_bert else self.max_length * 2,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\treturn_attention_mask=True,\n",
    "\t\t\t\treturn_tensors='pt',\n",
    "\t\t\t)\n",
    "\t\t\tinput_ids = encoding['input_ids'].flatten()\n",
    "\t\t\tattention_mask = encoding['attention_mask'].flatten()\n",
    "\t\t\treturn input_ids, attention_mask\n",
    "\n",
    "\t\tinput_ids_focal_method, attention_mask_focal_method = encode_text(focal_method_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_focal_cls, attention_mask_focal_cls = encode_text(focal_cls_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_response, attention_mask_response = encode_text(response, self.tokenizer_gpt)\n",
    "\n",
    "\t\tif idx_to_token:\n",
    "\t\t\treturn {\n",
    "\t\t\t\t'input_ids_focal_method': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_method),\n",
    "\t\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t\t'input_ids_focal_cls': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_cls),\n",
    "\t\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t\t'ids_response': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_response),\n",
    "\t\t\t\t'attention_mask_response': attention_mask_response\n",
    "\t\t\t}\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids_focal_method': input_ids_focal_method,\n",
    "\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t'input_ids_focal_cls': input_ids_focal_cls,\n",
    "\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t'ids_response': input_ids_response,\n",
    "\t\t\t'attention_mask_response': attention_mask_response\n",
    "\t\t}\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\t'''Функция возвращает длину датасета. В качестве длины берется размер датасета по axis = 0'''\n",
    "\t\treturn self.code_dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестируем написанный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.404557Z",
     "iopub.status.busy": "2024-12-12T06:15:58.404206Z",
     "iopub.status.idle": "2024-12-12T06:15:58.413360Z",
     "shell.execute_reply": "2024-12-12T06:15:58.412558Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.404498Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code2test_dataset = Code2TestDataset(code_dataset=code_dataset,\n",
    "                                     tokenizer_code_bert=tokenizer_code_bert,\n",
    "                                     tokenizer_gpt=tokenizerGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.525416Z",
     "iopub.status.busy": "2024-12-12T06:15:58.524794Z",
     "iopub.status.idle": "2024-12-12T06:15:58.533385Z",
     "shell.execute_reply": "2024-12-12T06:15:58.532537Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.525379Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина датасета составляет: 280458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Длина датасета составляет: {len(code2test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всё работает корректно! Следующим шагом необходимо разбить датасет на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.534956Z",
     "iopub.status.busy": "2024-12-12T06:15:58.534371Z",
     "iopub.status.idle": "2024-12-12T06:15:58.563399Z",
     "shell.execute_reply": "2024-12-12T06:15:58.562563Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.534920Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(dataset_cls = Code2TestDataset,\n",
    "\t\t\t\tmax_length = 512,\n",
    "\t\t\t\tdata = code_dataset,\n",
    "\t\t\t\ttokenizer_code_bert = tokenizer_code_bert,\n",
    "\t\t\t\ttokenizer_gpt = tokenizerGPT,\n",
    "\t\t\t\ttrain_size = 0.7):\n",
    "\t'''\n",
    "\tФункция get_datasets() возвращает train и val датасеты на основе конструктора AccentDataset, делая train_val_spilt\n",
    "\t\n",
    "\tПараметры:\n",
    "\t-dataset_cls: класс датасета, конструктор которого будет вызываться (default: Code2TestDataset)\n",
    "\t-max_length: максимальная статья последовательности токенов\n",
    "\t-data: датасает pd.DataFrame (default: code_dataset)\n",
    "\t-tokenizer_code_bert: токенизатор codeBERT (default: tokenizer_code_bert)\n",
    "\t-tokenizer_gpt: токенизатор GPT2 (default: tokenizer_gpt)\n",
    "\t-train_size: размер тренировочной выборки (default: 0.7)\n",
    "\t\n",
    "\t'''\n",
    "\t\n",
    "\tdataset = dataset_cls(code_dataset = data,\n",
    "\t\t\t\t\t   \ttokenizer_code_bert = tokenizer_code_bert,\n",
    "\t\t\t\t\t\ttokenizer_gpt=tokenizer_gpt,\n",
    "\t\t\t\t\t\tmax_length=max_length)\n",
    "\t\n",
    "\ttrain_size = int(train_size * len(dataset))\n",
    "\tval_size = len(dataset) - train_size\n",
    "\ttrain_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\treturn train_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset = get_datasets(train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверяем полученные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.564657Z",
     "iopub.status.busy": "2024-12-12T06:15:58.564392Z",
     "iopub.status.idle": "2024-12-12T06:15:58.569309Z",
     "shell.execute_reply": "2024-12-12T06:15:58.568439Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.564633Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество данных в train и val выборках соответственно: (252412, 28046)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество данных в train и val выборках соответственно: {len(train_dataset), len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.570612Z",
     "iopub.status.busy": "2024-12-12T06:15:58.570336Z",
     "iopub.status.idle": "2024-12-12T06:15:58.582210Z",
     "shell.execute_reply": "2024-12-12T06:15:58.581453Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.570588Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(tokens_ids, tokenizer):\n",
    "\t'''Декодирование последовательности токенов'''\n",
    "\tcode_bert_decoded = tokenizer.decode(tokens_ids)\n",
    "\tprint(f\"Декодированная строка: {code_bert_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее получим DataLoader, по которому будем итерироваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.647235Z",
     "iopub.status.busy": "2024-12-12T06:15:58.646948Z",
     "iopub.status.idle": "2024-12-12T06:15:58.657089Z",
     "shell.execute_reply": "2024-12-12T06:15:58.656387Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.647174Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loaders(train_dataset = train_dataset,\n",
    "\t\t\tval_dataset = val_dataset,\n",
    "\t\t\tshuffle_train = True,\n",
    "\t\t\tshuffle_val = False,\n",
    "\t\t\tbatch_size = 32):\n",
    "\t\n",
    "\t'''\n",
    "\tФункция get_loaders() для получения train, val даталоадеров\n",
    "\n",
    "\tПараметры:\n",
    "\t-train_dataset: тренировочный датасет (default: train_dataset)\n",
    "\t-val_dataset: валидационный датасет (default: val_dataset)\n",
    "\t-shuffle_train: флаг перемешивания для train (default: True)\n",
    "\t-shuffle_val: флаг перемешивания для val (default: False)\n",
    "\t-batch_size: размер батча данных (default: 32)\n",
    "\t'''\n",
    "\t\n",
    "\t# train_dataloader\n",
    "\ttrain_dataloader = DataLoader(\n",
    "\t\t\ttrain_dataset,   \n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_train,\n",
    "\t\t)\n",
    "\n",
    "\t# validation_dataloader\n",
    "\tvalidation_dataloader = DataLoader(\n",
    "\t\t\tval_dataset, \n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_val,\n",
    "\t\t)\n",
    "\t\n",
    "\t# Возвращаем даталоадеры\n",
    "\treturn train_dataloader, validation_dataloader\n",
    "\n",
    "train_dataloader, validation_dataloader = get_loaders(batch_size=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверка итерирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.724781Z",
     "iopub.status.busy": "2024-12-12T06:15:58.724413Z",
     "iopub.status.idle": "2024-12-12T06:15:58.777765Z",
     "shell.execute_reply": "2024-12-12T06:15:58.776878Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.724751Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/126206 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    if i == 0:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корректно отрабатывает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, собираем архитектуру и готовимся обучать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.779397Z",
     "iopub.status.busy": "2024-12-12T06:15:58.779096Z",
     "iopub.status.idle": "2024-12-12T06:15:58.782957Z",
     "shell.execute_reply": "2024-12-12T06:15:58.782039Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.779372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_code_bert = AutoModel.from_pretrained(\"microsoft/codebert-base\", output_hidden_states= True).to(device)\n",
    "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает модель codeBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.784258Z",
     "iopub.status.busy": "2024-12-12T06:15:58.783945Z",
     "iopub.status.idle": "2024-12-12T06:15:58.796543Z",
     "shell.execute_reply": "2024-12-12T06:15:58.795837Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.784223Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "\t\n",
    "# \t# Проверка корректности работы\n",
    "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\t\n",
    "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "# \tprint(last_hidden_state_code_bert.size())\n",
    "# \tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, для каждого токена мы получим свое закодированное значение размерности 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.802733Z",
     "iopub.status.busy": "2024-12-12T06:15:58.802139Z",
     "iopub.status.idle": "2024-12-12T06:15:58.806865Z",
     "shell.execute_reply": "2024-12-12T06:15:58.806127Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.802702Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "modelGPT2Path = \"gpt2\"\n",
    "# config = AutoConfig.from_pretrained(modelGPT2Path, is_decoder=True, add_cross_attention= True)\n",
    "# config.add_cross_attention = True  # Включение cross-attention\n",
    "\n",
    "# modelGPT2 = AutoModel.from_pretrained(modelGPT2Path, config=config).to(device)\n",
    "# modelGPT2.resize_token_embeddings(len(tokenizerGPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как работает модель GPTBigCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.808322Z",
     "iopub.status.busy": "2024-12-12T06:15:58.807994Z",
     "iopub.status.idle": "2024-12-12T06:15:58.819396Z",
     "shell.execute_reply": "2024-12-12T06:15:58.818477Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.808280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\t\n",
    "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "\n",
    "# \tprint(last_hidden_state_code_bert.size())\n",
    "\t\n",
    "# \t# Проверка корректности работы\n",
    "# \tresponse_input_ids = batch['ids_response'].to(device)\n",
    "# \tresponse_input_mask = batch['attention_mask_response'].to(device)\n",
    "# \tgpt_output = modelGPT2(input_ids=response_input_ids, \n",
    "# \t\t\t\t\t\t\t  attention_mask=response_input_mask, \n",
    "# \t\t\t\t\t\t\t  encoder_hidden_states = last_hidden_state_code_bert)\n",
    "# \tprint(gpt_output['last_hidden_state'].size())\n",
    "\t\n",
    "\t\n",
    "# \t# outputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \t# last_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "# \t# print(last_hidden_state_code_bert.size())\n",
    "# \tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ну, как-то худо-бедно всё это дело запускается. Пробуем строить модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.820718Z",
     "iopub.status.busy": "2024-12-12T06:15:58.820424Z",
     "iopub.status.idle": "2024-12-12T06:16:00.552702Z",
     "shell.execute_reply": "2024-12-12T06:16:00.552002Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.820688Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "class LargeCodeModel(nn.Module):\n",
    "\t'''Класс для сложной языковой модели, которая обрабатывает входной код'''\n",
    "\tdef __init__(self, bert_model_name, gpt2_name, batch_size = 8):\n",
    "\t\tsuper(LargeCodeModel, self).__init__()\n",
    "\t\t\n",
    "\t\tself.batch_size = batch_size\n",
    "\t\tself.bert1 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.bert2 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.tokenizer_code_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "\t\tself.new_special_tokens = ['<FUNC_TOKEN>',\n",
    "            '<INFO_TOKEN>',\n",
    "            '<CLS_TOKEN>', \n",
    "            '<AST_TOKEN>', \n",
    "            '<DESCRIPTION_TOKEN>',\n",
    "            '<COMMENTS_TOKEN>']\n",
    "\n",
    "\t\tself.special_tokens_dict = {\n",
    "\t\t\t'additional_special_tokens': new_special_tokens\n",
    "\t\t}\n",
    "\n",
    "\t\tself.tokenizer_code_bert.add_special_tokens(self.special_tokens_dict)\n",
    "\t\tself.bert1.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\t\tself.bert2.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\n",
    "\t\tself.gpt2_config = AutoConfig.from_pretrained(gpt2_name, is_decoder=True, add_cross_attention = True)\n",
    "\t\tself.gpt2_config.add_cross_attention = True  # Включение cross-attention\n",
    "\t\tself.tokenizerGPT = AutoTokenizer.from_pretrained(gpt2_name)\n",
    "\t\tself.tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\t\tself.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_name, config=self.gpt2_config)\n",
    "\t\tself.gpt2.resize_token_embeddings(len(self.tokenizerGPT))\n",
    "\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.bert1.config.hidden_size)\n",
    "\n",
    "\t\tself.projection = nn.Linear(\n",
    "            self.bert1.config.hidden_size + self.bert2.config.hidden_size,\n",
    "            self.gpt2.config.hidden_size\n",
    "        )\n",
    "\n",
    "\t# forward call\n",
    "\tdef forward(self, focal_method_input_ids, \n",
    "\t\t\t \t\t\tfocal_method_attention_masks, \n",
    "\t\t\t\t\t\tfocal_cls_input_ids,\n",
    "\t\t\t\t\t\tfocal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks):\n",
    "\t\t\n",
    "\t\t# print(focal_method_input_ids.size())\n",
    "\t\t# print(focal_method_attention_masks.size())\n",
    "\t\t# print(type(focal_method_input_ids))\n",
    "\t\t# print(type(focal_method_attention_masks))\n",
    "\t\t# print(type(focal_cls_input_ids))\n",
    "\t\t# print(type(focal_cls_attention_masks))\n",
    "\t\t# print(type(response_ids))\n",
    "\t\t# print(type(response_attention_masks))\n",
    "\t\t\n",
    "\t\tbert1_outputs = self.bert1(focal_method_input_ids, focal_method_attention_masks)\n",
    "\t\tlast_hidden_state_bert1 = bert1_outputs['last_hidden_state']\n",
    "\n",
    "\t\tbert2_outputs = self.bert2(focal_cls_input_ids, focal_cls_attention_masks)\n",
    "\t\tlast_hidden_state_bert2 = bert2_outputs['last_hidden_state']\n",
    "\n",
    "\t\t# print(last_hidden_state_bert1.size())\n",
    "\t\t# print(last_hidden_state_bert2.size())\n",
    "\n",
    "\t\tconcat_hidden_states = torch.cat([last_hidden_state_bert1, last_hidden_state_bert2], dim=1)\n",
    "\n",
    "\t\t# print(concat_hidden_states.size())\n",
    "\n",
    "\t\t# LayerNormalization\n",
    "\t\tnormalized_hidden_states = self.layer_norm(concat_hidden_states)\n",
    "\n",
    "\t\t# Для BatchNorm\n",
    "\t\t# batch_norm_input = concat_hidden_states.view(-1, 768)\n",
    "\t\t# normalized_hidden_states = self.batch_norm(batch_norm_input)\n",
    "\t\t# normalized_hidden_states = normalized_hidden_states.view(2, 1024, 768)\n",
    "\t\t# print(normalized_hidden_states.size())\n",
    "\t\t# print(torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1).size())\n",
    "\t\t# print(response_ids.size())\n",
    "\t\t# print(response_input_mask.size())\n",
    "\n",
    "\t\t# print(response_attention_masks.size())\n",
    "\n",
    "\t\t# print('No problems')\n",
    "\n",
    "\t\tquery_input_ids = self.tokenizerGPT.encode(\"<s>\", \n",
    "\t\t\t\t\t\t\t\t\t\t\t\treturn_tensors='pt', \n",
    "\t\t\t\t\t\t\t\t\t\t\t\tmax_length=1024, \n",
    "\t\t                                        padding='max_length').to(device)\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\tbatch_input_ids = query_input_ids.repeat(self.batch_size, 1)\t\t\t\t\t\t\t\t\t\t\t\n",
    "\t\tgpt2_outputs = self.gpt2(\n",
    "            input_ids=batch_input_ids,\n",
    "            attention_mask=response_attention_masks,\n",
    "            encoder_hidden_states=normalized_hidden_states,\n",
    "            encoder_attention_mask=torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1),\n",
    "\t\t\t\t\t\tlabels=response_ids\n",
    "        )\n",
    "\n",
    "\t\treturn gpt2_outputs\n",
    "\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлаживаем модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:16:00.554391Z",
     "iopub.status.busy": "2024-12-12T06:16:00.553899Z",
     "iopub.status.idle": "2024-12-12T06:16:11.986047Z",
     "shell.execute_reply": "2024-12-12T06:16:11.985054Z",
     "shell.execute_reply.started": "2024-12-12T06:16:00.554357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dadefe683c60425dba05c07ecd5699c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/499M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67b129a517dd41ff9a066355237259b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8e6e2a59db147e584213fc9ae4af606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CodeModel = LargeCodeModel(bert_model_name=\"microsoft/codebert-base\",\n",
    "                           gpt2_name=\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее необходимо объявить функцию train-val loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала необходимо объявить дополнительные функции для отображения времени и подсчёта метрик качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:17:35.226198Z",
     "iopub.status.busy": "2024-12-12T06:17:35.225592Z",
     "iopub.status.idle": "2024-12-12T06:17:35.233921Z",
     "shell.execute_reply": "2024-12-12T06:17:35.233076Z",
     "shell.execute_reply.started": "2024-12-12T06:17:35.226167Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "\t'''Функция форматирования времени'''\n",
    "\treturn str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "def token_accuracy_calc(logits, labels, attention_mask):\n",
    "\t'''Функция подсчета accuracy для данных'''\n",
    "\tpred_flat = np.argmax(logits, axis=-1).flatten()  \n",
    "\tlabels_flat = labels.flatten()  \n",
    "\tmask_flat = attention_mask.flatten()\n",
    "\taccuracy = np.sum(pred_flat[mask_flat] == labels_flat[mask_flat]) / len(labels_flat[mask_flat])\n",
    "\treturn accuracy\n",
    "\n",
    "def token_bleu_calc(logits, labels, attention_mask):\n",
    "\t'''Функция подсчета BLEU для данных'''\n",
    "\t\n",
    "\tpred_flat = np.argmax(logits, axis=-1)\n",
    "\tlabels_flat = labels\n",
    "\tmask_flat = attention_mask\n",
    "\t\n",
    "\tpred_tokens = []\n",
    "\ttrue_tokens = []\n",
    "\t\n",
    "\tfor i in range(pred_flat.shape[0]):\n",
    "\t\tpred_seq = pred_flat[i][mask_flat[i] == 1]\n",
    "\t\ttrue_seq = labels_flat[i][mask_flat[i] == 1]\n",
    "\t\tpred_tokens.append(pred_seq)\n",
    "\t\ttrue_tokens.append(true_seq)\n",
    "\n",
    "\tpred_strings = tokenizerGPT.batch_decode(pred_tokens, skip_special_tokens=True)\n",
    "\ttrue_strings = tokenizerGPT.batch_decode(true_tokens, skip_special_tokens=True)\n",
    "\t\n",
    "\t# Вычисление BLEU\n",
    "\tbleu_scores = []\n",
    "\tsmoothing_function = SmoothingFunction().method1\n",
    "\n",
    "\tfor pred, true in zip(pred_strings, true_strings):\n",
    "\t\tpred_tokens = pred.split()\n",
    "\t\ttrue_tokens = [true.split()]  \n",
    "\t\tbleu_score = sentence_bleu(true_tokens, pred_tokens, smoothing_function=smoothing_function)\n",
    "\t\tbleu_scores.append(bleu_score)\n",
    "\n",
    "\taverage_bleu = np.mean(bleu_scores)\n",
    "\treturn average_bleu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тестирование подсчета метрик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:17:38.272541Z",
     "iopub.status.busy": "2024-12-12T06:17:38.271935Z",
     "iopub.status.idle": "2024-12-12T06:17:38.278658Z",
     "shell.execute_reply": "2024-12-12T06:17:38.277585Z",
     "shell.execute_reply.started": "2024-12-12T06:17:38.272488Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence_upd(tokens_ids, tokenizer):\n",
    "\t'''Декодирование последовательности токенов'''\n",
    "\tcode_bert_decoded = tokenizer.decode(tokens_ids)\n",
    "\treturn code_bert_decoded\n",
    "\n",
    "# for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "# \t# writer = SummaryWriter('runs/LargeCodeModel_graph')\n",
    "\n",
    "# \t# print(batch)\n",
    "# \t# for key in list(batch.keys()):\n",
    "# \t# \tprint(batch[key].size())\n",
    "    \n",
    "# \tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "# \tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "# \tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "# \tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "# \tresponse_ids = batch['ids_response'].to(device)\n",
    "# \tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "# \t# input_data = (\n",
    "#     #     focal_method_input_ids,\n",
    "#     #     focal_method_attention_masks,\n",
    "#     #     focal_cls_input_ids,\n",
    "#     #     focal_cls_attention_masks,\n",
    "#     #     response_ids,\n",
    "#     #     response_attention_masks\n",
    "#     # )\n",
    "\n",
    "\n",
    "# \t# writer.add_graph(CodeModel, input_data)\n",
    "# \t# writer.close()\n",
    "\n",
    "# \toutput_codeLM = CodeModel(focal_method_input_ids, focal_method_attention_masks,\n",
    "# \t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "# \t\t\t\t\t\tresponse_ids, response_attention_masks)\n",
    "\t\n",
    "# \tloss = output_codeLM.loss\n",
    "# \tlogits = output_codeLM.logits\n",
    "\n",
    "# \tlogits = logits.detach().cpu().numpy()\n",
    "# \tresponse_ids = response_ids.cpu().numpy()\n",
    "# \tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "# \taccuracy_train = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
    "\t\n",
    "# \tprint(output['logits'].size())\n",
    "# \tprint(output['loss'])\n",
    "# \tprint(f\"Testing accuracy: {accuracy_train}\")\n",
    "\n",
    "# \toutput_ids = np.argmax(logits, axis=-1)\n",
    "# \t# print(response_ids.shape)\n",
    "\n",
    "# \tstring = decode_sequence_upd(output_ids[0], tokenizerGPT).replace('<PAD>', '')\n",
    "# \t# print(output_ids.shape) \n",
    "# \tgt_string = decode_sequence_upd(response_ids[0], tokenizerGPT).replace('<PAD>', '')\n",
    "\n",
    "# \tbleu_score = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
    "# \tprint(f\"BLEU score: {bleu_score}\")\n",
    "\n",
    "# \t# print(output_ids.shape)\n",
    "# \t# decoded_strings = [tokenizerGPT.decode(output_ids[i], skip_special_tokens=True) for i in range(output_ids.shape[0])]\n",
    "# \t# print(decoded_strings)\n",
    "# \t# print(f\"output string: {string}\")\n",
    "# \t# print(f\"Ground Truth string: {gt_string}\")\n",
    "# \tbreak\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как минимимум оно запускается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее реализуем саму функцию train-val-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед этим объями дополнительные настройки обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:18:59.067221Z",
     "iopub.status.busy": "2024-12-12T06:18:59.066535Z",
     "iopub.status.idle": "2024-12-12T06:18:59.076961Z",
     "shell.execute_reply": "2024-12-12T06:18:59.076008Z",
     "shell.execute_reply.started": "2024-12-12T06:18:59.067191Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(CodeModel.parameters(), lr=3e-5)\n",
    "num_epochs = 15\n",
    "train_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps\n",
    ")\n",
    "tensorboard_log_dir = 'runs/CodeModelLogs/'\n",
    "tensorboard_path_accuracy = 'runs/model_accuracy_score_{:.2f}.pth'\n",
    "tensorboard_path_loss = 'runs/model_val_loss_{:.2f}.pth'\n",
    "tensorboard_path_bleu = 'runs/model_bleu_score_{:.2f}.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И, наконец, функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:19:00.573329Z",
     "iopub.status.busy": "2024-12-12T06:19:00.573023Z",
     "iopub.status.idle": "2024-12-12T06:19:00.594827Z",
     "shell.execute_reply": "2024-12-12T06:19:00.593904Z",
     "shell.execute_reply.started": "2024-12-12T06:19:00.573302Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_val_loop_codeLM(model = CodeModel, \n",
    "\t\t\t\t\t\ttrain_loader = train_dataloader, \n",
    "\t\t\t\t\t\tval_loader = validation_dataloader, \n",
    "\t\t\t\t\t\toptimizer = optimizer, \n",
    "\t\t\t\t\t\tscheduler = lr_scheduler, \n",
    "\t\t\t\t\t\tnum_epochs = num_epochs, \n",
    "\t\t\t\t\t\tdevice = 'cuda', \n",
    "\t\t\t\t\t\tmodel_save_path_accuracy = tensorboard_path_accuracy, \n",
    "\t\t\t\t\t\tmodel_save_path_loss = tensorboard_path_loss, \n",
    "\t\t\t\t\t\tmodel_save_path_bleu = tensorboard_path_bleu, \n",
    "\t\t\t\t\t\ttensorboard_log_dir = tensorboard_log_dir, \n",
    "\t\t\t\t\t\tgradient_accumulation_steps = 2, \n",
    "\t\t\t\t\t\teval_every = 1, \n",
    "\t\t\t\t\t\ttest_step_only = False):\n",
    "\t'''\n",
    "\tФункция для реализации train-val loop обучения нашей модели\n",
    "\t\n",
    "\tПараметры:\n",
    "\t-model: модель нейронной сети\n",
    "\t-train_loader: тренировочный датасет\n",
    "\t-val_loader: валидационный датасет\n",
    "\t-optimizer: оптимизатор\n",
    "\t-scheduler: изменение для learning_rate (расписание)\n",
    "\t-num_epochs: число эпох для обучения\n",
    "\t-device: устройство\n",
    "\t-model_save_path_accuracy: путь для сохранения весов модели (с лучшей accuracy)\n",
    "\t-model_save_path_loss: путь для сохранения весов модели (с лучим val_loss)\n",
    "\t-model_save_path_bleu: путь для сохранения весов модели (с лучим val_bleu_score)\n",
    "\t-tensorboard_log_dir: путь для записи логов в TensorBoard, \n",
    "\t-gradient_accumulation_steps: число шагов для накопления градиентов\n",
    "\t-eval_every: число шагов, через которые делаем валидацию\n",
    "\t-test_step_only: вспомогательная логика для тестирования обучения небольшого числа шагов (default: False)\n",
    "\t'''\n",
    "\n",
    "\twriter = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "\thistory = {\n",
    "\t\t'train_loss': [],\n",
    "\t\t'train_bleu': [],\n",
    "\t\t'train_accuracy': [],\n",
    "\t\t'val_accuracy': [],\n",
    "\t\t'val_loss': [],\n",
    "\t\t'val_bleu': []\n",
    "\t}\n",
    "\tbest_val_loss = float('inf')\n",
    "\tbest_val_bleu = 0.0\n",
    "\tbest_val_accuracy = 0.0\n",
    "\tmodel.to(device)\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(\"\")\n",
    "\t\tprint('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
    "\t\tprint('Training...')\n",
    "\t\t\n",
    "\t\tt0 = time.time()\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_train_loss = 0\n",
    "\t\ttotal_train_bleu = 0\n",
    "\t\ttotal_train_accuracy = 0\n",
    "\t\tnum_train_steps = 0\n",
    "\n",
    "\t\tfor step, batch in enumerate(tqdm(train_loader)):\n",
    "\t\t\t\n",
    "\t\t\tif test_step_only and step >= 1:  # Прерываем после первого батча\n",
    "\t\t\t\tbreak\n",
    "\t\t\t\t\n",
    "\t\t\t\n",
    "\t\t\tif step % 1500 == 0 and not step == 0:\n",
    "\t\t\t\t# Calculate elapsed time in minutes.\n",
    "\t\t\t\telapsed = format_time(time.time() - t0)\n",
    "\t\t\t\t# Report progress.\n",
    "\t\t\t\tprint('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
    "\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks)\n",
    "\t\t\t\n",
    "\t\t\tloss = output_codeLM.loss\n",
    "\t\t\tlogits = output_codeLM.logits\n",
    "\n",
    "\t\t\tlogits = logits.detach().cpu().numpy()\n",
    "\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
    "\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "\t\t\taccuracy_train = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
    "\t\t\tbleu_train = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
    "\n",
    "\t\t\ttotal_train_accuracy += accuracy_train\n",
    "\t\t\ttotal_train_bleu += bleu_train\n",
    "\n",
    "\t\t\ttotal_train_loss += loss.item()\n",
    "\t\t\tnum_train_steps += 1\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tif (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Клипаем накопленные градиенты\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tscheduler.step()\n",
    "\t\t\t\n",
    "\t\tavg_train_loss = total_train_loss / num_train_steps\n",
    "\t\tavg_train_accuracy = total_train_accuracy / num_train_steps\n",
    "\t\tavg_train_bleu_score = total_train_bleu / num_train_steps\n",
    "\n",
    "\t\ttraining_time = format_time(time.time() - t0)\n",
    "\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\t\t# print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "\t\t# print(\"  Average training BLEU score: {0:.2f}\".format(avg_train_bleu_score))\n",
    "\t\tprint(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\t\thistory['train_loss'].append(avg_train_loss)\n",
    "\t\thistory['train_accuracy'].append(avg_train_accuracy)\n",
    "\t\thistory['train_bleu'].append(avg_train_bleu_score)\n",
    "\n",
    "\t\t# Логирование в TensorBoard для обучения\n",
    "\t\twriter.add_scalar(\"Train/Loss\", avg_train_loss, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Train/Accuracy\", avg_train_accuracy, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Train/BLEU_score\", avg_train_bleu_score, epoch + 1)\n",
    "\n",
    "\t\tprint(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, Train BLEU score: {avg_train_bleu_score:.4f}\")\n",
    "\t\t\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"Running Validation...\")\n",
    "\n",
    "\n",
    "\t\tt0 = time.time()\n",
    "\n",
    "\t\t# Put the model in evaluation mode--the dropout layers behave differently\n",
    "\t\t# during evaluation.\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\tif (epoch + 1) % eval_every == 0:\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\ttotal_eval_loss = 0\n",
    "\t\t\ttotal_eval_accuracy = 0\n",
    "\t\t\ttotal_eval_bleu = 0\n",
    "\t\t\tnum_eval_steps = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor batch in val_loader:\n",
    "\t\t\t\t\n",
    "\t\t\t\tif test_step_only and num_eval_steps >= 1:  # Прерываем если хотим проконтроллировать\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\t\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "\t\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "\t\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "\t\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
    "\t\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "\t\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks)\n",
    "\t\t\t\n",
    "\t\t\t\tloss = output_codeLM.loss\n",
    "\t\t\t\tlogits = output_codeLM.logits\n",
    "\n",
    "\t\t\t\tlogits = logits.detach().cpu().numpy()\n",
    "\t\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
    "\t\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "\t\t\t\taccuracy_val = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
    "\t\t\t\tbleu_val = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
    "\t\t\t\t\n",
    "\t\t\t\ttotal_eval_accuracy += accuracy_val\n",
    "\t\t\t\ttotal_eval_bleu += bleu_val\n",
    "\n",
    "\t\t\t\ttotal_eval_loss += loss.item()\n",
    "\t\t\t\tnum_eval_steps += 1\n",
    "\n",
    "\t\tavg_val_loss = total_eval_loss / num_eval_steps\n",
    "\t\tavg_val_accuracy = total_eval_accuracy / num_eval_steps\n",
    "\t\tavg_val_bleu_score = total_eval_bleu / num_eval_steps\n",
    "\n",
    "\t\thistory['val_loss'].append(avg_val_loss)\n",
    "\t\thistory['val_accuracy'].append(avg_val_accuracy)\n",
    "\t\thistory['val_bleu'].append(avg_val_bleu_score)\n",
    "\n",
    "\t\t# Логирование в TensorBoard для валидации\n",
    "\t\twriter.add_scalar(\"Validation/Loss\", avg_val_loss, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Validation/Accuracy\", avg_val_accuracy, epoch + 1)\n",
    "\n",
    "\t\tprint(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f},  Validation BLEU score: {avg_val_bleu_score:.4f}\")\n",
    "\n",
    "\t\t# Ну вот тут надо настроить, чтобы \n",
    "\t\t# Если точность выше, то сохраняем веса\n",
    "\t\tif avg_val_accuracy > best_val_accuracy:\n",
    "\t\t\tbest_val_accuracy = avg_val_accuracy\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_accuracy.format(best_val_accuracy))\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_accuracy.format(best_val_accuracy)}\")\n",
    "\n",
    "\t\t# Если лосс ниже, то сохраняем веса \n",
    "\t\tif avg_val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = avg_val_loss\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_loss.format(best_val_loss))\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_loss.format(best_val_loss)}\")\n",
    "\t\t\n",
    "\t\t# Если BLEU выше, то сохраняем веса \n",
    "\t\tif avg_val_bleu_score > best_val_bleu:\n",
    "\t\t\tbest_val_bleu = avg_val_bleu_score\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_bleu.format(best_val_bleu))\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_bleu.format(best_val_bleu)}\")\n",
    "\t\n",
    "\twriter.close()\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наконец, пробуем запустить обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_results = train_val_loop_codeLM(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6256753,
     "sourceId": 10137849,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256764,
     "sourceId": 10137869,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6283970,
     "sourceId": 10174103,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6285702,
     "sourceId": 10176633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
