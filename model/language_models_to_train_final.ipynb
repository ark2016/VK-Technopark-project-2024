{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfmIB0OoVogr"
   },
   "source": [
    "Архитектура модели анализа кода"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VteU_AcwVogu"
   },
   "source": [
    "В данном файле проводится анализ архитектуры модели, токенизатора и подготовка к обучению модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-eHJpaGaVogu"
   },
   "source": [
    "Импортируем необходимые модули"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:13.382026Z",
     "iopub.status.busy": "2024-12-12T06:15:13.381216Z",
     "iopub.status.idle": "2024-12-12T06:15:30.190140Z",
     "shell.execute_reply": "2024-12-12T06:15:30.189378Z",
     "shell.execute_reply.started": "2024-12-12T06:15:13.381983Z"
    },
    "id": "DvRmY-2wVogu",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "from torch.utils.tensorboard import summary, writer, SummaryWriter\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GPdie2iuVogw"
   },
   "source": [
    "Устанавливаем SEED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:30.193189Z",
     "iopub.status.busy": "2024-12-12T06:15:30.192033Z",
     "iopub.status.idle": "2024-12-12T06:15:30.201396Z",
     "shell.execute_reply": "2024-12-12T06:15:30.200706Z",
     "shell.execute_reply.started": "2024-12-12T06:15:30.193156Z"
    },
    "id": "vkUUT9UWVogw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m0pJFBuVogw"
   },
   "source": [
    "Далее считываем исходный датасет и немного дорабатываем его"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Hx5Vi6NMWGGL"
   },
   "outputs": [],
   "source": [
    "dataset_path = '/Users/chervonikov_alexey/Desktop/projects/Technopark_Autumn_2024/NN_course_project/model/upd_code_dataset.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:30.202945Z",
     "iopub.status.busy": "2024-12-12T06:15:30.202362Z",
     "iopub.status.idle": "2024-12-12T06:15:52.551962Z",
     "shell.execute_reply": "2024-12-12T06:15:52.551214Z",
     "shell.execute_reply.started": "2024-12-12T06:15:30.202918Z"
    },
    "id": "rful6GZ0Vogw",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code_dataset = pd.read_parquet(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.553687Z",
     "iopub.status.busy": "2024-12-12T06:15:52.553301Z",
     "iopub.status.idle": "2024-12-12T06:15:52.573492Z",
     "shell.execute_reply": "2024-12-12T06:15:52.572692Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.553649Z"
    },
    "id": "D3cjvyVZVogx",
    "outputId": "1ceb1be0-110a-403b-ca61-b0fdc5aa4234",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "      <th>input_string_focal_method</th>\n",
       "      <th>input_string_focal_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0                                        <AST_TOKEN>   \n",
       "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "3                                        <AST_TOKEN>   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
       "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
       "\n",
       "                                      focal_cls_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "3                                       <INFO_TOKEN>   \n",
       "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
       "\n",
       "                           input_string_focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                              input_string_focal_cls  \n",
       "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.575470Z",
     "iopub.status.busy": "2024-12-12T06:15:52.575217Z",
     "iopub.status.idle": "2024-12-12T06:15:52.642582Z",
     "shell.execute_reply": "2024-12-12T06:15:52.641652Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.575446Z"
    },
    "id": "Nd1CvXC4Vogx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code_dataset = code_dataset.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POH-HrSeVogy"
   },
   "source": [
    "Наконец, переходим к анализу архитектур нейросетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Ck51yW9Vogy"
   },
   "source": [
    "Решено использовать подход, основанный на обучении (fine-tuning) нейросети CodeBERT, в основе которой лежит модель RoBERTa. Далее будем использовать метамодель в виде декодера (CodeGen или GPTBigCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:52.649211Z",
     "iopub.status.busy": "2024-12-12T06:15:52.648956Z",
     "iopub.status.idle": "2024-12-12T06:15:54.090651Z",
     "shell.execute_reply": "2024-12-12T06:15:54.089570Z",
     "shell.execute_reply.started": "2024-12-12T06:15:52.649177Z"
    },
    "id": "PJfhnrquVogy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUNfTCywVogy"
   },
   "source": [
    "Device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:54.092773Z",
     "iopub.status.busy": "2024-12-12T06:15:54.092032Z",
     "iopub.status.idle": "2024-12-12T06:15:54.263313Z",
     "shell.execute_reply": "2024-12-12T06:15:54.262376Z",
     "shell.execute_reply.started": "2024-12-12T06:15:54.092724Z"
    },
    "id": "30M8OI7XVogy",
    "outputId": "16337460-6fa7-4cad-d8a9-a828266be5a9",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qlsY_ktWWTV4",
    "outputId": "7168e93e-6f73-48f3-bf37-d28215babf10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: nvidia-smi\n"
     ]
    }
   ],
   "source": [
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.820718Z",
     "iopub.status.busy": "2024-12-12T06:15:58.820424Z",
     "iopub.status.idle": "2024-12-12T06:16:00.552702Z",
     "shell.execute_reply": "2024-12-12T06:16:00.552002Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.820688Z"
    },
    "id": "-SuTXoOcVog7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, AutoConfig\n",
    "\n",
    "class LargeCodeModel(nn.Module):\n",
    "\t'''Класс для сложной языковой модели, которая обрабатывает входной код'''\n",
    "\tdef __init__(self, bert_model_name, gpt2_name):\n",
    "\t\tsuper(LargeCodeModel, self).__init__()\n",
    "\n",
    "\t\tself.bert1 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.bert2 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
    "\t\tself.tokenizer_code_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "\t\tself.new_special_tokens = ['<FUNC_TOKEN>',\n",
    "            '<INFO_TOKEN>',\n",
    "            '<CLS_TOKEN>',\n",
    "            '<AST_TOKEN>',\n",
    "            '<DESCRIPTION_TOKEN>',\n",
    "            '<COMMENTS_TOKEN>']\n",
    "\n",
    "\t\tself.special_tokens_dict = {\n",
    "\t\t\t'additional_special_tokens': self.new_special_tokens\n",
    "\t\t}\n",
    "\n",
    "\t\tself.tokenizer_code_bert.add_special_tokens(self.special_tokens_dict)\n",
    "\t\tself.bert1.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\t\tself.bert2.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
    "\n",
    "\t\tself.gpt2_config = AutoConfig.from_pretrained(gpt2_name, is_decoder=True, add_cross_attention = True)\n",
    "\t\tself.gpt2_config.add_cross_attention = True  # Включение cross-attention\n",
    "\t\tself.tokenizerGPT = AutoTokenizer.from_pretrained(gpt2_name)\n",
    "\t\tself.tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})\n",
    "\t\tself.tokenizerGPT.add_special_tokens(self.special_tokens_dict)\n",
    "\t\tself.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_name, config=self.gpt2_config)\n",
    "\t\tself.gpt2.resize_token_embeddings(len(self.tokenizerGPT))\n",
    "\t\tself.att = 0\n",
    "\n",
    "\t\tself.layer_norm = nn.LayerNorm(self.bert1.config.hidden_size)\n",
    "\n",
    "\t\tself.projection = nn.Linear(\n",
    "            self.bert1.config.hidden_size + self.bert2.config.hidden_size,\n",
    "            self.gpt2.config.hidden_size\n",
    "        )\n",
    "\n",
    "\t# forward call\n",
    "\tdef forward(self, focal_method_input_ids,\n",
    "\t\t\t \t\t\tfocal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids,\n",
    "\t\t\t\t\t\tfocal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
    "\t\t\t\t\t\tdecoder_input_ids_focal_method,\n",
    "\t\t\t\t\t\tdecoder_input_ids_focal_cls,\n",
    "\t\t\t\t\t\tattention_mask_focal_method_decoder,\n",
    "\t\t\t\t\t\tattention_mask_focal_cls_decoder):\n",
    "\n",
    "\t\t# print(focal_method_input_ids.size())\n",
    "\t\t# print(focal_method_attention_masks.size())\n",
    "\t\t# print(type(focal_method_input_ids))\n",
    "\t\t# print(type(focal_method_attention_masks))\n",
    "\t\t# print(type(focal_cls_input_ids))\n",
    "\t\t# print(type(focal_cls_attention_masks))\n",
    "\t\t# print(type(response_ids))\n",
    "\t\t# print(type(response_attention_masks))\n",
    "\n",
    "\t\tdecoder_input_ids = torch.cat([decoder_input_ids_focal_method,\n",
    "\t\t                                  decoder_input_ids_focal_cls], dim=1)\n",
    "\n",
    "\t\tdecoder_attention_masks = torch.cat([attention_mask_focal_method_decoder,\n",
    "\t\t                                  attention_mask_focal_cls_decoder], dim=1)\n",
    "\t\t\n",
    "\t\tself.att = decoder_attention_masks[0]\n",
    "\t\t# print(decoder_input_ids[0])\n",
    "\t\t# print(decoder_attention_masks[0])\n",
    "\n",
    "\t\t# print(decoder_input_ids.size())\n",
    "\n",
    "\t\tbert1_outputs = self.bert1(focal_method_input_ids, focal_method_attention_masks)\n",
    "\t\tlast_hidden_state_bert1 = bert1_outputs['last_hidden_state']\n",
    "\n",
    "\t\tbert2_outputs = self.bert2(focal_cls_input_ids, focal_cls_attention_masks)\n",
    "\t\tlast_hidden_state_bert2 = bert2_outputs['last_hidden_state']\n",
    "\n",
    "\t\t# print(last_hidden_state_bert1.size())\n",
    "\t\t# print(last_hidden_state_bert2.size())\n",
    "\n",
    "\t\tconcat_hidden_states = torch.cat([last_hidden_state_bert1,\n",
    "\t\t                                  last_hidden_state_bert2], dim=1)\n",
    "\n",
    "\t\t# print(concat_hidden_states.size())\n",
    "\n",
    "\t\t# LayerNormalization\n",
    "\t\tnormalized_hidden_states = self.layer_norm(concat_hidden_states)\n",
    "\n",
    "\t\t# Для BatchNorm\n",
    "\t\t# batch_norm_input = concat_hidden_states.view(-1, 768)\n",
    "\t\t# normalized_hidden_states = self.batch_norm(batch_norm_input)\n",
    "\t\t# normalized_hidden_states = normalized_hidden_states.view(2, 1024, 768)\n",
    "\t\t# print(normalized_hidden_states.size())\n",
    "\t\t# print(torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1).size())\n",
    "\t\t# print(response_ids.size())\n",
    "\t\t# print(response_input_mask.size())\n",
    "\n",
    "\t\t# print(response_attention_masks.size())\n",
    "\n",
    "\t\t# print('No problems')\n",
    "\n",
    "\t\tgpt2_outputs = self.gpt2(\n",
    "            input_ids=decoder_input_ids,\n",
    "            attention_mask=decoder_attention_masks,\n",
    "            encoder_hidden_states=normalized_hidden_states,\n",
    "            encoder_attention_mask=torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1),\n",
    "\t\t\tlabels=response_ids\n",
    "        )\n",
    "\n",
    "\t\treturn gpt2_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:16:00.554391Z",
     "iopub.status.busy": "2024-12-12T06:16:00.553899Z",
     "iopub.status.idle": "2024-12-12T06:16:11.986047Z",
     "shell.execute_reply": "2024-12-12T06:16:11.985054Z",
     "shell.execute_reply.started": "2024-12-12T06:16:00.554357Z"
    },
    "id": "J1v-RkeDVog7",
    "outputId": "14960716-2f0e-48a9-c88a-2cf8f4a9e273",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CodeModel = LargeCodeModel(bert_model_name=\"microsoft/codebert-base\",\n",
    "                           gpt2_name=\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.370424Z",
     "iopub.status.busy": "2024-12-12T06:15:58.370199Z",
     "iopub.status.idle": "2024-12-12T06:15:58.389555Z",
     "shell.execute_reply": "2024-12-12T06:15:58.388650Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.370401Z"
    },
    "id": "3LINtQWoVog0",
    "outputId": "43a62fe3-99c1-407a-e7d6-fd36e241d17b",
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>focal_method</th>\n",
       "      <th>focal_cls</th>\n",
       "      <th>focal_method_ast</th>\n",
       "      <th>focal_cls_ast</th>\n",
       "      <th>focal_method_info</th>\n",
       "      <th>focal_cls_info</th>\n",
       "      <th>input_string_focal_method</th>\n",
       "      <th>input_string_focal_cls</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>from microdot import Microdot, Response, abort...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt;</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt;</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
       "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
       "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
       "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            response  \\\n",
       "0  from microdot import Microdot, Response, abort...   \n",
       "1  from microdot import Microdot, Response, abort...   \n",
       "2  from microdot import Microdot, Response, abort...   \n",
       "3  from microdot import Microdot, Response, abort...   \n",
       "4  from pyner.named_entity.corpus import bio2bioe...   \n",
       "\n",
       "                                        focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                                           focal_cls  \\\n",
       "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
       "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
       "\n",
       "                                    focal_method_ast  \\\n",
       "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                       focal_cls_ast  \\\n",
       "0                                        <AST_TOKEN>   \n",
       "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
       "3                                        <AST_TOKEN>   \n",
       "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
       "\n",
       "                                   focal_method_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
       "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
       "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
       "\n",
       "                                      focal_cls_info  \\\n",
       "0                                       <INFO_TOKEN>   \n",
       "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
       "3                                       <INFO_TOKEN>   \n",
       "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
       "\n",
       "                           input_string_focal_method  \\\n",
       "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
       "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
       "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
       "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
       "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
       "\n",
       "                              input_string_focal_cls  \n",
       "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
       "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
       "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVjZ2q0xVog1"
   },
   "source": [
    "Далее необхоимо описать класс Dataset для нашей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.391373Z",
     "iopub.status.busy": "2024-12-12T06:15:58.390891Z",
     "iopub.status.idle": "2024-12-12T06:15:58.400975Z",
     "shell.execute_reply": "2024-12-12T06:15:58.400275Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.391333Z"
    },
    "id": "IcGyKj3-Vog1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Code2TestDataset(Dataset):\n",
    "\t'''Класс датасет для задачи генерации тестов'''\n",
    "\n",
    "\tdef __init__(self, code_dataset, tokenizer_code_bert, tokenizer_gpt, max_length=512):\n",
    "\t\t'''\n",
    "\t\tКонструктор датасета\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- code_dataset: датасет pd.DataFrame\n",
    "\t\t- tokenizer_code_bert: токенизатор code_bert\n",
    "\t\t- tokenizer_gpt: токенизатор gpt\n",
    "\t\t- max_length: максимальная длина последовательности (default: 512)\n",
    "\t\t'''\n",
    "\t\tself.code_dataset = code_dataset\n",
    "\t\tself.tokenizer_code_bert = tokenizer_code_bert\n",
    "\t\tself.tokenizer_gpt = tokenizer_gpt\n",
    "\t\tself.max_length = max_length\n",
    "\n",
    "\tdef __getitem__(self, idx, idx_to_token=False):\n",
    "\t\t'''\n",
    "\t\tGet-метод - возвращает сэмпл по индексу\n",
    "\n",
    "\t\tПараметры:\n",
    "\t\t- idx: индекс\n",
    "\t\t- idx_to_token: флаг для отображения токенов из индексов (default: False)\n",
    "\t\t'''\n",
    "\t\tfocal_method_input = self.code_dataset.at[idx, 'input_string_focal_method']\n",
    "\t\tfocal_cls_input = self.code_dataset.at[idx, 'input_string_focal_cls']\n",
    "\t\tresponse = self.code_dataset.at[idx, 'response']\n",
    "\n",
    "\t\tdef encode_text(text, tokenizer, tokenizer_flag = True):\n",
    "\t\t\tencoding = tokenizer.encode_plus(\n",
    "\t\t\t\ttext,\n",
    "\t\t\t\tadd_special_tokens=True,\n",
    "\t\t\t\tmax_length=self.max_length if tokenizer_flag else self.max_length * 2,\n",
    "\t\t\t\tpadding='max_length',\n",
    "\t\t\t\ttruncation=True,\n",
    "\t\t\t\treturn_attention_mask=True,\n",
    "\t\t\t\treturn_tensors='pt',\n",
    "\t\t\t)\n",
    "\t\t\tinput_ids = encoding['input_ids'].flatten()\n",
    "\t\t\tattention_mask = encoding['attention_mask'].flatten()\n",
    "\t\t\treturn input_ids, attention_mask\n",
    "\n",
    "\t\tinput_ids_focal_method, attention_mask_focal_method = encode_text(focal_method_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_focal_cls, attention_mask_focal_cls = encode_text(focal_cls_input, self.tokenizer_code_bert)\n",
    "\t\tinput_ids_response, attention_mask_response = encode_text(response, self.tokenizer_gpt, tokenizer_flag = False)\n",
    "\t\tinput_ids_focal_method_decoder, attention_mask_focal_method_decoder = encode_text(focal_method_input, self.tokenizer_gpt)\n",
    "\t\tinput_ids_focal_cls_decoder, attention_mask_focal_cls_decoder = encode_text(focal_cls_input, self.tokenizer_gpt)\n",
    "\n",
    "\t\tif idx_to_token:\n",
    "\t\t\treturn {\n",
    "\t\t\t\t'input_ids_focal_method': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_method),\n",
    "\t\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t\t'input_ids_focal_cls': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_cls),\n",
    "\t\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t\t'ids_response': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_response),\n",
    "\t\t\t\t'attention_mask_response': attention_mask_response, \n",
    "\t\t\t\t'input_ids_focal_method_decoder': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_focal_method_decoder),\n",
    "\t\t\t\t'attention_mask_focal_method_decoder': attention_mask_focal_method_decoder,\n",
    "\t\t\t\t'input_ids_focal_cls_decoder': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_focal_cls_decoder),\n",
    "\t\t\t\t'attention_mask_focal_cls_decoder': attention_mask_focal_cls_decoder\n",
    "\t\t\t}\n",
    "\t\treturn {\n",
    "\t\t\t'input_ids_focal_method': input_ids_focal_method,\n",
    "\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
    "\t\t\t'input_ids_focal_cls': input_ids_focal_cls,\n",
    "\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
    "\t\t\t'ids_response': input_ids_response,\n",
    "\t\t\t'attention_mask_response': attention_mask_response,\n",
    "\t\t\t'input_ids_focal_method_decoder': input_ids_focal_method_decoder,\n",
    "\t\t\t'attention_mask_focal_method_decoder': attention_mask_focal_method_decoder,\n",
    "\t\t\t'input_ids_focal_cls_decoder': input_ids_focal_cls_decoder,\n",
    "\t\t\t'attention_mask_focal_cls_decoder': attention_mask_focal_cls_decoder\n",
    "\t\t}\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\t'''Функция возвращает длину датасета. В качестве длины берется размер датасета по axis = 0'''\n",
    "\t\treturn self.code_dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnLXXplbVog1"
   },
   "source": [
    "Тестируем написанный класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.404557Z",
     "iopub.status.busy": "2024-12-12T06:15:58.404206Z",
     "iopub.status.idle": "2024-12-12T06:15:58.413360Z",
     "shell.execute_reply": "2024-12-12T06:15:58.412558Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.404498Z"
    },
    "id": "1Y0OaQr4Vog2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "code2test_dataset = Code2TestDataset(code_dataset=code_dataset,\n",
    "                                     tokenizer_code_bert=CodeModel.tokenizer_code_bert,\n",
    "                                     tokenizer_gpt=CodeModel.tokenizerGPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<FUNC_TOKEN>', 'Ġdef', 'Ġget', '(', 'self', ',', 'Ġkey', ',', 'Ġdefault', '=', 'None', '):', 'Ġk', 'l', 'Ġ=', 'Ġkey', '.', 'lower', '()', 'Ġreturn', 'Ġsuper', '().', 'get', '(', 'self', '.', 'key', 'map', '.', 'get', '(', 'kl', ',', 'Ġk', 'l', '),', 'Ġdefault', ')', 'Ġ', '<INFO_TOKEN>', 'Ġ', '<AST_TOKEN>', 'ĠModule', '(', 'Ġbody', '=[', 'ĠFunction', 'Def', '(', 'Ġname', \"='\", 'get', \"',\", 'Ġargs', '=', 'arg', 'uments', '(', 'Ġpos', 'only', 'args', '=[', '],', 'Ġargs', '=[', 'Ġarg', '(', 'arg', \"='\", 'self', \"'),\", 'Ġarg', '(', 'arg', \"='\", 'key', \"'),\", 'Ġarg', '(', 'arg', \"='\", 'default', \"')\", '],', 'Ġk', 'w', 'only', 'args', '=[', '],', 'Ġk', 'w', '_', 'default', 's', '=[', '],', 'Ġdefaults', '=[', 'ĠConstant', '(', 'value', '=', 'None', ')]', '),', 'Ġbody', '=[', 'ĠAss', 'ign', '(', 'Ġtargets', '=[', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Store', '())', '],', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'key', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'lower', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', '],', 'Ġkeywords', '=[', '])', '),', 'ĠReturn', '(', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Call', '(', 'Ġfunc', '=', 'Name', '(', 'id', \"='\", 'super', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', '],', 'Ġkeywords', '=', '[]', '),', 'Ġatt', 'r', \"='\", 'get', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠCall', '(', 'Ġfunc', '=', 'Attribute', '(', 'Ġvalue', '=', 'Attribute', '(', 'Ġvalue', '=', 'Name', '(', 'id', \"='\", 'self', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'key', 'map', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġatt', 'r', \"='\", 'get', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'Ġargs', '=[', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Load', '()', '),', 'ĠName', '(', 'id', \"='\", 'kl', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '),', 'ĠName', '(', 'id', \"='\", 'default', \"',\", 'Ġc', 'tx', '=', 'Load', '())', '],', 'Ġkeywords', '=', '[]', '))', '],', 'Ġdecor', 'ator', '_', 'list', '=[', '],', 'Ġtype', '_', 'params', '=[', '])', '],', 'Ġtype', '_', 'ign', 'ores', '=[', '])', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(0, True)['input_ids_focal_method_decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<CLS_TOKEN>', 'Ġ', '<FUNC_TOKEN>', 'Ġ', '<INFO_TOKEN>', 'Ġ', '<AST_TOKEN>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "print(code2test_dataset.__getitem__(0, True)['input_ids_focal_cls_decoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([code2test_dataset[0]['attention_mask_focal_method_decoder'],\n",
    "\t\t\tcode2test_dataset[0]['attention_mask_focal_cls_decoder']])[512:1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.525416Z",
     "iopub.status.busy": "2024-12-12T06:15:58.524794Z",
     "iopub.status.idle": "2024-12-12T06:15:58.533385Z",
     "shell.execute_reply": "2024-12-12T06:15:58.532537Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.525379Z"
    },
    "id": "n09zZavXVog2",
    "outputId": "116bc7b9-06df-4c8c-99f0-f9f9ba88bff5",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Длина датасета составляет: 280458\n"
     ]
    }
   ],
   "source": [
    "print(f\"Длина датасета составляет: {len(code2test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYarvk5JVog2"
   },
   "source": [
    "Всё работает корректно! Следующим шагом необходимо разбить датасет на train и val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.534956Z",
     "iopub.status.busy": "2024-12-12T06:15:58.534371Z",
     "iopub.status.idle": "2024-12-12T06:15:58.563399Z",
     "shell.execute_reply": "2024-12-12T06:15:58.562563Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.534920Z"
    },
    "id": "CNEwuOEoVog2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_datasets(dataset_cls = Code2TestDataset,\n",
    "\t\t\t\tmax_length = 512,\n",
    "\t\t\t\tdata = code_dataset,\n",
    "\t\t\t\ttokenizer_code_bert = CodeModel.tokenizer_code_bert,\n",
    "\t\t\t\ttokenizer_gpt = CodeModel.tokenizerGPT,\n",
    "\t\t\t\ttrain_size = 0.7):\n",
    "\t'''\n",
    "\tФункция get_datasets() возвращает train и val датасеты на основе конструктора AccentDataset, делая train_val_spilt\n",
    "\n",
    "\tПараметры:\n",
    "\t-dataset_cls: класс датасета, конструктор которого будет вызываться (default: Code2TestDataset)\n",
    "\t-max_length: максимальная статья последовательности токенов\n",
    "\t-data: датасает pd.DataFrame (default: code_dataset)\n",
    "\t-tokenizer_code_bert: токенизатор codeBERT (default: tokenizer_code_bert)\n",
    "\t-tokenizer_gpt: токенизатор GPT2 (default: tokenizer_gpt)\n",
    "\t-train_size: размер тренировочной выборки (default: 0.7)\n",
    "\n",
    "\t'''\n",
    "\n",
    "\tdataset = dataset_cls(code_dataset = data,\n",
    "\t\t\t\t\t   \ttokenizer_code_bert = tokenizer_code_bert,\n",
    "\t\t\t\t\t\ttokenizer_gpt=tokenizer_gpt,\n",
    "\t\t\t\t\t\tmax_length=max_length)\n",
    "\n",
    "\ttrain_size = int(train_size * len(dataset))\n",
    "\tval_size = len(dataset) - train_size\n",
    "\ttrain_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "\treturn train_dataset, test_dataset\n",
    "\n",
    "train_dataset, val_dataset = get_datasets(train_size=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztZgraV0Vog3"
   },
   "source": [
    "Проверяем полученные датасеты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.564657Z",
     "iopub.status.busy": "2024-12-12T06:15:58.564392Z",
     "iopub.status.idle": "2024-12-12T06:15:58.569309Z",
     "shell.execute_reply": "2024-12-12T06:15:58.568439Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.564633Z"
    },
    "id": "LULHQUCnVog3",
    "outputId": "cb026161-05c8-4a3d-92cf-a77789d0f7a1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество данных в train и val выборках соответственно: (252412, 28046)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Количество данных в train и val выборках соответственно: {len(train_dataset), len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.570612Z",
     "iopub.status.busy": "2024-12-12T06:15:58.570336Z",
     "iopub.status.idle": "2024-12-12T06:15:58.582210Z",
     "shell.execute_reply": "2024-12-12T06:15:58.581453Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.570588Z"
    },
    "id": "VopeUmP8Vog3",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(tokens_ids, tokenizer):\n",
    "\t'''Декодирование последовательности токенов'''\n",
    "\tcode_bert_decoded = tokenizer.decode(tokens_ids)\n",
    "\tprint(f\"Декодированная строка: {code_bert_decoded}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eYiR3Y8AVog3"
   },
   "source": [
    "Далее получим DataLoader, по которому будем итерироваться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.647235Z",
     "iopub.status.busy": "2024-12-12T06:15:58.646948Z",
     "iopub.status.idle": "2024-12-12T06:15:58.657089Z",
     "shell.execute_reply": "2024-12-12T06:15:58.656387Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.647174Z"
    },
    "id": "0Gi5MpUUVog4",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def get_loaders(train_dataset = train_dataset,\n",
    "\t\t\tval_dataset = val_dataset,\n",
    "\t\t\tshuffle_train = True,\n",
    "\t\t\tshuffle_val = False,\n",
    "\t\t\tbatch_size = 32):\n",
    "\n",
    "\t'''\n",
    "\tФункция get_loaders() для получения train, val даталоадеров\n",
    "\n",
    "\tПараметры:\n",
    "\t-train_dataset: тренировочный датасет (default: train_dataset)\n",
    "\t-val_dataset: валидационный датасет (default: val_dataset)\n",
    "\t-shuffle_train: флаг перемешивания для train (default: True)\n",
    "\t-shuffle_val: флаг перемешивания для val (default: False)\n",
    "\t-batch_size: размер батча данных (default: 32)\n",
    "\t'''\n",
    "\n",
    "\t# train_dataloader\n",
    "\ttrain_dataloader = DataLoader(\n",
    "\t\t\ttrain_dataset,\n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_train,\n",
    "\t\t)\n",
    "\n",
    "\t# validation_dataloader\n",
    "\tvalidation_dataloader = DataLoader(\n",
    "\t\t\tval_dataset,\n",
    "\t\t\tbatch_size = batch_size,\n",
    "\t\t\tshuffle = shuffle_val,\n",
    "\t\t)\n",
    "\n",
    "\t# Возвращаем даталоадеры\n",
    "\treturn train_dataloader, validation_dataloader\n",
    "\n",
    "train_dataloader, validation_dataloader = get_loaders(batch_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PV1IPmb8Vog4"
   },
   "source": [
    "Проверка итерирования"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.724781Z",
     "iopub.status.busy": "2024-12-12T06:15:58.724413Z",
     "iopub.status.idle": "2024-12-12T06:15:58.777765Z",
     "shell.execute_reply": "2024-12-12T06:15:58.776878Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.724751Z"
    },
    "id": "ysaV_an7Vog4",
    "outputId": "3583c93b-6615-448b-e8e3-d38cdf6e5826",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/63103 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "    if i == 0:\n",
    "        break\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uUlMgUFKVog5"
   },
   "source": [
    "Корректно отрабатывает!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEjTqd5-Vog5"
   },
   "source": [
    "Далее, собираем архитектуру и готовимся обучать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.779397Z",
     "iopub.status.busy": "2024-12-12T06:15:58.779096Z",
     "iopub.status.idle": "2024-12-12T06:15:58.782957Z",
     "shell.execute_reply": "2024-12-12T06:15:58.782039Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.779372Z"
    },
    "id": "YI_0YNygVog5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# model_code_bert = AutoModel.from_pretrained(\"microsoft/codebert-base\", output_hidden_states= True).to(device)\n",
    "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mqIcVC-pVog6"
   },
   "source": [
    "Как работает модель codeBERT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.784258Z",
     "iopub.status.busy": "2024-12-12T06:15:58.783945Z",
     "iopub.status.idle": "2024-12-12T06:15:58.796543Z",
     "shell.execute_reply": "2024-12-12T06:15:58.795837Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.784223Z"
    },
    "id": "wADKQZZrVog6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "# \t# Проверка корректности работы\n",
    "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "# \tprint(last_hidden_state_code_bert.size())\n",
    "# \tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dGD1xAyrVog6"
   },
   "source": [
    "Таким образом, для каждого токена мы получим свое закодированное значение размерности 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ys-7r4xlVog6"
   },
   "source": [
    "Модель GPT2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.802733Z",
     "iopub.status.busy": "2024-12-12T06:15:58.802139Z",
     "iopub.status.idle": "2024-12-12T06:15:58.806865Z",
     "shell.execute_reply": "2024-12-12T06:15:58.806127Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.802702Z"
    },
    "id": "NOe0aQa2Vog6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoConfig\n",
    "\n",
    "# modelGPT2Path = \"gpt2\"\n",
    "# # config = AutoConfig.from_pretrained(modelGPT2Path, is_decoder=True, add_cross_attention= True)\n",
    "# # config.add_cross_attention = True  # Включение cross-attention\n",
    "\n",
    "# # modelGPT2 = AutoModel.from_pretrained(modelGPT2Path, config=config).to(device)\n",
    "# # modelGPT2.resize_token_embeddings(len(tokenizerGPT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOAH9fG3Vog6"
   },
   "source": [
    "Как работает модель GPTBigCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:15:58.808322Z",
     "iopub.status.busy": "2024-12-12T06:15:58.807994Z",
     "iopub.status.idle": "2024-12-12T06:15:58.819396Z",
     "shell.execute_reply": "2024-12-12T06:15:58.818477Z",
     "shell.execute_reply.started": "2024-12-12T06:15:58.808280Z"
    },
    "id": "-hDRPlpQVog7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(train_dataloader):\n",
    "\n",
    "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "\n",
    "# \tprint(last_hidden_state_code_bert.size())\n",
    "\n",
    "# \t# Проверка корректности работы\n",
    "# \tresponse_input_ids = batch['ids_response'].to(device)\n",
    "# \tresponse_input_mask = batch['attention_mask_response'].to(device)\n",
    "# \tgpt_output = modelGPT2(input_ids=response_input_ids,\n",
    "# \t\t\t\t\t\t\t  attention_mask=response_input_mask,\n",
    "# \t\t\t\t\t\t\t  encoder_hidden_states = last_hidden_state_code_bert)\n",
    "# \tprint(gpt_output['last_hidden_state'].size())\n",
    "\n",
    "\n",
    "# \t# outputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
    "# \t# last_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
    "# \t# print(last_hidden_state_code_bert.size())\n",
    "# \tbreak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4sDVYyLVog7"
   },
   "source": [
    "Ну, как-то худо-бедно всё это дело запускается. Пробуем строить модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ4GpL3eVog7"
   },
   "source": [
    "Отлаживаем модель"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SF48wREWVog7"
   },
   "source": [
    "Далее необходимо объявить функцию train-val loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FSZq2n4eVog7"
   },
   "source": [
    "Для начала необходимо объявить дополнительные функции для отображения времени и подсчёта метрик качества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:17:35.226198Z",
     "iopub.status.busy": "2024-12-12T06:17:35.225592Z",
     "iopub.status.idle": "2024-12-12T06:17:35.233921Z",
     "shell.execute_reply": "2024-12-12T06:17:35.233076Z",
     "shell.execute_reply.started": "2024-12-12T06:17:35.226167Z"
    },
    "id": "StkmqyMxVog7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def format_time(elapsed):\n",
    "\t'''Функция форматирования времени'''\n",
    "\treturn str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
    "\n",
    "def token_accuracy_calc(logits, labels, attention_mask):\n",
    "\t'''Функция подсчета accuracy для данных'''\n",
    "\tpred_flat = np.argmax(logits, axis=-1).flatten()\n",
    "\tlabels_flat = labels.flatten()\n",
    "\tmask_flat = attention_mask.flatten()\n",
    "\taccuracy = np.sum(pred_flat[mask_flat] == labels_flat[mask_flat]) / len(labels_flat[mask_flat])\n",
    "\treturn accuracy\n",
    "\n",
    "def token_bleu_calc(logits, labels, attention_mask):\n",
    "\t'''Функция подсчета BLEU для данных'''\n",
    "\n",
    "\tpred_flat = np.argmax(logits, axis=-1)\n",
    "\tlabels_flat = labels\n",
    "\tmask_flat = attention_mask\n",
    "\n",
    "\tpred_tokens = []\n",
    "\ttrue_tokens = []\n",
    "\n",
    "\tfor i in range(pred_flat.shape[0]):\n",
    "\t\tpred_seq = pred_flat[i][mask_flat[i] == 1]\n",
    "\t\ttrue_seq = labels_flat[i][mask_flat[i] == 1]\n",
    "\t\tpred_tokens.append(pred_seq)\n",
    "\t\ttrue_tokens.append(true_seq)\n",
    "\n",
    "\tpred_strings = CodeModel.tokenizerGPT.batch_decode(pred_tokens, skip_special_tokens=True)\n",
    "\ttrue_strings = CodeModel.tokenizerGPT.batch_decode(true_tokens, skip_special_tokens=True)\n",
    "\n",
    "\t# Вычисление BLEU\n",
    "\tbleu_scores = []\n",
    "\tsmoothing_function = SmoothingFunction().method1\n",
    "\n",
    "\tfor pred, true in zip(pred_strings, true_strings):\n",
    "\t\tpred_tokens = pred.split()\n",
    "\t\ttrue_tokens = [true.split()]\n",
    "\t\tbleu_score = sentence_bleu(true_tokens, pred_tokens, smoothing_function=smoothing_function)\n",
    "\t\tbleu_scores.append(bleu_score)\n",
    "\n",
    "\taverage_bleu = np.mean(bleu_scores)\n",
    "\treturn average_bleu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Yk8znRoVog8"
   },
   "source": [
    "Как минимимум оно запускается"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S_zQsOmVog8"
   },
   "source": [
    "Далее реализуем саму функцию train-val-loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zi040W98Vog8"
   },
   "source": [
    "Перед этим объями дополнительные настройки обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:18:59.067221Z",
     "iopub.status.busy": "2024-12-12T06:18:59.066535Z",
     "iopub.status.idle": "2024-12-12T06:18:59.076961Z",
     "shell.execute_reply": "2024-12-12T06:18:59.076008Z",
     "shell.execute_reply.started": "2024-12-12T06:18:59.067191Z"
    },
    "id": "AkfdOP5PVog8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_scheduler\n",
    "\n",
    "optimizer = AdamW(CodeModel.parameters(), lr=3e-5)\n",
    "num_epochs = 1\n",
    "train_steps = len(train_dataloader) * num_epochs\n",
    "lr_scheduler = get_scheduler(\n",
    "    name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=train_steps\n",
    ")\n",
    "tensorboard_log_dir = 'runs/CodeModelLogs/'\n",
    "tensorboard_path_accuracy = 'runs/model_accuracy_score.pth'\n",
    "tensorboard_path_loss = 'runs/model_val_loss.pth'\n",
    "tensorboard_path_bleu = 'runs/model_bleu_score.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMkcIBXEVog8"
   },
   "source": [
    "И, наконец, функция:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T06:19:00.573329Z",
     "iopub.status.busy": "2024-12-12T06:19:00.573023Z",
     "iopub.status.idle": "2024-12-12T06:19:00.594827Z",
     "shell.execute_reply": "2024-12-12T06:19:00.593904Z",
     "shell.execute_reply.started": "2024-12-12T06:19:00.573302Z"
    },
    "id": "fsb-uwMuVog8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_val_loop_codeLM(model = CodeModel,\n",
    "\t\t\t\t\t\ttrain_loader = train_dataloader,\n",
    "\t\t\t\t\t\tval_loader = validation_dataloader,\n",
    "\t\t\t\t\t\toptimizer = optimizer,\n",
    "\t\t\t\t\t\tscheduler = lr_scheduler,\n",
    "\t\t\t\t\t\tnum_epochs = num_epochs,\n",
    "\t\t\t\t\t\tdevice = 'cuda',\n",
    "\t\t\t\t\t\tmodel_save_path_accuracy = tensorboard_path_accuracy,\n",
    "\t\t\t\t\t\tmodel_save_path_loss = tensorboard_path_loss,\n",
    "\t\t\t\t\t\tmodel_save_path_bleu = tensorboard_path_bleu,\n",
    "\t\t\t\t\t\ttensorboard_log_dir = tensorboard_log_dir,\n",
    "\t\t\t\t\t\tgradient_accumulation_steps = 2,\n",
    "\t\t\t\t\t\teval_every = 1,\n",
    "\t\t\t\t\t\ttest_step_only = False):\n",
    "\t'''\n",
    "\tФункция для реализации train-val loop обучения нашей модели\n",
    "\n",
    "\tПараметры:\n",
    "\t-model: модель нейронной сети\n",
    "\t-train_loader: тренировочный датасет\n",
    "\t-val_loader: валидационный датасет\n",
    "\t-optimizer: оптимизатор\n",
    "\t-scheduler: изменение для learning_rate (расписание)\n",
    "\t-num_epochs: число эпох для обучения\n",
    "\t-device: устройство\n",
    "\t-model_save_path_accuracy: путь для сохранения весов модели (с лучшей accuracy)\n",
    "\t-model_save_path_loss: путь для сохранения весов модели (с лучим val_loss)\n",
    "\t-model_save_path_bleu: путь для сохранения весов модели (с лучим val_bleu_score)\n",
    "\t-tensorboard_log_dir: путь для записи логов в TensorBoard,\n",
    "\t-gradient_accumulation_steps: число шагов для накопления градиентов\n",
    "\t-eval_every: число шагов, через которые делаем валидацию\n",
    "\t-test_step_only: вспомогательная логика для тестирования обучения небольшого числа шагов (default: False)\n",
    "\t'''\n",
    "\n",
    "\twriter = SummaryWriter(log_dir=tensorboard_log_dir)\n",
    "\thistory = {\n",
    "\t\t'train_loss': [],\n",
    "\t\t'train_bleu': [],\n",
    "\t\t'train_accuracy': [],\n",
    "\t\t'val_accuracy': [],\n",
    "\t\t'val_loss': [],\n",
    "\t\t'val_bleu': []\n",
    "\t}\n",
    "\tbest_val_loss = float('inf')\n",
    "\tbest_val_bleu = 0.0\n",
    "\tbest_val_accuracy = 0.0\n",
    "\tmodel.to(device)\n",
    "\n",
    "\tfor epoch in range(num_epochs):\n",
    "\t\tprint(\"\")\n",
    "\t\tprint('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
    "\t\tprint('Training...')\n",
    "\n",
    "\t\tt0 = time.time()\n",
    "\t\tmodel.train()\n",
    "\t\ttotal_train_loss = 0\n",
    "\t\ttotal_train_bleu = 0\n",
    "\t\ttotal_train_accuracy = 0\n",
    "\t\tnum_train_steps = 0\n",
    "\n",
    "\t\tfor step, batch in enumerate(tqdm(train_loader)):\n",
    "\n",
    "\t\t\tif test_step_only and step >= 800:  # Прерываем после первого батча\n",
    "\t\t\t\tbreak\n",
    "\n",
    "\n",
    "\t\t\tif step % 1500 == 0 and not step == 0:\n",
    "\t\t\t\t# Calculate elapsed time in minutes.\n",
    "\t\t\t\telapsed = format_time(time.time() - t0)\n",
    "\t\t\t\t# Report progress.\n",
    "\t\t\t\tprint('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\t\t\toptimizer.zero_grad()\n",
    "\n",
    "\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
    "\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "\t\t\tinput_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
    "\t\t\tinput_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
    "\n",
    "\t\t\tattention_mask_focal_method_decoder = batch['attention_mask_focal_method_decoder'].to(device)\n",
    "\t\t\tattention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
    "\n",
    "\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
    "\t\t\t\t\t\tinput_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
    "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
    "\n",
    "\t\t\tloss = output_codeLM.loss\n",
    "\t\t\tlogits = output_codeLM.logits\n",
    "\n",
    "\t\t\tlogits = logits.detach().cpu().numpy()\n",
    "\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
    "\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "\t\t\taccuracy_train = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
    "\t\t\tbleu_train = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
    "\n",
    "\t\t\ttotal_train_accuracy += accuracy_train\n",
    "\t\t\ttotal_train_bleu += bleu_train\n",
    "\n",
    "\t\t\ttotal_train_loss += loss.item()\n",
    "\t\t\tnum_train_steps += 1\n",
    "\n",
    "\t\t\tloss.backward()\n",
    "\n",
    "\t\t\tif (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
    "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Клипаем накопленные градиенты\n",
    "\t\t\t\toptimizer.step()\n",
    "\t\t\t\tscheduler.step()\n",
    "\n",
    "\t\tavg_train_loss = total_train_loss / num_train_steps\n",
    "\t\tavg_train_accuracy = total_train_accuracy / num_train_steps\n",
    "\t\tavg_train_bleu_score = total_train_bleu / num_train_steps\n",
    "\n",
    "\t\ttraining_time = format_time(time.time() - t0)\n",
    "\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "\t\t# print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
    "\t\t# print(\"  Average training BLEU score: {0:.2f}\".format(avg_train_bleu_score))\n",
    "\t\tprint(\"  Training epoch took: {:}\".format(training_time))\n",
    "\n",
    "\t\thistory['train_loss'].append(avg_train_loss)\n",
    "\t\thistory['train_accuracy'].append(avg_train_accuracy)\n",
    "\t\thistory['train_bleu'].append(avg_train_bleu_score)\n",
    "\n",
    "\t\t# Логирование в TensorBoard для обучения\n",
    "\t\twriter.add_scalar(\"Train/Loss\", avg_train_loss, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Train/Accuracy\", avg_train_accuracy, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Train/BLEU_score\", avg_train_bleu_score, epoch + 1)\n",
    "\n",
    "\t\tprint(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, Train BLEU score: {avg_train_bleu_score:.4f}\")\n",
    "\n",
    "\t\tprint(\"\")\n",
    "\t\tprint(\"Running Validation...\")\n",
    "\n",
    "\n",
    "\t\tt0 = time.time()\n",
    "\n",
    "\t\t# Put the model in evaluation mode--the dropout layers behave differently\n",
    "\t\t# during evaluation.\n",
    "\t\tmodel.eval()\n",
    "\n",
    "\t\tif (epoch + 1) % eval_every == 0:\n",
    "\t\t\tmodel.eval()\n",
    "\t\t\ttotal_eval_loss = 0\n",
    "\t\t\ttotal_eval_accuracy = 0\n",
    "\t\t\ttotal_eval_bleu = 0\n",
    "\t\t\tnum_eval_steps = 0\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\tfor batch in tqdm(val_loader):\n",
    "\t\t\t\tif test_step_only and num_eval_steps >= 200:  # Прерываем если хотим проконтроллировать\n",
    "\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "\t\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "\t\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "\t\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "\t\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
    "\t\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "\t\t\t\tinput_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
    "\t\t\t\tinput_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
    "\n",
    "\t\t\t\tattention_mask_focal_method_decoder = batch['attention_mask_focal_method_decoder'].to(device)\n",
    "\t\t\t\tattention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
    "\n",
    "\t\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
    "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
    "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
    "\t\t\t\t\t\tinput_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
    "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
    "\n",
    "\t\t\t\tloss = output_codeLM.loss\n",
    "\t\t\t\tlogits = output_codeLM.logits\n",
    "\n",
    "\t\t\t\tlogits = logits.detach().cpu().numpy()\n",
    "\t\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
    "\t\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "\t\t\t\taccuracy_val = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
    "\t\t\t\tbleu_val = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
    "\n",
    "\t\t\t\ttotal_eval_accuracy += accuracy_val\n",
    "\t\t\t\ttotal_eval_bleu += bleu_val\n",
    "\n",
    "\t\t\t\ttotal_eval_loss += loss.item()\n",
    "\t\t\t\tnum_eval_steps += 1\n",
    "\n",
    "\t\tavg_val_loss = total_eval_loss / num_eval_steps\n",
    "\t\tavg_val_accuracy = total_eval_accuracy / num_eval_steps\n",
    "\t\tavg_val_bleu_score = total_eval_bleu / num_eval_steps\n",
    "\n",
    "\t\thistory['val_loss'].append(avg_val_loss)\n",
    "\t\thistory['val_accuracy'].append(avg_val_accuracy)\n",
    "\t\thistory['val_bleu'].append(avg_val_bleu_score)\n",
    "\n",
    "\t\t# Логирование в TensorBoard для валидации\n",
    "\t\twriter.add_scalar(\"Validation/Loss\", avg_val_loss, epoch + 1)\n",
    "\t\twriter.add_scalar(\"Validation/Accuracy\", avg_val_accuracy, epoch + 1)\n",
    "\n",
    "\t\tprint(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f},  Validation BLEU score: {avg_val_bleu_score:.4f}\")\n",
    "\n",
    "\t\t# Ну вот тут надо настроить, чтобы\n",
    "\t\t# Если точность выше, то сохраняем веса\n",
    "\t\tif avg_val_accuracy > best_val_accuracy:\n",
    "\t\t\tbest_val_accuracy = avg_val_accuracy\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_accuracy)\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_accuracy}\")\n",
    "\n",
    "\t\t# Если лосс ниже, то сохраняем веса\n",
    "\t\tif avg_val_loss < best_val_loss:\n",
    "\t\t\tbest_val_loss = avg_val_loss\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_loss)\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_loss}\")\n",
    "\n",
    "\t\t# Если BLEU выше, то сохраняем веса\n",
    "\t\tif avg_val_bleu_score > best_val_bleu:\n",
    "\t\t\tbest_val_bleu = avg_val_bleu_score\n",
    "\t\t\ttorch.save(model.state_dict(), model_save_path_bleu)\n",
    "\t\t\tprint(f\"Model saved to {model_save_path_bleu}\")\n",
    "\n",
    "\twriter.close()\n",
    "\treturn history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-r6lh1igVog9"
   },
   "source": [
    "Наконец, пробуем запустить обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "rMHiyXdsVog9",
    "outputId": "bfd0c4bc-c63b-4335-b3e9-f6fa6426008c",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_results = train_val_loop_codeLM(device='cpu', test_step_only = True, num_epochs = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(len(CodeModel.att))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hiOV6rtIVog9",
    "outputId": "2bd630b2-a216-4bf1-88fb-e470eadb3380"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7012 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Декодированная строка: <s><FUNC_TOKEN> def align_sequences(read_seq, read_qual, ref_seq, alignment, gap_char='-'): read, qual, ref = [], [], [] read_pos, ref_pos = 0, 0 errors_per_read_pos = [0] * len(read_seq) for c in alignment.cigar_parts: cigar_type = c[-1] cigar_size = int(c[:-1]) if cigar_type == 'M': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) for i in range(cigar_size): if read_seq[read_pos+i] != ref_seq[ref_pos+i]: errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size ref_pos += cigar_size if cigar_type == 'I': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(gap_char * cigar_size) for i in range(cigar_size): errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size if cigar_type == 'D': read.append(gap_char * cigar_size) qual.append(gap_char * cigar_size) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) errors_per_read_pos[read_pos] += cigar_size ref_pos += cigar_size return ''.join(read), ''.join(qual), ''.join(ref), errors_per_read_pos <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='align_sequences', args=arguments( posonlyargs=[], args=[ arg(arg='read_seq'), arg(arg='read_qual'), arg(arg='ref_seq'), arg(arg='alignment'), arg(arg='gap_char')], kwonlyargs=[], kw_defaults=[],</s>\n",
      "Декодированная строка: <s><CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
      "Декодированная строка: <FUNC_TOKEN> def align_sequences(read_seq, read_qual, ref_seq, alignment, gap_char='-'): read, qual, ref = [], [], [] read_pos, ref_pos = 0, 0 errors_per_read_pos = [0] * len(read_seq) for c in alignment.cigar_parts: cigar_type = c[-1] cigar_size = int(c[:-1]) if cigar_type == 'M': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) for i in range(cigar_size): if read_seq[read_pos+i] != ref_seq[ref_pos+i]: errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size ref_pos += cigar_size if cigar_type == 'I': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(gap_char * cigar_size) for i in range(cigar_size): errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size if cigar_type == 'D': read.append(gap_char * cigar_size) qual.append(gap_char * cigar_size) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) errors_per_read_pos[read_pos] += cigar_size ref_pos += cigar_size return ''.join(read), ''.join(qual), ''.join(ref), errors_per_read_pos <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='align_sequences', args=arguments( posonlyargs=[], args=[ arg(arg='read_seq'), arg(arg='read_qual'), arg(arg='ref_seq'), arg(arg='alignment'), arg(arg='gap_char')], k\n",
      "Декодированная строка: <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
      "Prediction:\n",
      "Декодированная строка: \n",
      "..\n",
      "_ import\n",
      " import\n",
      "\n",
      "_\n",
      ".\n",
      "      \n",
      "                                                                                                                                     <PAD>        <PAD>   <PAD><PAD> <PAD> <PAD><PAD><PAD> <PAD><PAD><PAD><PAD><PAD>    <PAD> <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
    "\n",
    "      # print(batch)\n",
    "\n",
    "      focal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
    "      focal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
    "\n",
    "      focal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
    "      focal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
    "\n",
    "      response_ids = batch['ids_response'].to(device)\n",
    "      response_attention_masks = batch['attention_mask_response'].to(device)\n",
    "\n",
    "      input_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
    "      input_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
    "\n",
    "      attention_mask_focal_method_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
    "      attention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
    "\n",
    "      output_codeLM = CodeModel(focal_method_input_ids, focal_method_attention_masks,\n",
    "          focal_cls_input_ids, focal_cls_attention_masks,\n",
    "          response_ids, response_attention_masks,\n",
    "          input_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
    "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
    "\n",
    "      loss = output_codeLM.loss\n",
    "      logits = output_codeLM.logits\n",
    "\n",
    "      logits = logits.detach().cpu().numpy()\n",
    "      response_ids = response_ids.cpu().numpy()\n",
    "      response_attention_masks = response_attention_masks.cpu().numpy()\n",
    "\n",
    "      pred_flat = np.argmax(logits, axis=-1)\n",
    "      # print(\"Shape:\")\n",
    "      # print(pred_flat[0][:20])\n",
    "\n",
    "      print()\n",
    "      # print(\"Исходная строка:\")\n",
    "      decode_sequence(focal_method_input_ids[3], CodeModel.tokenizer_code_bert)\n",
    "      decode_sequence(focal_cls_input_ids[3], CodeModel.tokenizer_code_bert)\n",
    "\n",
    "      # decode_sequence(focal_method_input_ids[0], CodeModel.tokenizer_code_bert)\n",
    "\n",
    "      # print(\"GT:\")\n",
    "      decode_sequence(input_ids_focal_method_decoder[3], CodeModel.tokenizerGPT)\n",
    "      decode_sequence(input_ids_focal_cls_decoder[3], CodeModel.tokenizerGPT)\n",
    "\n",
    "      print(\"Prediction:\")\n",
    "      decode_sequence(pred_flat[2], CodeModel.tokenizerGPT)\n",
    "\n",
    "      break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BM1oR57Xm0pw"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6256753,
     "sourceId": 10137849,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6256764,
     "sourceId": 10137869,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6283970,
     "sourceId": 10174103,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6285702,
     "sourceId": 10176633,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30804,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
