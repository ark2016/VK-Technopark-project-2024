{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfmIB0OoVogr"
      },
      "source": [
        "Архитектура модели анализа кода"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VteU_AcwVogu"
      },
      "source": [
        "В данном файле проводится анализ архитектуры модели, токенизатора и подготовка к обучению модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eHJpaGaVogu"
      },
      "source": [
        "Импортируем необходимые модули"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:13.382026Z",
          "iopub.status.busy": "2024-12-12T06:15:13.381216Z",
          "iopub.status.idle": "2024-12-12T06:15:30.190140Z",
          "shell.execute_reply": "2024-12-12T06:15:30.189378Z",
          "shell.execute_reply.started": "2024-12-12T06:15:13.381983Z"
        },
        "trusted": true,
        "id": "DvRmY-2wVogu"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "from torch.utils.tensorboard import summary, writer, SummaryWriter\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPdie2iuVogw"
      },
      "source": [
        "Устанавливаем SEED"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:30.193189Z",
          "iopub.status.busy": "2024-12-12T06:15:30.192033Z",
          "iopub.status.idle": "2024-12-12T06:15:30.201396Z",
          "shell.execute_reply": "2024-12-12T06:15:30.200706Z",
          "shell.execute_reply.started": "2024-12-12T06:15:30.193156Z"
        },
        "trusted": true,
        "id": "vkUUT9UWVogw"
      },
      "outputs": [],
      "source": [
        "SEED = 42\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RD-KDIUtV8mB",
        "outputId": "b7517b31-e5d6-4f3d-b495-6432f8f04fb5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_m0pJFBuVogw"
      },
      "source": [
        "Далее считываем исходный датасет и немного дорабатываем его"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/MyDrive/upd_code_dataset.parquet'"
      ],
      "metadata": {
        "id": "Hx5Vi6NMWGGL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:30.202945Z",
          "iopub.status.busy": "2024-12-12T06:15:30.202362Z",
          "iopub.status.idle": "2024-12-12T06:15:52.551962Z",
          "shell.execute_reply": "2024-12-12T06:15:52.551214Z",
          "shell.execute_reply.started": "2024-12-12T06:15:30.202918Z"
        },
        "trusted": true,
        "id": "rful6GZ0Vogw"
      },
      "outputs": [],
      "source": [
        "code_dataset = pd.read_parquet(dataset_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:52.553687Z",
          "iopub.status.busy": "2024-12-12T06:15:52.553301Z",
          "iopub.status.idle": "2024-12-12T06:15:52.573492Z",
          "shell.execute_reply": "2024-12-12T06:15:52.572692Z",
          "shell.execute_reply.started": "2024-12-12T06:15:52.553649Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "D3cjvyVZVogx",
        "outputId": "1ceb1be0-110a-403b-ca61-b0fdc5aa4234"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            response  \\\n",
              "0  from microdot import Microdot, Response, abort...   \n",
              "1  from microdot import Microdot, Response, abort...   \n",
              "2  from microdot import Microdot, Response, abort...   \n",
              "3  from microdot import Microdot, Response, abort...   \n",
              "4  from pyner.named_entity.corpus import bio2bioe...   \n",
              "\n",
              "                                        focal_method  \\\n",
              "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
              "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
              "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
              "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
              "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
              "\n",
              "                                           focal_cls  \\\n",
              "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
              "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
              "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
              "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
              "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
              "\n",
              "                                    focal_method_ast  \\\n",
              "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "\n",
              "                                       focal_cls_ast  \\\n",
              "0                                        <AST_TOKEN>   \n",
              "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
              "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
              "3                                        <AST_TOKEN>   \n",
              "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "\n",
              "                                   focal_method_info  \\\n",
              "0                                       <INFO_TOKEN>   \n",
              "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
              "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
              "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
              "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
              "\n",
              "                                      focal_cls_info  \\\n",
              "0                                       <INFO_TOKEN>   \n",
              "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
              "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
              "3                                       <INFO_TOKEN>   \n",
              "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
              "\n",
              "                           input_string_focal_method  \\\n",
              "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
              "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
              "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
              "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
              "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
              "\n",
              "                              input_string_focal_cls  \n",
              "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
              "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
              "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
              "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
              "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e24ca3ec-fea4-4d49-9537-25b6a69aa950\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>focal_method</th>\n",
              "      <th>focal_cls</th>\n",
              "      <th>focal_method_ast</th>\n",
              "      <th>focal_cls_ast</th>\n",
              "      <th>focal_method_info</th>\n",
              "      <th>focal_cls_info</th>\n",
              "      <th>input_string_focal_method</th>\n",
              "      <th>input_string_focal_cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e24ca3ec-fea4-4d49-9537-25b6a69aa950')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e24ca3ec-fea4-4d49-9537-25b6a69aa950 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e24ca3ec-fea4-4d49-9537-25b6a69aa950');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-8e7d4b30-fa52-44c2-a5ca-93a171293485\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8e7d4b30-fa52-44c2-a5ca-93a171293485')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-8e7d4b30-fa52-44c2-a5ca-93a171293485 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "code_dataset"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "code_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:52.575470Z",
          "iopub.status.busy": "2024-12-12T06:15:52.575217Z",
          "iopub.status.idle": "2024-12-12T06:15:52.642582Z",
          "shell.execute_reply": "2024-12-12T06:15:52.641652Z",
          "shell.execute_reply.started": "2024-12-12T06:15:52.575446Z"
        },
        "trusted": true,
        "id": "Nd1CvXC4Vogx"
      },
      "outputs": [],
      "source": [
        "code_dataset = code_dataset.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "POH-HrSeVogy"
      },
      "source": [
        "Наконец, переходим к анализу архитектур нейросетей"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ck51yW9Vogy"
      },
      "source": [
        "Решено использовать подход, основанный на обучении (fine-tuning) нейросети CodeBERT, в основе которой лежит модель RoBERTa. Далее будем использовать метамодель в виде декодера (CodeGen или GPTBigCode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:52.649211Z",
          "iopub.status.busy": "2024-12-12T06:15:52.648956Z",
          "iopub.status.idle": "2024-12-12T06:15:54.090651Z",
          "shell.execute_reply": "2024-12-12T06:15:54.089570Z",
          "shell.execute_reply.started": "2024-12-12T06:15:52.649177Z"
        },
        "trusted": true,
        "id": "PJfhnrquVogy"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUNfTCywVogy"
      },
      "source": [
        "Device:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:54.092773Z",
          "iopub.status.busy": "2024-12-12T06:15:54.092032Z",
          "iopub.status.idle": "2024-12-12T06:15:54.263313Z",
          "shell.execute_reply": "2024-12-12T06:15:54.262376Z",
          "shell.execute_reply.started": "2024-12-12T06:15:54.092724Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30M8OI7XVogy",
        "outputId": "16337460-6fa7-4cad-d8a9-a828266be5a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlsY_ktWWTV4",
        "outputId": "7168e93e-6f73-48f3-bf37-d28215babf10"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Dec 12 19:43:37 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   28C    P0              43W / 400W |      5MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_vs7aDTVogz"
      },
      "source": [
        "Токенизаторы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:54.264744Z",
          "iopub.status.busy": "2024-12-12T06:15:54.264409Z",
          "iopub.status.idle": "2024-12-12T06:15:58.345038Z",
          "shell.execute_reply": "2024-12-12T06:15:58.344170Z",
          "shell.execute_reply.started": "2024-12-12T06:15:54.264710Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tundjLADVogz",
        "outputId": "de74886c-558b-4391-904a-b4129a0de0e6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "tokenizer_code_bert = AutoTokenizer.from_pretrained(\"microsoft/codebert-base\")\n",
        "tokenizerGPT = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9lPDVJkVogz"
      },
      "source": [
        "Посмотрим как работает базовый токенизатор для CodeBERT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjBXDw7SVogz"
      },
      "source": [
        "Перед этим добавим новые служебные токены:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.346632Z",
          "iopub.status.busy": "2024-12-12T06:15:58.346241Z",
          "iopub.status.idle": "2024-12-12T06:15:58.353377Z",
          "shell.execute_reply": "2024-12-12T06:15:58.352545Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.346593Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dE95GTppVog0",
        "outputId": "e6e5b191-2b8d-48ad-af17-960855633b96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "new_special_tokens = ['<FUNC_TOKEN>',\n",
        "            '<INFO_TOKEN>',\n",
        "            '<CLS_TOKEN>',\n",
        "            '<AST_TOKEN>',\n",
        "            '<DESCRIPTION_TOKEN>',\n",
        "            '<COMMENTS_TOKEN>']\n",
        "\n",
        "special_tokens_dict = {\n",
        "    'additional_special_tokens': new_special_tokens\n",
        "}\n",
        "\n",
        "tokenizer_code_bert.add_special_tokens(special_tokens_dict)\n",
        "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.354761Z",
          "iopub.status.busy": "2024-12-12T06:15:58.354472Z",
          "iopub.status.idle": "2024-12-12T06:15:58.369240Z",
          "shell.execute_reply": "2024-12-12T06:15:58.368554Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.354735Z"
        },
        "trusted": true,
        "id": "8qquJi09Vog0"
      },
      "outputs": [],
      "source": [
        "def tokenization_example(input_str: str):\n",
        "\t'''Функция отображения результатов токенизации'''\n",
        "\tcode_bert_tokens_example = tokenizer_code_bert.tokenize(input_str)\n",
        "\tcode_bert_tokens_ids = tokenizer_code_bert.convert_tokens_to_ids(code_bert_tokens_example)\n",
        "\tcode_bert_decoded = tokenizer_code_bert.decode(code_bert_tokens_ids)\n",
        "\tprint(f\"Длина закодированной последовательности: {len(code_bert_tokens_example)}\")\n",
        "\tprint(f\"Как выглядят токены исходной фразы: {code_bert_tokens_example}\")\n",
        "\tprint(f\"Индексы токенов: {code_bert_tokens_ids}\")\n",
        "\tprint(f\"Декодированная строка: {code_bert_decoded}\")\n",
        "\n",
        "# tokenization_example(code_dataset['input_string_focal_method'].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.370424Z",
          "iopub.status.busy": "2024-12-12T06:15:58.370199Z",
          "iopub.status.idle": "2024-12-12T06:15:58.389555Z",
          "shell.execute_reply": "2024-12-12T06:15:58.388650Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.370401Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "id": "3LINtQWoVog0",
        "outputId": "43a62fe3-99c1-407a-e7d6-fd36e241d17b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            response  \\\n",
              "0  from microdot import Microdot, Response, abort...   \n",
              "1  from microdot import Microdot, Response, abort...   \n",
              "2  from microdot import Microdot, Response, abort...   \n",
              "3  from microdot import Microdot, Response, abort...   \n",
              "4  from pyner.named_entity.corpus import bio2bioe...   \n",
              "\n",
              "                                        focal_method  \\\n",
              "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
              "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
              "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
              "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
              "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
              "\n",
              "                                           focal_cls  \\\n",
              "0                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
              "1  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
              "2  <CLS_TOKEN> class Microdot: def route(self, ur...   \n",
              "3                           <CLS_TOKEN> <FUNC_TOKEN>   \n",
              "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...   \n",
              "\n",
              "                                    focal_method_ast  \\\n",
              "0  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "1  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "2  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "3  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "\n",
              "                                       focal_cls_ast  \\\n",
              "0                                        <AST_TOKEN>   \n",
              "1  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
              "2  <AST_TOKEN> Module( body=[ ClassDef( name='Mic...   \n",
              "3                                        <AST_TOKEN>   \n",
              "4  <AST_TOKEN> Module( body=[ FunctionDef( name='...   \n",
              "\n",
              "                                   focal_method_info  \\\n",
              "0                                       <INFO_TOKEN>   \n",
              "1  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
              "2  <INFO_TOKEN> <DESCRIPTION_TOKEN> Decorator tha...   \n",
              "3  <INFO_TOKEN> <DESCRIPTION_TOKEN> Mount a sub-a...   \n",
              "4  <INFO_TOKEN> <DESCRIPTION_TOKEN> should be bio...   \n",
              "\n",
              "                                      focal_cls_info  \\\n",
              "0                                       <INFO_TOKEN>   \n",
              "1  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
              "2  <INFO_TOKEN> Module( body=[ ClassDef( name='Mi...   \n",
              "3                                       <INFO_TOKEN>   \n",
              "4  <INFO_TOKEN> Module( body=[ FunctionDef( name=...   \n",
              "\n",
              "                           input_string_focal_method  \\\n",
              "0  <FUNC_TOKEN> def get(self, key, default=None):...   \n",
              "1  <FUNC_TOKEN> def get(self, url_pattern): retur...   \n",
              "2  <FUNC_TOKEN> def post(self, url_pattern): retu...   \n",
              "3  <FUNC_TOKEN> def mount(self, subapp, url_prefi...   \n",
              "4  <FUNC_TOKEN> def iob2bio(tags): processed_tags...   \n",
              "\n",
              "                              input_string_focal_cls  \n",
              "0  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
              "1  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
              "2  <CLS_TOKEN> class Microdot: def route(self, ur...  \n",
              "3  <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN>  \n",
              "4  <CLS_TOKEN> def split_tag(tag: str): if tag in...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-94e52f81-3f01-4b69-b04e-1e94117e1986\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>response</th>\n",
              "      <th>focal_method</th>\n",
              "      <th>focal_cls</th>\n",
              "      <th>focal_method_ast</th>\n",
              "      <th>focal_cls_ast</th>\n",
              "      <th>focal_method_info</th>\n",
              "      <th>focal_cls_info</th>\n",
              "      <th>input_string_focal_method</th>\n",
              "      <th>input_string_focal_cls</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, key, default=None):...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def get(self, url_pattern): retur...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ ClassDef( name='Mic...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Decorator tha...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ ClassDef( name='Mi...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def post(self, url_pattern): retu...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; class Microdot: def route(self, ur...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>from microdot import Microdot, Response, abort...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt;</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt;</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; Mount a sub-a...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt;</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def mount(self, subapp, url_prefi...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; &lt;FUNC_TOKEN&gt; &lt;INFO_TOKEN&gt; &lt;AST_TOKEN&gt;</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>from pyner.named_entity.corpus import bio2bioe...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;AST_TOKEN&gt; Module( body=[ FunctionDef( name='...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; &lt;DESCRIPTION_TOKEN&gt; should be bio...</td>\n",
              "      <td>&lt;INFO_TOKEN&gt; Module( body=[ FunctionDef( name=...</td>\n",
              "      <td>&lt;FUNC_TOKEN&gt; def iob2bio(tags): processed_tags...</td>\n",
              "      <td>&lt;CLS_TOKEN&gt; def split_tag(tag: str): if tag in...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-94e52f81-3f01-4b69-b04e-1e94117e1986')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-94e52f81-3f01-4b69-b04e-1e94117e1986 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-94e52f81-3f01-4b69-b04e-1e94117e1986');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6e5da05f-0493-4268-a51e-ef963b246698\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6e5da05f-0493-4268-a51e-ef963b246698')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6e5da05f-0493-4268-a51e-ef963b246698 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "code_dataset"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "code_dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVjZ2q0xVog1"
      },
      "source": [
        "Далее необхоимо описать класс Dataset для нашей модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.391373Z",
          "iopub.status.busy": "2024-12-12T06:15:58.390891Z",
          "iopub.status.idle": "2024-12-12T06:15:58.400975Z",
          "shell.execute_reply": "2024-12-12T06:15:58.400275Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.391333Z"
        },
        "trusted": true,
        "id": "IcGyKj3-Vog1"
      },
      "outputs": [],
      "source": [
        "class Code2TestDataset(Dataset):\n",
        "\t'''Класс датасет для задачи генерации тестов'''\n",
        "\n",
        "\tdef __init__(self, code_dataset, tokenizer_code_bert, tokenizer_gpt, max_length=512):\n",
        "\t\t'''\n",
        "\t\tКонструктор датасета\n",
        "\n",
        "\t\tПараметры:\n",
        "\t\t- code_dataset: датасет pd.DataFrame\n",
        "\t\t- tokenizer_code_bert: токенизатор code_bert\n",
        "\t\t- tokenizer_gpt: токенизатор gpt\n",
        "\t\t- max_length: максимальная длина последовательности (default: 512)\n",
        "\t\t'''\n",
        "\t\tself.code_dataset = code_dataset\n",
        "\t\tself.tokenizer_code_bert = tokenizer_code_bert\n",
        "\t\tself.tokenizer_gpt = tokenizer_gpt\n",
        "\t\tself.max_length = max_length\n",
        "\n",
        "\tdef __getitem__(self, idx, idx_to_token=False):\n",
        "\t\t'''\n",
        "\t\tGet-метод - возвращает сэмпл по индексу\n",
        "\n",
        "\t\tПараметры:\n",
        "\t\t- idx: индекс\n",
        "\t\t- idx_to_token: флаг для отображения токенов из индексов (default: False)\n",
        "\t\t'''\n",
        "\t\tfocal_method_input = self.code_dataset.at[idx, 'input_string_focal_method']\n",
        "\t\tfocal_cls_input = self.code_dataset.at[idx, 'input_string_focal_cls']\n",
        "\t\tresponse = self.code_dataset.at[idx, 'response']\n",
        "\n",
        "\t\tdef encode_text(text, tokenizer, tokenizer_flag = True):\n",
        "\t\t\tencoding = tokenizer.encode_plus(\n",
        "\t\t\t\ttext,\n",
        "\t\t\t\tadd_special_tokens=True,\n",
        "\t\t\t\tmax_length=self.max_length if tokenizer_flag else self.max_length * 2,\n",
        "\t\t\t\tpadding='max_length',\n",
        "\t\t\t\ttruncation=True,\n",
        "\t\t\t\treturn_attention_mask=True,\n",
        "\t\t\t\treturn_tensors='pt',\n",
        "\t\t\t)\n",
        "\t\t\tinput_ids = encoding['input_ids'].flatten()\n",
        "\t\t\tattention_mask = encoding['attention_mask'].flatten()\n",
        "\t\t\treturn input_ids, attention_mask\n",
        "\n",
        "\t\tinput_ids_focal_method, attention_mask_focal_method = encode_text(focal_method_input, self.tokenizer_code_bert)\n",
        "\t\tinput_ids_focal_cls, attention_mask_focal_cls = encode_text(focal_cls_input, self.tokenizer_code_bert)\n",
        "\t\tinput_ids_response, attention_mask_response = encode_text(response, self.tokenizer_gpt, tokenizer_flag = False)\n",
        "\t\tinput_ids_focal_method_decoder, attention_mask_focal_method_decoder = encode_text(focal_method_input, self.tokenizer_gpt)\n",
        "\t\tinput_ids_focal_cls_decoder, attention_mask_focal_cls_decoder = encode_text(focal_cls_input, self.tokenizer_gpt)\n",
        "\n",
        "\t\tif idx_to_token:\n",
        "\t\t\treturn {\n",
        "\t\t\t\t'input_ids_focal_method': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_method),\n",
        "\t\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
        "\t\t\t\t'input_ids_focal_cls': self.tokenizer_code_bert.convert_ids_to_tokens(input_ids_focal_cls),\n",
        "\t\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
        "\t\t\t\t'ids_response': self.tokenizer_gpt.convert_ids_to_tokens(input_ids_response),\n",
        "\t\t\t\t'attention_mask_response': attention_mask_response\n",
        "\t\t\t}\n",
        "\t\treturn {\n",
        "\t\t\t'input_ids_focal_method': input_ids_focal_method,\n",
        "\t\t\t'attention_mask_focal_method': attention_mask_focal_method,\n",
        "\t\t\t'input_ids_focal_cls': input_ids_focal_cls,\n",
        "\t\t\t'attention_mask_focal_cls': attention_mask_focal_cls,\n",
        "\t\t\t'ids_response': input_ids_response,\n",
        "\t\t\t'attention_mask_response': attention_mask_response,\n",
        "\t\t\t'input_ids_focal_method_decoder': input_ids_focal_method_decoder,\n",
        "\t\t\t'attention_mask_focal_method_decoder': attention_mask_focal_method_decoder,\n",
        "\t\t\t'input_ids_focal_cls_decoder': input_ids_focal_cls_decoder,\n",
        "\t\t\t'attention_mask_focal_cls_decoder': attention_mask_focal_cls_decoder\n",
        "\t\t}\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\t'''Функция возвращает длину датасета. В качестве длины берется размер датасета по axis = 0'''\n",
        "\t\treturn self.code_dataset.shape[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnLXXplbVog1"
      },
      "source": [
        "Тестируем написанный класс"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.404557Z",
          "iopub.status.busy": "2024-12-12T06:15:58.404206Z",
          "iopub.status.idle": "2024-12-12T06:15:58.413360Z",
          "shell.execute_reply": "2024-12-12T06:15:58.412558Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.404498Z"
        },
        "trusted": true,
        "id": "1Y0OaQr4Vog2"
      },
      "outputs": [],
      "source": [
        "code2test_dataset = Code2TestDataset(code_dataset=code_dataset,\n",
        "                                     tokenizer_code_bert=tokenizer_code_bert,\n",
        "                                     tokenizer_gpt=tokenizerGPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.525416Z",
          "iopub.status.busy": "2024-12-12T06:15:58.524794Z",
          "iopub.status.idle": "2024-12-12T06:15:58.533385Z",
          "shell.execute_reply": "2024-12-12T06:15:58.532537Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.525379Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n09zZavXVog2",
        "outputId": "116bc7b9-06df-4c8c-99f0-f9f9ba88bff5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Длина датасета составляет: 280458\n"
          ]
        }
      ],
      "source": [
        "print(f\"Длина датасета составляет: {len(code2test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYarvk5JVog2"
      },
      "source": [
        "Всё работает корректно! Следующим шагом необходимо разбить датасет на train и val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.534956Z",
          "iopub.status.busy": "2024-12-12T06:15:58.534371Z",
          "iopub.status.idle": "2024-12-12T06:15:58.563399Z",
          "shell.execute_reply": "2024-12-12T06:15:58.562563Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.534920Z"
        },
        "trusted": true,
        "id": "CNEwuOEoVog2"
      },
      "outputs": [],
      "source": [
        "def get_datasets(dataset_cls = Code2TestDataset,\n",
        "\t\t\t\tmax_length = 512,\n",
        "\t\t\t\tdata = code_dataset,\n",
        "\t\t\t\ttokenizer_code_bert = tokenizer_code_bert,\n",
        "\t\t\t\ttokenizer_gpt = tokenizerGPT,\n",
        "\t\t\t\ttrain_size = 0.7):\n",
        "\t'''\n",
        "\tФункция get_datasets() возвращает train и val датасеты на основе конструктора AccentDataset, делая train_val_spilt\n",
        "\n",
        "\tПараметры:\n",
        "\t-dataset_cls: класс датасета, конструктор которого будет вызываться (default: Code2TestDataset)\n",
        "\t-max_length: максимальная статья последовательности токенов\n",
        "\t-data: датасает pd.DataFrame (default: code_dataset)\n",
        "\t-tokenizer_code_bert: токенизатор codeBERT (default: tokenizer_code_bert)\n",
        "\t-tokenizer_gpt: токенизатор GPT2 (default: tokenizer_gpt)\n",
        "\t-train_size: размер тренировочной выборки (default: 0.7)\n",
        "\n",
        "\t'''\n",
        "\n",
        "\tdataset = dataset_cls(code_dataset = data,\n",
        "\t\t\t\t\t   \ttokenizer_code_bert = tokenizer_code_bert,\n",
        "\t\t\t\t\t\ttokenizer_gpt=tokenizer_gpt,\n",
        "\t\t\t\t\t\tmax_length=max_length)\n",
        "\n",
        "\ttrain_size = int(train_size * len(dataset))\n",
        "\tval_size = len(dataset) - train_size\n",
        "\ttrain_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "\treturn train_dataset, test_dataset\n",
        "\n",
        "train_dataset, val_dataset = get_datasets(train_size=0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztZgraV0Vog3"
      },
      "source": [
        "Проверяем полученные датасеты"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.564657Z",
          "iopub.status.busy": "2024-12-12T06:15:58.564392Z",
          "iopub.status.idle": "2024-12-12T06:15:58.569309Z",
          "shell.execute_reply": "2024-12-12T06:15:58.568439Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.564633Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LULHQUCnVog3",
        "outputId": "cb026161-05c8-4a3d-92cf-a77789d0f7a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Количество данных в train и val выборках соответственно: (252412, 28046)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Количество данных в train и val выборках соответственно: {len(train_dataset), len(val_dataset)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.570612Z",
          "iopub.status.busy": "2024-12-12T06:15:58.570336Z",
          "iopub.status.idle": "2024-12-12T06:15:58.582210Z",
          "shell.execute_reply": "2024-12-12T06:15:58.581453Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.570588Z"
        },
        "trusted": true,
        "id": "VopeUmP8Vog3"
      },
      "outputs": [],
      "source": [
        "def decode_sequence(tokens_ids, tokenizer):\n",
        "\t'''Декодирование последовательности токенов'''\n",
        "\tcode_bert_decoded = tokenizer.decode(tokens_ids)\n",
        "\tprint(f\"Декодированная строка: {code_bert_decoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYiR3Y8AVog3"
      },
      "source": [
        "Далее получим DataLoader, по которому будем итерироваться"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.647235Z",
          "iopub.status.busy": "2024-12-12T06:15:58.646948Z",
          "iopub.status.idle": "2024-12-12T06:15:58.657089Z",
          "shell.execute_reply": "2024-12-12T06:15:58.656387Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.647174Z"
        },
        "trusted": true,
        "id": "0Gi5MpUUVog4"
      },
      "outputs": [],
      "source": [
        "def get_loaders(train_dataset = train_dataset,\n",
        "\t\t\tval_dataset = val_dataset,\n",
        "\t\t\tshuffle_train = True,\n",
        "\t\t\tshuffle_val = False,\n",
        "\t\t\tbatch_size = 32):\n",
        "\n",
        "\t'''\n",
        "\tФункция get_loaders() для получения train, val даталоадеров\n",
        "\n",
        "\tПараметры:\n",
        "\t-train_dataset: тренировочный датасет (default: train_dataset)\n",
        "\t-val_dataset: валидационный датасет (default: val_dataset)\n",
        "\t-shuffle_train: флаг перемешивания для train (default: True)\n",
        "\t-shuffle_val: флаг перемешивания для val (default: False)\n",
        "\t-batch_size: размер батча данных (default: 32)\n",
        "\t'''\n",
        "\n",
        "\t# train_dataloader\n",
        "\ttrain_dataloader = DataLoader(\n",
        "\t\t\ttrain_dataset,\n",
        "\t\t\tbatch_size = batch_size,\n",
        "\t\t\tshuffle = shuffle_train,\n",
        "\t\t)\n",
        "\n",
        "\t# validation_dataloader\n",
        "\tvalidation_dataloader = DataLoader(\n",
        "\t\t\tval_dataset,\n",
        "\t\t\tbatch_size = batch_size,\n",
        "\t\t\tshuffle = shuffle_val,\n",
        "\t\t)\n",
        "\n",
        "\t# Возвращаем даталоадеры\n",
        "\treturn train_dataloader, validation_dataloader\n",
        "\n",
        "train_dataloader, validation_dataloader = get_loaders(batch_size=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PV1IPmb8Vog4"
      },
      "source": [
        "Проверка итерирования"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.724781Z",
          "iopub.status.busy": "2024-12-12T06:15:58.724413Z",
          "iopub.status.idle": "2024-12-12T06:15:58.777765Z",
          "shell.execute_reply": "2024-12-12T06:15:58.776878Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.724751Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ysaV_an7Vog4",
        "outputId": "3583c93b-6615-448b-e8e3-d38cdf6e5826"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/63103 [00:00<?, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "for i, batch in enumerate(tqdm(train_dataloader)):\n",
        "    if i == 0:\n",
        "        break\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUlMgUFKVog5"
      },
      "source": [
        "Корректно отрабатывает!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEjTqd5-Vog5"
      },
      "source": [
        "Далее, собираем архитектуру и готовимся обучать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.779397Z",
          "iopub.status.busy": "2024-12-12T06:15:58.779096Z",
          "iopub.status.idle": "2024-12-12T06:15:58.782957Z",
          "shell.execute_reply": "2024-12-12T06:15:58.782039Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.779372Z"
        },
        "trusted": true,
        "id": "YI_0YNygVog5"
      },
      "outputs": [],
      "source": [
        "# model_code_bert = AutoModel.from_pretrained(\"microsoft/codebert-base\", output_hidden_states= True).to(device)\n",
        "# model_code_bert.resize_token_embeddings(len(tokenizer_code_bert))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqIcVC-pVog6"
      },
      "source": [
        "Как работает модель codeBERT:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.784258Z",
          "iopub.status.busy": "2024-12-12T06:15:58.783945Z",
          "iopub.status.idle": "2024-12-12T06:15:58.796543Z",
          "shell.execute_reply": "2024-12-12T06:15:58.795837Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.784223Z"
        },
        "trusted": true,
        "id": "wADKQZZrVog6"
      },
      "outputs": [],
      "source": [
        "# for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "# \t# Проверка корректности работы\n",
        "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
        "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
        "\n",
        "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
        "# \tprint(last_hidden_state_code_bert.size())\n",
        "# \tbreak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGD1xAyrVog6"
      },
      "source": [
        "Таким образом, для каждого токена мы получим свое закодированное значение размерности 768"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys-7r4xlVog6"
      },
      "source": [
        "Модель GPT2:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.802733Z",
          "iopub.status.busy": "2024-12-12T06:15:58.802139Z",
          "iopub.status.idle": "2024-12-12T06:15:58.806865Z",
          "shell.execute_reply": "2024-12-12T06:15:58.806127Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.802702Z"
        },
        "trusted": true,
        "id": "NOe0aQa2Vog6"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoConfig\n",
        "\n",
        "modelGPT2Path = \"gpt2\"\n",
        "# config = AutoConfig.from_pretrained(modelGPT2Path, is_decoder=True, add_cross_attention= True)\n",
        "# config.add_cross_attention = True  # Включение cross-attention\n",
        "\n",
        "# modelGPT2 = AutoModel.from_pretrained(modelGPT2Path, config=config).to(device)\n",
        "# modelGPT2.resize_token_embeddings(len(tokenizerGPT))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOAH9fG3Vog6"
      },
      "source": [
        "Как работает модель GPTBigCode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.808322Z",
          "iopub.status.busy": "2024-12-12T06:15:58.807994Z",
          "iopub.status.idle": "2024-12-12T06:15:58.819396Z",
          "shell.execute_reply": "2024-12-12T06:15:58.818477Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.808280Z"
        },
        "trusted": true,
        "id": "-hDRPlpQVog7"
      },
      "outputs": [],
      "source": [
        "# for i, batch in enumerate(train_dataloader):\n",
        "\n",
        "# \tb_input_ids = batch['input_ids_focal_method'].to(device)\n",
        "# \tb_input_mask = batch['attention_mask_focal_method'].to(device)\n",
        "\n",
        "# \toutputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "# \tlast_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
        "\n",
        "# \tprint(last_hidden_state_code_bert.size())\n",
        "\n",
        "# \t# Проверка корректности работы\n",
        "# \tresponse_input_ids = batch['ids_response'].to(device)\n",
        "# \tresponse_input_mask = batch['attention_mask_response'].to(device)\n",
        "# \tgpt_output = modelGPT2(input_ids=response_input_ids,\n",
        "# \t\t\t\t\t\t\t  attention_mask=response_input_mask,\n",
        "# \t\t\t\t\t\t\t  encoder_hidden_states = last_hidden_state_code_bert)\n",
        "# \tprint(gpt_output['last_hidden_state'].size())\n",
        "\n",
        "\n",
        "# \t# outputs_code_bert = model_code_bert(b_input_ids, attention_mask=b_input_mask)\n",
        "# \t# last_hidden_state_code_bert = outputs_code_bert['last_hidden_state']\n",
        "# \t# print(last_hidden_state_code_bert.size())\n",
        "# \tbreak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4sDVYyLVog7"
      },
      "source": [
        "Ну, как-то худо-бедно всё это дело запускается. Пробуем строить модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:15:58.820718Z",
          "iopub.status.busy": "2024-12-12T06:15:58.820424Z",
          "iopub.status.idle": "2024-12-12T06:16:00.552702Z",
          "shell.execute_reply": "2024-12-12T06:16:00.552002Z",
          "shell.execute_reply.started": "2024-12-12T06:15:58.820688Z"
        },
        "trusted": true,
        "id": "-SuTXoOcVog7"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "class LargeCodeModel(nn.Module):\n",
        "\t'''Класс для сложной языковой модели, которая обрабатывает входной код'''\n",
        "\tdef __init__(self, bert_model_name, gpt2_name):\n",
        "\t\tsuper(LargeCodeModel, self).__init__()\n",
        "\n",
        "\t\tself.bert1 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
        "\t\tself.bert2 = AutoModel.from_pretrained(bert_model_name, output_hidden_states= True)\n",
        "\t\tself.tokenizer_code_bert = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "\n",
        "\t\tself.new_special_tokens = ['<FUNC_TOKEN>',\n",
        "            '<INFO_TOKEN>',\n",
        "            '<CLS_TOKEN>',\n",
        "            '<AST_TOKEN>',\n",
        "            '<DESCRIPTION_TOKEN>',\n",
        "            '<COMMENTS_TOKEN>']\n",
        "\n",
        "\t\tself.special_tokens_dict = {\n",
        "\t\t\t'additional_special_tokens': new_special_tokens\n",
        "\t\t}\n",
        "\n",
        "\t\tself.tokenizer_code_bert.add_special_tokens(self.special_tokens_dict)\n",
        "\t\tself.bert1.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
        "\t\tself.bert2.resize_token_embeddings(len(self.tokenizer_code_bert))\n",
        "\n",
        "\t\tself.gpt2_config = AutoConfig.from_pretrained(gpt2_name, is_decoder=True, add_cross_attention = True)\n",
        "\t\tself.gpt2_config.add_cross_attention = True  # Включение cross-attention\n",
        "\t\tself.tokenizerGPT = AutoTokenizer.from_pretrained(gpt2_name)\n",
        "\t\tself.tokenizerGPT.add_special_tokens({'pad_token': '<PAD>'})\n",
        "\t\tself.gpt2 = GPT2LMHeadModel.from_pretrained(gpt2_name, config=self.gpt2_config)\n",
        "\t\tself.gpt2.resize_token_embeddings(len(self.tokenizerGPT))\n",
        "\n",
        "\t\tself.layer_norm = nn.LayerNorm(self.bert1.config.hidden_size)\n",
        "\n",
        "\t\tself.projection = nn.Linear(\n",
        "            self.bert1.config.hidden_size + self.bert2.config.hidden_size,\n",
        "            self.gpt2.config.hidden_size\n",
        "        )\n",
        "\n",
        "\t# forward call\n",
        "\tdef forward(self, focal_method_input_ids,\n",
        "\t\t\t \t\t\tfocal_method_attention_masks,\n",
        "\t\t\t\t\t\tfocal_cls_input_ids,\n",
        "\t\t\t\t\t\tfocal_cls_attention_masks,\n",
        "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
        "\t\t\t\t\t\tdecoder_input_ids_focal_method,\n",
        "\t\t\t\t\t\tdecoder_input_ids_focal_cls,\n",
        "\t\t\t\t\t\tattention_mask_focal_method_decoder,\n",
        "\t\t\t\t\t\tattention_mask_focal_cls_decoder):\n",
        "\n",
        "\t\t# print(focal_method_input_ids.size())\n",
        "\t\t# print(focal_method_attention_masks.size())\n",
        "\t\t# print(type(focal_method_input_ids))\n",
        "\t\t# print(type(focal_method_attention_masks))\n",
        "\t\t# print(type(focal_cls_input_ids))\n",
        "\t\t# print(type(focal_cls_attention_masks))\n",
        "\t\t# print(type(response_ids))\n",
        "\t\t# print(type(response_attention_masks))\n",
        "\n",
        "\t\tdecoder_input_ids = torch.cat([decoder_input_ids_focal_method,\n",
        "\t\t                                  decoder_input_ids_focal_cls], dim=1)\n",
        "\n",
        "\t\tdecoder_attention_masks = torch.cat([attention_mask_focal_method_decoder,\n",
        "\t\t                                  attention_mask_focal_cls_decoder], dim=1)\n",
        "\n",
        "\t\t# print(decoder_input_ids.size())\n",
        "\n",
        "\t\tbert1_outputs = self.bert1(focal_method_input_ids, focal_method_attention_masks)\n",
        "\t\tlast_hidden_state_bert1 = bert1_outputs['last_hidden_state']\n",
        "\n",
        "\t\tbert2_outputs = self.bert2(focal_cls_input_ids, focal_cls_attention_masks)\n",
        "\t\tlast_hidden_state_bert2 = bert2_outputs['last_hidden_state']\n",
        "\n",
        "\t\t# print(last_hidden_state_bert1.size())\n",
        "\t\t# print(last_hidden_state_bert2.size())\n",
        "\n",
        "\t\tconcat_hidden_states = torch.cat([last_hidden_state_bert1,\n",
        "\t\t                                  last_hidden_state_bert2], dim=1)\n",
        "\n",
        "\t\t# print(concat_hidden_states.size())\n",
        "\n",
        "\t\t# LayerNormalization\n",
        "\t\tnormalized_hidden_states = self.layer_norm(concat_hidden_states)\n",
        "\n",
        "\t\t# Для BatchNorm\n",
        "\t\t# batch_norm_input = concat_hidden_states.view(-1, 768)\n",
        "\t\t# normalized_hidden_states = self.batch_norm(batch_norm_input)\n",
        "\t\t# normalized_hidden_states = normalized_hidden_states.view(2, 1024, 768)\n",
        "\t\t# print(normalized_hidden_states.size())\n",
        "\t\t# print(torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1).size())\n",
        "\t\t# print(response_ids.size())\n",
        "\t\t# print(response_input_mask.size())\n",
        "\n",
        "\t\t# print(response_attention_masks.size())\n",
        "\n",
        "\t\t# print('No problems')\n",
        "\n",
        "\t\tgpt2_outputs = self.gpt2(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_masks,\n",
        "            encoder_hidden_states=normalized_hidden_states,\n",
        "            encoder_attention_mask=torch.cat([focal_method_attention_masks, focal_cls_attention_masks], dim=1),\n",
        "\t\t\t\t\t\tlabels=response_ids\n",
        "        )\n",
        "\n",
        "\t\treturn gpt2_outputs\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJ4GpL3eVog7"
      },
      "source": [
        "Отлаживаем модель"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:16:00.554391Z",
          "iopub.status.busy": "2024-12-12T06:16:00.553899Z",
          "iopub.status.idle": "2024-12-12T06:16:11.986047Z",
          "shell.execute_reply": "2024-12-12T06:16:11.985054Z",
          "shell.execute_reply.started": "2024-12-12T06:16:00.554357Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1v-RkeDVog7",
        "outputId": "14960716-2f0e-48a9-c88a-2cf8f4a9e273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.crossattention.c_attn.bias', 'h.0.crossattention.c_attn.weight', 'h.0.crossattention.c_proj.bias', 'h.0.crossattention.c_proj.weight', 'h.0.crossattention.q_attn.bias', 'h.0.crossattention.q_attn.weight', 'h.0.ln_cross_attn.bias', 'h.0.ln_cross_attn.weight', 'h.1.crossattention.c_attn.bias', 'h.1.crossattention.c_attn.weight', 'h.1.crossattention.c_proj.bias', 'h.1.crossattention.c_proj.weight', 'h.1.crossattention.q_attn.bias', 'h.1.crossattention.q_attn.weight', 'h.1.ln_cross_attn.bias', 'h.1.ln_cross_attn.weight', 'h.10.crossattention.c_attn.bias', 'h.10.crossattention.c_attn.weight', 'h.10.crossattention.c_proj.bias', 'h.10.crossattention.c_proj.weight', 'h.10.crossattention.q_attn.bias', 'h.10.crossattention.q_attn.weight', 'h.10.ln_cross_attn.bias', 'h.10.ln_cross_attn.weight', 'h.11.crossattention.c_attn.bias', 'h.11.crossattention.c_attn.weight', 'h.11.crossattention.c_proj.bias', 'h.11.crossattention.c_proj.weight', 'h.11.crossattention.q_attn.bias', 'h.11.crossattention.q_attn.weight', 'h.11.ln_cross_attn.bias', 'h.11.ln_cross_attn.weight', 'h.2.crossattention.c_attn.bias', 'h.2.crossattention.c_attn.weight', 'h.2.crossattention.c_proj.bias', 'h.2.crossattention.c_proj.weight', 'h.2.crossattention.q_attn.bias', 'h.2.crossattention.q_attn.weight', 'h.2.ln_cross_attn.bias', 'h.2.ln_cross_attn.weight', 'h.3.crossattention.c_attn.bias', 'h.3.crossattention.c_attn.weight', 'h.3.crossattention.c_proj.bias', 'h.3.crossattention.c_proj.weight', 'h.3.crossattention.q_attn.bias', 'h.3.crossattention.q_attn.weight', 'h.3.ln_cross_attn.bias', 'h.3.ln_cross_attn.weight', 'h.4.crossattention.c_attn.bias', 'h.4.crossattention.c_attn.weight', 'h.4.crossattention.c_proj.bias', 'h.4.crossattention.c_proj.weight', 'h.4.crossattention.q_attn.bias', 'h.4.crossattention.q_attn.weight', 'h.4.ln_cross_attn.bias', 'h.4.ln_cross_attn.weight', 'h.5.crossattention.c_attn.bias', 'h.5.crossattention.c_attn.weight', 'h.5.crossattention.c_proj.bias', 'h.5.crossattention.c_proj.weight', 'h.5.crossattention.q_attn.bias', 'h.5.crossattention.q_attn.weight', 'h.5.ln_cross_attn.bias', 'h.5.ln_cross_attn.weight', 'h.6.crossattention.c_attn.bias', 'h.6.crossattention.c_attn.weight', 'h.6.crossattention.c_proj.bias', 'h.6.crossattention.c_proj.weight', 'h.6.crossattention.q_attn.bias', 'h.6.crossattention.q_attn.weight', 'h.6.ln_cross_attn.bias', 'h.6.ln_cross_attn.weight', 'h.7.crossattention.c_attn.bias', 'h.7.crossattention.c_attn.weight', 'h.7.crossattention.c_proj.bias', 'h.7.crossattention.c_proj.weight', 'h.7.crossattention.q_attn.bias', 'h.7.crossattention.q_attn.weight', 'h.7.ln_cross_attn.bias', 'h.7.ln_cross_attn.weight', 'h.8.crossattention.c_attn.bias', 'h.8.crossattention.c_attn.weight', 'h.8.crossattention.c_proj.bias', 'h.8.crossattention.c_proj.weight', 'h.8.crossattention.q_attn.bias', 'h.8.crossattention.q_attn.weight', 'h.8.ln_cross_attn.bias', 'h.8.ln_cross_attn.weight', 'h.9.crossattention.c_attn.bias', 'h.9.crossattention.c_attn.weight', 'h.9.crossattention.c_proj.bias', 'h.9.crossattention.c_proj.weight', 'h.9.crossattention.q_attn.bias', 'h.9.crossattention.q_attn.weight', 'h.9.ln_cross_attn.bias', 'h.9.ln_cross_attn.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "CodeModel = LargeCodeModel(bert_model_name=\"microsoft/codebert-base\",\n",
        "                           gpt2_name=\"gpt2\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SF48wREWVog7"
      },
      "source": [
        "Далее необходимо объявить функцию train-val loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSZq2n4eVog7"
      },
      "source": [
        "Для начала необходимо объявить дополнительные функции для отображения времени и подсчёта метрик качества"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:17:35.226198Z",
          "iopub.status.busy": "2024-12-12T06:17:35.225592Z",
          "iopub.status.idle": "2024-12-12T06:17:35.233921Z",
          "shell.execute_reply": "2024-12-12T06:17:35.233076Z",
          "shell.execute_reply.started": "2024-12-12T06:17:35.226167Z"
        },
        "trusted": true,
        "id": "StkmqyMxVog7"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "\t'''Функция форматирования времени'''\n",
        "\treturn str(datetime.timedelta(seconds=int(round((elapsed)))))\n",
        "\n",
        "def token_accuracy_calc(logits, labels, attention_mask):\n",
        "\t'''Функция подсчета accuracy для данных'''\n",
        "\tpred_flat = np.argmax(logits, axis=-1).flatten()\n",
        "\tlabels_flat = labels.flatten()\n",
        "\tmask_flat = attention_mask.flatten()\n",
        "\taccuracy = np.sum(pred_flat[mask_flat] == labels_flat[mask_flat]) / len(labels_flat[mask_flat])\n",
        "\treturn accuracy\n",
        "\n",
        "def token_bleu_calc(logits, labels, attention_mask):\n",
        "\t'''Функция подсчета BLEU для данных'''\n",
        "\n",
        "\tpred_flat = np.argmax(logits, axis=-1)\n",
        "\tlabels_flat = labels\n",
        "\tmask_flat = attention_mask\n",
        "\n",
        "\tpred_tokens = []\n",
        "\ttrue_tokens = []\n",
        "\n",
        "\tfor i in range(pred_flat.shape[0]):\n",
        "\t\tpred_seq = pred_flat[i][mask_flat[i] == 1]\n",
        "\t\ttrue_seq = labels_flat[i][mask_flat[i] == 1]\n",
        "\t\tpred_tokens.append(pred_seq)\n",
        "\t\ttrue_tokens.append(true_seq)\n",
        "\n",
        "\tpred_strings = tokenizerGPT.batch_decode(pred_tokens, skip_special_tokens=True)\n",
        "\ttrue_strings = tokenizerGPT.batch_decode(true_tokens, skip_special_tokens=True)\n",
        "\n",
        "\t# Вычисление BLEU\n",
        "\tbleu_scores = []\n",
        "\tsmoothing_function = SmoothingFunction().method1\n",
        "\n",
        "\tfor pred, true in zip(pred_strings, true_strings):\n",
        "\t\tpred_tokens = pred.split()\n",
        "\t\ttrue_tokens = [true.split()]\n",
        "\t\tbleu_score = sentence_bleu(true_tokens, pred_tokens, smoothing_function=smoothing_function)\n",
        "\t\tbleu_scores.append(bleu_score)\n",
        "\n",
        "\taverage_bleu = np.mean(bleu_scores)\n",
        "\treturn average_bleu\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yk8znRoVog8"
      },
      "source": [
        "Как минимимум оно запускается"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0S_zQsOmVog8"
      },
      "source": [
        "Далее реализуем саму функцию train-val-loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi040W98Vog8"
      },
      "source": [
        "Перед этим объями дополнительные настройки обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:18:59.067221Z",
          "iopub.status.busy": "2024-12-12T06:18:59.066535Z",
          "iopub.status.idle": "2024-12-12T06:18:59.076961Z",
          "shell.execute_reply": "2024-12-12T06:18:59.076008Z",
          "shell.execute_reply.started": "2024-12-12T06:18:59.067191Z"
        },
        "trusted": true,
        "id": "AkfdOP5PVog8"
      },
      "outputs": [],
      "source": [
        "from transformers import AdamW, get_scheduler\n",
        "\n",
        "optimizer = AdamW(CodeModel.parameters(), lr=3e-5)\n",
        "num_epochs = 1\n",
        "train_steps = len(train_dataloader) * num_epochs\n",
        "lr_scheduler = get_scheduler(\n",
        "    name='linear',\n",
        "    optimizer=optimizer,\n",
        "    num_warmup_steps=0,\n",
        "    num_training_steps=train_steps\n",
        ")\n",
        "tensorboard_log_dir = 'runs/CodeModelLogs/'\n",
        "tensorboard_path_accuracy = 'runs/model_accuracy_score_{:.2f}.pth'\n",
        "tensorboard_path_loss = 'runs/model_val_loss_{:.2f}.pth'\n",
        "tensorboard_path_bleu = 'runs/model_bleu_score_{:.2f}.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMkcIBXEVog8"
      },
      "source": [
        "И, наконец, функция:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-12T06:19:00.573329Z",
          "iopub.status.busy": "2024-12-12T06:19:00.573023Z",
          "iopub.status.idle": "2024-12-12T06:19:00.594827Z",
          "shell.execute_reply": "2024-12-12T06:19:00.593904Z",
          "shell.execute_reply.started": "2024-12-12T06:19:00.573302Z"
        },
        "trusted": true,
        "id": "fsb-uwMuVog8"
      },
      "outputs": [],
      "source": [
        "def train_val_loop_codeLM(model = CodeModel,\n",
        "\t\t\t\t\t\ttrain_loader = train_dataloader,\n",
        "\t\t\t\t\t\tval_loader = validation_dataloader,\n",
        "\t\t\t\t\t\toptimizer = optimizer,\n",
        "\t\t\t\t\t\tscheduler = lr_scheduler,\n",
        "\t\t\t\t\t\tnum_epochs = num_epochs,\n",
        "\t\t\t\t\t\tdevice = 'cuda',\n",
        "\t\t\t\t\t\tmodel_save_path_accuracy = tensorboard_path_accuracy,\n",
        "\t\t\t\t\t\tmodel_save_path_loss = tensorboard_path_loss,\n",
        "\t\t\t\t\t\tmodel_save_path_bleu = tensorboard_path_bleu,\n",
        "\t\t\t\t\t\ttensorboard_log_dir = tensorboard_log_dir,\n",
        "\t\t\t\t\t\tgradient_accumulation_steps = 2,\n",
        "\t\t\t\t\t\teval_every = 1,\n",
        "\t\t\t\t\t\ttest_step_only = False):\n",
        "\t'''\n",
        "\tФункция для реализации train-val loop обучения нашей модели\n",
        "\n",
        "\tПараметры:\n",
        "\t-model: модель нейронной сети\n",
        "\t-train_loader: тренировочный датасет\n",
        "\t-val_loader: валидационный датасет\n",
        "\t-optimizer: оптимизатор\n",
        "\t-scheduler: изменение для learning_rate (расписание)\n",
        "\t-num_epochs: число эпох для обучения\n",
        "\t-device: устройство\n",
        "\t-model_save_path_accuracy: путь для сохранения весов модели (с лучшей accuracy)\n",
        "\t-model_save_path_loss: путь для сохранения весов модели (с лучим val_loss)\n",
        "\t-model_save_path_bleu: путь для сохранения весов модели (с лучим val_bleu_score)\n",
        "\t-tensorboard_log_dir: путь для записи логов в TensorBoard,\n",
        "\t-gradient_accumulation_steps: число шагов для накопления градиентов\n",
        "\t-eval_every: число шагов, через которые делаем валидацию\n",
        "\t-test_step_only: вспомогательная логика для тестирования обучения небольшого числа шагов (default: False)\n",
        "\t'''\n",
        "\n",
        "\twriter = SummaryWriter(log_dir=tensorboard_log_dir)\n",
        "\thistory = {\n",
        "\t\t'train_loss': [],\n",
        "\t\t'train_bleu': [],\n",
        "\t\t'train_accuracy': [],\n",
        "\t\t'val_accuracy': [],\n",
        "\t\t'val_loss': [],\n",
        "\t\t'val_bleu': []\n",
        "\t}\n",
        "\tbest_val_loss = float('inf')\n",
        "\tbest_val_bleu = 0.0\n",
        "\tbest_val_accuracy = 0.0\n",
        "\tmodel.to(device)\n",
        "\n",
        "\tfor epoch in range(num_epochs):\n",
        "\t\tprint(\"\")\n",
        "\t\tprint('======== Epoch {:} / {:} ========'.format(epoch + 1, num_epochs))\n",
        "\t\tprint('Training...')\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\t\tmodel.train()\n",
        "\t\ttotal_train_loss = 0\n",
        "\t\ttotal_train_bleu = 0\n",
        "\t\ttotal_train_accuracy = 0\n",
        "\t\tnum_train_steps = 0\n",
        "\n",
        "\t\tfor step, batch in enumerate(tqdm(train_loader)):\n",
        "\n",
        "\t\t\tif test_step_only and step >= 800:  # Прерываем после первого батча\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\n",
        "\t\t\tif step % 1500 == 0 and not step == 0:\n",
        "\t\t\t\t# Calculate elapsed time in minutes.\n",
        "\t\t\t\telapsed = format_time(time.time() - t0)\n",
        "\t\t\t\t# Report progress.\n",
        "\t\t\t\tprint('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\n",
        "\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
        "\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
        "\n",
        "\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
        "\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
        "\n",
        "\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
        "\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
        "\n",
        "\t\t\tinput_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
        "\t\t\tinput_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
        "\n",
        "\t\t\tattention_mask_focal_method_decoder = batch['attention_mask_focal_method_decoder'].to(device)\n",
        "\t\t\tattention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
        "\n",
        "\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
        "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
        "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
        "\t\t\t\t\t\tinput_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
        "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
        "\n",
        "\t\t\tloss = output_codeLM.loss\n",
        "\t\t\tlogits = output_codeLM.logits\n",
        "\n",
        "\t\t\tlogits = logits.detach().cpu().numpy()\n",
        "\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
        "\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
        "\n",
        "\t\t\taccuracy_train = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
        "\t\t\tbleu_train = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
        "\n",
        "\t\t\ttotal_train_accuracy += accuracy_train\n",
        "\t\t\ttotal_train_bleu += bleu_train\n",
        "\n",
        "\t\t\ttotal_train_loss += loss.item()\n",
        "\t\t\tnum_train_steps += 1\n",
        "\n",
        "\t\t\tloss.backward()\n",
        "\n",
        "\t\t\tif (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_dataloader):\n",
        "\t\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) # Клипаем накопленные градиенты\n",
        "\t\t\t\toptimizer.step()\n",
        "\t\t\t\tscheduler.step()\n",
        "\n",
        "\t\tavg_train_loss = total_train_loss / num_train_steps\n",
        "\t\tavg_train_accuracy = total_train_accuracy / num_train_steps\n",
        "\t\tavg_train_bleu_score = total_train_bleu / num_train_steps\n",
        "\n",
        "\t\ttraining_time = format_time(time.time() - t0)\n",
        "\n",
        "\t\tprint(\"\")\n",
        "\t\tprint(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "\t\t# print(\"  Average training accuracy: {0:.2f}\".format(avg_train_accuracy))\n",
        "\t\t# print(\"  Average training BLEU score: {0:.2f}\".format(avg_train_bleu_score))\n",
        "\t\tprint(\"  Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\t\thistory['train_loss'].append(avg_train_loss)\n",
        "\t\thistory['train_accuracy'].append(avg_train_accuracy)\n",
        "\t\thistory['train_bleu'].append(avg_train_bleu_score)\n",
        "\n",
        "\t\t# Логирование в TensorBoard для обучения\n",
        "\t\twriter.add_scalar(\"Train/Loss\", avg_train_loss, epoch + 1)\n",
        "\t\twriter.add_scalar(\"Train/Accuracy\", avg_train_accuracy, epoch + 1)\n",
        "\t\twriter.add_scalar(\"Train/BLEU_score\", avg_train_bleu_score, epoch + 1)\n",
        "\n",
        "\t\tprint(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {avg_train_accuracy:.4f}, Train BLEU score: {avg_train_bleu_score:.4f}\")\n",
        "\n",
        "\t\tprint(\"\")\n",
        "\t\tprint(\"Running Validation...\")\n",
        "\n",
        "\n",
        "\t\tt0 = time.time()\n",
        "\n",
        "\t\t# Put the model in evaluation mode--the dropout layers behave differently\n",
        "\t\t# during evaluation.\n",
        "\t\tmodel.eval()\n",
        "\n",
        "\t\tif (epoch + 1) % eval_every == 0:\n",
        "\t\t\tmodel.eval()\n",
        "\t\t\ttotal_eval_loss = 0\n",
        "\t\t\ttotal_eval_accuracy = 0\n",
        "\t\t\ttotal_eval_bleu = 0\n",
        "\t\t\tnum_eval_steps = 0\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\tfor batch in tqdm(val_loader):\n",
        "\t\t\t\tif test_step_only and num_eval_steps >= 200:  # Прерываем если хотим проконтроллировать\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\t\tfocal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
        "\t\t\t\tfocal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
        "\n",
        "\t\t\t\tfocal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
        "\t\t\t\tfocal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
        "\n",
        "\t\t\t\tresponse_ids = batch['ids_response'].to(device)\n",
        "\t\t\t\tresponse_attention_masks = batch['attention_mask_response'].to(device)\n",
        "\n",
        "\t\t\t\tinput_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
        "\t\t\t\tinput_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
        "\n",
        "\t\t\t\tattention_mask_focal_method_decoder = batch['attention_mask_focal_method_decoder'].to(device)\n",
        "\t\t\t\tattention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
        "\n",
        "\t\t\t\toutput_codeLM = model(focal_method_input_ids, focal_method_attention_masks,\n",
        "\t\t\t\t\t\tfocal_cls_input_ids, focal_cls_attention_masks,\n",
        "\t\t\t\t\t\tresponse_ids, response_attention_masks,\n",
        "\t\t\t\t\t\tinput_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
        "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
        "\n",
        "\t\t\t\tloss = output_codeLM.loss\n",
        "\t\t\t\tlogits = output_codeLM.logits\n",
        "\n",
        "\t\t\t\tlogits = logits.detach().cpu().numpy()\n",
        "\t\t\t\tresponse_ids = response_ids.cpu().numpy()\n",
        "\t\t\t\tresponse_attention_masks = response_attention_masks.cpu().numpy()\n",
        "\n",
        "\t\t\t\taccuracy_val = token_accuracy_calc(logits, response_ids, response_attention_masks)\n",
        "\t\t\t\tbleu_val = token_bleu_calc(logits, response_ids, response_attention_masks)\n",
        "\n",
        "\t\t\t\ttotal_eval_accuracy += accuracy_val\n",
        "\t\t\t\ttotal_eval_bleu += bleu_val\n",
        "\n",
        "\t\t\t\ttotal_eval_loss += loss.item()\n",
        "\t\t\t\tnum_eval_steps += 1\n",
        "\n",
        "\t\tavg_val_loss = total_eval_loss / num_eval_steps\n",
        "\t\tavg_val_accuracy = total_eval_accuracy / num_eval_steps\n",
        "\t\tavg_val_bleu_score = total_eval_bleu / num_eval_steps\n",
        "\n",
        "\t\thistory['val_loss'].append(avg_val_loss)\n",
        "\t\thistory['val_accuracy'].append(avg_val_accuracy)\n",
        "\t\thistory['val_bleu'].append(avg_val_bleu_score)\n",
        "\n",
        "\t\t# Логирование в TensorBoard для валидации\n",
        "\t\twriter.add_scalar(\"Validation/Loss\", avg_val_loss, epoch + 1)\n",
        "\t\twriter.add_scalar(\"Validation/Accuracy\", avg_val_accuracy, epoch + 1)\n",
        "\n",
        "\t\tprint(f\"Validation Loss: {avg_val_loss:.4f}, Validation Accuracy: {avg_val_accuracy:.4f},  Validation BLEU score: {avg_val_bleu_score:.4f}\")\n",
        "\n",
        "\t\t# Ну вот тут надо настроить, чтобы\n",
        "\t\t# Если точность выше, то сохраняем веса\n",
        "\t\t# if avg_val_accuracy > best_val_accuracy:\n",
        "\t\t# \tbest_val_accuracy = avg_val_accuracy\n",
        "\t\t# \ttorch.save(model.state_dict(), model_save_path_accuracy.format(best_val_accuracy))\n",
        "\t\t# \tprint(f\"Model saved to {model_save_path_accuracy.format(best_val_accuracy)}\")\n",
        "\n",
        "\t\t# # Если лосс ниже, то сохраняем веса\n",
        "\t\t# if avg_val_loss < best_val_loss:\n",
        "\t\t# \tbest_val_loss = avg_val_loss\n",
        "\t\t# \ttorch.save(model.state_dict(), model_save_path_loss.format(best_val_loss))\n",
        "\t\t# \tprint(f\"Model saved to {model_save_path_loss.format(best_val_loss)}\")\n",
        "\n",
        "\t\t# # Если BLEU выше, то сохраняем веса\n",
        "\t\t# if avg_val_bleu_score > best_val_bleu:\n",
        "\t\t# \tbest_val_bleu = avg_val_bleu_score\n",
        "\t\t# \ttorch.save(model.state_dict(), model_save_path_bleu.format(best_val_bleu))\n",
        "\t\t# \tprint(f\"Model saved to {model_save_path_bleu.format(best_val_bleu)}\")\n",
        "\n",
        "\twriter.close()\n",
        "\treturn history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r6lh1igVog9"
      },
      "source": [
        "Наконец, пробуем запустить обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rMHiyXdsVog9",
        "outputId": "bfd0c4bc-c63b-4335-b3e9-f6fa6426008c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 1 ========\n",
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  1%|▏         | 800/63103 [17:08<22:15:15,  1.29s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "  Average training loss: 4.88\n",
            "  Training epoch took: 0:17:09\n",
            "Train Loss: 4.8787, Train Accuracy: 0.0110, Train BLEU score: 0.0000\n",
            "\n",
            "Running Validation...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  3%|▎         | 200/7012 [03:15<1:50:57,  1.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Loss: 2.7915, Validation Accuracy: 0.0113,  Validation BLEU score: 0.0009\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_results = train_val_loop_codeLM(device='cuda', test_step_only = True, num_epochs = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hiOV6rtIVog9",
        "outputId": "2bd630b2-a216-4bf1-88fb-e470eadb3380"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/7012 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Декодированная строка: <s><FUNC_TOKEN> def align_sequences(read_seq, read_qual, ref_seq, alignment, gap_char='-'): read, qual, ref = [], [], [] read_pos, ref_pos = 0, 0 errors_per_read_pos = [0] * len(read_seq) for c in alignment.cigar_parts: cigar_type = c[-1] cigar_size = int(c[:-1]) if cigar_type == 'M': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) for i in range(cigar_size): if read_seq[read_pos+i] != ref_seq[ref_pos+i]: errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size ref_pos += cigar_size if cigar_type == 'I': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(gap_char * cigar_size) for i in range(cigar_size): errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size if cigar_type == 'D': read.append(gap_char * cigar_size) qual.append(gap_char * cigar_size) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) errors_per_read_pos[read_pos] += cigar_size ref_pos += cigar_size return ''.join(read), ''.join(qual), ''.join(ref), errors_per_read_pos <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='align_sequences', args=arguments( posonlyargs=[], args=[ arg(arg='read_seq'), arg(arg='read_qual'), arg(arg='ref_seq'), arg(arg='alignment'), arg(arg='gap_char')], kwonlyargs=[], kw_defaults=[],</s>\n",
            "Декодированная строка: <s><CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN></s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Декодированная строка: <FUNC_TOKEN> def align_sequences(read_seq, read_qual, ref_seq, alignment, gap_char='-'): read, qual, ref = [], [], [] read_pos, ref_pos = 0, 0 errors_per_read_pos = [0] * len(read_seq) for c in alignment.cigar_parts: cigar_type = c[-1] cigar_size = int(c[:-1]) if cigar_type == 'M': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) for i in range(cigar_size): if read_seq[read_pos+i] != ref_seq[ref_pos+i]: errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size ref_pos += cigar_size if cigar_type == 'I': read.append(read_seq[read_pos:read_pos+cigar_size]) qual.append(read_qual[read_pos:read_pos+cigar_size]) ref.append(gap_char * cigar_size) for i in range(cigar_size): errors_per_read_pos[read_pos+i] += 1 read_pos += cigar_size if cigar_type == 'D': read.append(gap_char * cigar_size) qual.append(gap_char * cigar_size) ref.append(ref_seq[ref_pos:ref_pos+cigar_size]) errors_per_read_pos[read_pos] += cigar_size ref_pos += cigar_size return ''.join(read), ''.join(qual), ''.join(ref), errors_per_read_pos <INFO_TOKEN> <AST_TOKEN> Module( body=[ FunctionDef( name='align_sequences', args=arguments( posonlyargs=[], args=[ arg(arg='read_seq'), arg(arg='read_qual'), arg(arg='ref_seq'), arg(arg='alignment'), arg(arg='gap_char')], k\n",
            "Декодированная строка: <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <AST_TOKEN><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n",
            "Prediction:\n",
            "Декодированная строка: \n",
            "..\n",
            "_ import\n",
            " import\n",
            "\n",
            "_\n",
            ".\n",
            "      \n",
            "                                                                                                                                     <PAD>        <PAD>   <PAD><PAD> <PAD> <PAD><PAD><PAD> <PAD><PAD><PAD><PAD><PAD>    <PAD> <PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD><PAD>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(tqdm(validation_dataloader)):\n",
        "\n",
        "      # print(batch)\n",
        "\n",
        "      focal_method_input_ids = batch['input_ids_focal_method'].to(device)\n",
        "      focal_method_attention_masks = batch['attention_mask_focal_method'].to(device)\n",
        "\n",
        "      focal_cls_input_ids = batch['input_ids_focal_cls'].to(device)\n",
        "      focal_cls_attention_masks = batch['attention_mask_focal_cls'].to(device)\n",
        "\n",
        "      response_ids = batch['ids_response'].to(device)\n",
        "      response_attention_masks = batch['attention_mask_response'].to(device)\n",
        "\n",
        "      input_ids_focal_method_decoder = batch['input_ids_focal_method_decoder'].to(device)\n",
        "      input_ids_focal_cls_decoder = batch['input_ids_focal_cls_decoder'].to(device)\n",
        "\n",
        "      attention_mask_focal_method_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
        "      attention_mask_focal_cls_decoder = batch['attention_mask_focal_cls_decoder'].to(device)\n",
        "\n",
        "      output_codeLM = CodeModel(focal_method_input_ids, focal_method_attention_masks,\n",
        "          focal_cls_input_ids, focal_cls_attention_masks,\n",
        "          response_ids, response_attention_masks,\n",
        "          input_ids_focal_method_decoder, input_ids_focal_cls_decoder,\n",
        "\t\t\t\t\t\tattention_mask_focal_method_decoder, attention_mask_focal_cls_decoder)\n",
        "\n",
        "      loss = output_codeLM.loss\n",
        "      logits = output_codeLM.logits\n",
        "\n",
        "      logits = logits.detach().cpu().numpy()\n",
        "      response_ids = response_ids.cpu().numpy()\n",
        "      response_attention_masks = response_attention_masks.cpu().numpy()\n",
        "\n",
        "      pred_flat = np.argmax(logits, axis=-1)\n",
        "      # print(\"Shape:\")\n",
        "      # print(pred_flat[0][:20])\n",
        "\n",
        "      print()\n",
        "      # print(\"Исходная строка:\")\n",
        "      decode_sequence(focal_method_input_ids[3], CodeModel.tokenizer_code_bert)\n",
        "      decode_sequence(focal_cls_input_ids[3], CodeModel.tokenizer_code_bert)\n",
        "\n",
        "      # decode_sequence(focal_method_input_ids[0], CodeModel.tokenizer_code_bert)\n",
        "\n",
        "      # print(\"GT:\")\n",
        "      decode_sequence(input_ids_focal_method_decoder[3], CodeModel.tokenizerGPT)\n",
        "      decode_sequence(input_ids_focal_cls_decoder[3], CodeModel.tokenizerGPT)\n",
        "\n",
        "      print(\"Prediction:\")\n",
        "      decode_sequence(pred_flat[2], CodeModel.tokenizerGPT)\n",
        "\n",
        "      break\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BM1oR57Xm0pw"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6256753,
          "sourceId": 10137849,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6256764,
          "sourceId": 10137869,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6283970,
          "sourceId": 10174103,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6285702,
          "sourceId": 10176633,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30804,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}