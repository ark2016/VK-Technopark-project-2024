#### Архитектура нейронной сети

Данный раздел посвящён описанию первой части архитектуры нашего решения. В данном блоке общей модели будет произведён анализ текста кода для *focal_method* и *focal_cls* (и извлечённых из них признаков соответственно), описанных в разделе [dataset](dataset). Данная модель будет генерировать первоначальное решение, которое будет улучшаться в следующем разделе с помощью большой языковой модели.

#### Файлы 

* [create_dataset_m2t_tokens.py](create_dataset_m2t_tokens.py) - расширенная реализация аналогичного метода из раздела [dataset](dataset), окончательно приводящая данные к готовому для подачи в сеть виду. Данный функционал преобразует *focal_method* и *focal_cls* к виду *input_string_focal_method* и *input_string_focal_cls*, характеризующие вход моделей. 

* [final_data_preparation_lab.ipynb](final_data_preparation_lab.ipynb) - файл (.ipynb), в котором показана лаборатория с анализом и примером приведения датасета к искомому формату.

* [torch_transformers_utils.py](torch_transformers_utils.py) - файл (.py) с основными функциями и классами для подготовки данных и моделей к train-val-loop. Основные сущности - это ```class Code2TestDataset(Dataset):``` и ```class LargeCodeModel(nn.Module)```, характеризующие класс датасета для формирования даталоадера и класс модели для реализации цикла обучения. Также реализуются функции ```def get_datasets()``` и ```def get_loaders()```,  необходимые для получения датасетов и даталоадеров. Остальное - служебные функции и переменные для отладки и проверки корректности работы кода.

* [model_research_lab.ipynb](model_research_lab.ipynb) - файл (.ipynb), в котором показана лаборатория с анализом подбора архитектуры и реализацией train-val-loop (дополняется).

* [model_research_lab_launch.ipynb](model_research_lab_launch.ipynb) - файл (.ipynb), в котором показана лаборатория с анализом подбора архитектуры и реализацией train-val-loop.

* [languagemodels.ipynb](languagemodels.ipynb) - файл (.ipynb), в котором реализуется train-val-loop. ***Важно***: именно этот файл реализует обучение модели.

* [pictures](pictures) - папка с изображениями (например, архитектуры сети).


#### Примечания

* Входные данные приведены к следующему формату:

    * <FUNC_TOKEN> <текст исследуемой функции> <INFO_TOKEN> <DESCRIPTION_TOKEN> <текст описания функции> <COMMENTS_TOKEN> <текст комментариев, встреченных в коде> <AST_TOKEN> <текст ast-представления структуры кода> - *input_string_focal_method*.

    * <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <DESCRIPTION_TOKEN> <текст описания функции> <COMMENTS_TOKEN> <текст комментариев, встреченных в коде> <AST_TOKEN> <текст ast-представления структуры кода> - *input_string_focal_cls*.

* Для согласованности с датасетом оценки метрик тестирования проводится регуляризация данных, для которых нехарактерно наличие *focal_cls*, описывающих окружение функции, принятно решение замаскировать 30% (~85k сэмплов)*input_string_focal_cls* и получить формат <CLS_TOKEN> <FUNC_TOKEN> <INFO_TOKEN> <DESCRIPTION_TOKEN> <COMMENTS_TOKEN> <AST_TOKEN>. Подобный подход призван осуществить регуляризацию данных, что позволит избежать конфликт с новой серией данных, которые поступят на втором этапе обучения.

* Для учёта токенов токенизаторы для ```codeBERT``` модели расширяются добавлением новых токенов. GPT2 токенизатор расширяется \<PAD\> токеном.

* Ссылка на частично [подготовленный датасет](https://cloud.mail.ru/public/UUQb/BQsxBFqMb)

#### Инструкция по запуску обучения

1. Скачать датасет по [ссылке](https://cloud.mail.ru/public/UUQb/BQsxBFqMb)

2. Скачать файлы [model_research_lab_launch.ipynb](model_research_lab_launch.ipynb) и [create_dataset_m2t_tokens.py](create_dataset_m2t_tokens.py). Первый будет импортировать второй для дополнительной обработки данных.

3. Выполнить ячейки в ноутбуке.

4. ***Важно***: при вызове функции ```get_loaders```(batch_size = \<YOUR_BATCH_SIZE\>) выбрать разумный batch_size. По умолчанию выставлен 2 для отладки.

5. ***Важно***: при вызове функции ```train_val_loop_codeLM``` параметр ```test_step_only``` сделать ```False``` и ```device = 'cuda'```. По умолчанию выставлены ```True``` и ```cpu``` для отладки.

6. ***Если что-то непонятно, обратиться ко мне за консультацией !!!***

#### Или другая инструкция (UPD)

1. Скачать полностью подготовленный датасет по [ссылке]([https://cloud.mail.ru/public/sKTM/4V9QD9qJD](https://cloud.mail.ru/public/sKTM/4V9QD9qJD)). Этот файл (.parquet) будет считываться в ноутбуке обучения.

2. Скачать файл [languagemodels.ipynb](languagemodels.ipynb), который реализует обучение.

3. Выполнить ячейки в ноутбуке.

4. ***Важно***: при вызове функции ```get_loaders```(batch_size = \<YOUR_BATCH_SIZE\>) выбрать разумный batch_size. По умолчанию выставлен 2 для отладки.

5. ***Важно***: при вызове функции ```train_val_loop_codeLM``` параметр ```test_step_only``` сделать ```False``` и ```device = 'cuda'```. По умолчанию выставлены ```True``` и ```cpu``` для отладки.



